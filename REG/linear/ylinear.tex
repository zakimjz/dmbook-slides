\lecture{linear}{linear}

\date{Chapter 23: Linear Regression}

\begin{frame}
\titlepage
\end{frame}



%\chapter{Linear Regression}
%\label{ch:reg:linear}
%\label{chap:reg:linear}
%
%% add algorithms, and add exercise for excluding w_0 from ridge, i.e.,
%% do not penalize the bias term (for linear and kernel regression).
%
%
%\index{predictor variable|see{independent variable}}
%\index{explanatory variable|see{independent variable}}
%\index{independent variable}
%\index{linear regression!independent variable}
%\index{response variable|see{dependent variable}}
%\index{dependent variable}
%\index{regression!linear regression}
%\index{linear regression!dependent variable}
%\index{linear regression!response variable}
%\index{error term!linear regression}
%\index{linear regression!error term}

\begin{frame}{Regression}
Given  $X_1, X_2, \cdots, X_d$ 
({\em predictor}, {\em explanatory}, or {\em independent} variables),
	and given $Y$ ({\em response} or {\em dependent} variable),
{\em regression} aims to predict $Y$ based on $X$.

\medskip

That is, the goal is to learn a {\em
regression function} $f$, such that
\begin{align*}
    Y = f(X_1, X_2, \cdots, X_d) + \varepsilon  = f(\bX) + \varepsilon
\end{align*}
where $\bX = (X_1, X_2, \cdots, X_d)^T$ is the multivariate random
variable comprising the predictor attributes, 
and $\varepsilon$ is a random {\em error term} that is assumed
to be independent of $\bX$. 

\medskip

$Y$ is comprised of two
components, one dependent on $X$, and 
the other, coming from the error term, independent of the predictor
attributes. 

\medskip

The error term encapsulates inherent uncertainty in $Y$, as
well as, possibly the effect of unobserved, hidden or {\em latent}
variables.
\end{frame}
%
%In this chapter we discuss linear regression, where the regression
%function $f$ is assumed to be a linear function of the parameters of the
%model. We also discuss regularized linear regression models considering
%both $L_2$ (ridge regression) and $L_1$ (Lasso) regularization. Finally,
%we use the kernel trick to perform kernel ridge regression that can
%handle non-linear models.
%
%% \enlargethispage{2\baselineskip}
%\section{Linear Regression Model}
\begin{frame}{Linear Regression}
In {\em linear regression} the function $f$ is assumed to be linear in
$\bX$,  that is
\begin{empheq}[box=\tcbhighmath]{align*}
    f(\bX) = \trueb + \truew_1 X_1 + \truew_2 X_2 + \cdots + \truew_d
    X_d = \trueb +
    \sum_{i=1}^d \truew_i X_i 
    = \trueb + \truebw^T\bX
\end{empheq}
$\trueb$ is the true (unknown) {\em bias} term,
%\index{linear regression!bias}
%\index{linear regression!regression coefficient}
$\truew_i$ is the true (unknown) {\em regression coefficient} or {\em weight}
for attribute $X_i$, and $\truebw = (\truew_1, \truew_2, \cdots,
\truew_d)^T$ is the true $d$-dimensional
weight vector. 

\medskip

$f$ specifies a hyperplane in $\setR^{d+1}$, where
$\truebw$ is the the weight vector that is normal or orthogonal to the hyperplane, 
and $\trueb$ is the {\em intercept} or offset term.
%(see %\cref{sec:eda:highdim:hd_objects}). 

\medskip

$f$ is completely specified by the $d+1$ parameters comprising $\trueb$ and 
$\truew_i$, for $i=1,\cdots,d$.
\end{frame}
%
%The true bias and regression coefficients are unknown. Therefore, we
%have to estimate them from the training dataset $\bD$
%comprising $n$ points $\bx_i \in \setR^{d}$ in a $d$-dimensional space, and the
%corresponding response values $y_i \in \setR$, for $i=1,2,\cdots,n$.
%Let $b$ denote the estimated value for the true bias $\trueb$, and let
%$w_i$ denote the estimated value  for the true regression coefficient
%$\truew_i$, for $i=1,2,\cdots,d$. Let $\bw = (w_1, w_2, \cdots, w_d)^T$
%denote the vector of estimated weights.
%Given the estimated bias and weight values, we can predict the response for any
%given input or test point $\bx = (x_1, x_2, \cdots, x_d)^T$, as follows
%\begin{align*}
%    \hy = b + w_1 x_1 + \cdots + w_d x_d = b + \bw^T\bx
%\end{align*}
%Due to the random error term, 
%the predicted value $\hy$ will not in general match
%the observed response $y$ for the given input $\bx$. This is 
%true even for the training data. 
%The difference between the observed and predicted response, called the {\em
%residual error}, is given as
%\begin{align*}
%    \epsilon = y - \hy = y - b - \bw^T\bx
%\end{align*}
%Note that the residual error $\epsilon$ is different from the random
%statistical error $\varepsilon$, which measures the difference between
%the observed and the (unknown) true response.
%The residual error $\epsilon$ is an estimator of the random 
%error term $\varepsilon$.
%\index{linear regression!residual error}
%
\begin{frame}{Linear regression}
A common approach to predicting the bias and regression coefficients is
to use the method of {\em least squares}. 

\medskip
Given the training
data $\bD$ with points $\bx_i$ and response values $y_i$ (for
$i=1,\cdots,n$), we seek values $b$
and $\bw$, so as to minimize the sum of squared residual errors (SSE)
%\index{linear regression!SSE}
%\index{linear regression!squared error}
%\index{SSE!linear regression}
%\index{sum of squared residual errors|see{SSE}}
\begin{empheq}[box=\tcbhighmath]{align*}
    SSE = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n \lB(y_i - \hy_i \rB)^2 
    = \sum_{i=1}^n \lB(y_i - b - \bw^T\bx_i \rB)^2
    \label{eq:reg:linear:sse}
\end{empheq}
%In the following sections, we will estimate the unknown parameters, by first
%considering the case of a single predictor variable, and then
%looking at the general case of multiple predictors.
%
%\section{Bivariate Regression}
In bivariate regression, $\bD$ comprises a
single predictor attribute, $X = (x_1, x_2, \cdots, x_n)^T$, along with
 $Y = (y_1, y_2, \cdots, y_n)^T$:
\begin{align*}
    \tcbhighmath{
    \hy_i = f(x_i) = b + w \cdot x_i}
    \label{eq:reg:linear:bivariate}
\end{align*}
\end{frame}
%
%Thus, we seek the straight line $f(x)$
%with slope $w$ and intercept $b$ that {\em best fits} the data.
\begin{frame}{Bivariate Regression}
The residual error is 
%\begin{align*}
$    \epsilon_i = y_i - \hy_i$
%\end{align*}
%Note that $\abs{\epsilon_i}$ denotes the vertical distance between the
%fitted and observed response. 
and the best line that minimizes the SSE:
\begin{align*}
    \tcbhighmath{
    \min_{b, w} SSE  = \sum_{i=1}^n \epsilon_i^2
     = \sum_{i=1}^n (y_i - \hy_i)^2
 =  \sum_{i=1}^n (y_i - b - w \cdot x_i)^2}
 %\label{eq:reg:linear:bivar_SSE_obj}
\end{align*}
%
%To solve this objective, 
We differentiate it with
respect to $b$ and set the result to $0$:
\begin{align*}
    \frac{\partial}{\partial b} SSE & = -2 \sum_{i=1}^n (y_i - b - w\cdot
    x_i) = 0 \notag\\
%        & \implies \sum_{i=1}^n b = \sum_{i=1}^n y_i - w \sum_{i=1}^n
%        x_i \notag\\
        & \implies b = \frac{1}{n} \sum_{i=1}^n y_i - w \cdot \frac{1}{n}
        \sum_{i=1}^n x_i \notag
\end{align*}
Therefore, we have
\begin{align*}
    \tcbhighmath{
    b = \mu_Y - w \cdot \mu_X}
    %\label{eq:reg:linear:b}
\end{align*}
%where $\mu_Y$ is the sample mean for the response and $\mu_X$ is the
%sample mean for the predictor attribute.
%Similarly, 
\end{frame}

\begin{frame}{Bivariate Regression}
Differentiating with respect to $w$, we obtain
\begin{align*}
    \frac{\partial}{\partial w} SSE 
    & = -2 \sum_{i=1}^n x_i (y_i - b - w\cdot x_i) = 0 \notag\\
& \implies \sum_{i=1}^n x_i \cdot y_i - b \sum_{i=1}^n x_i  - w \sum_{i=1}^n x_i^2 = 0\\ %\label{eq:reg:linear:ww}
%\intertext{substituting $b$, we have}
& \implies \sum_{i=1}^n x_i\cdot  y_i - \mu_Y\sum_{i=1}^n x_i + w\cdot \mu_X \sum_{i=1}^n
x_i - w \sum_{i=1}^n x_i^2 = 0 \notag\\
%& \implies w \lB(\sum_{i=1}^n x_i^2 - n \cdot \mu_X^2\rB) =
%\lB(\sum_{i=1}^n x_i\cdot y_i\rB) - n\cdot
%\mu_X\cdot \mu_Y \notag\\
& \implies w = \frac{\sum_{i=1}^n x_i\cdot y_i - n\cdot \mu_X\cdot \mu_Y}{\sum_{i=1}^n
    x_i^2 - n\cdot \mu_X^2}
%\label{eq:reg:linear:w}
\end{align*}
%
The regression coefficient $w$ can also be written as
\begin{align*}
    \tcbhighmath{
    w = \frac{\sum_{i=1}^n (x_i - \mu_X)(y_i - \mu_Y)}{\sum_{i=1}^n (x_i
    - \mu_X)^2} = \frac{\sigma_{XY}}{\sigma_X^2} =
\frac{\ocov(X,Y)}{\var(X)}}
%\label{eq:reg:linear:w-cov-var}
\end{align*}
%where $\sigma_X^2$ is the variance of $X$ and $\sigma_{XY}$ is the
%covariance between $X$ and $Y$.
\end{frame}
%Noting that the correlation between $X$ and $Y$ is given as
%$\rho_{XY} = \tfrac{\sigma_{XY}}{\sigma_X \cdot \sigma_Y}$, we can also
%write $w$ as
%\begin{align*}
%    w = \rho_{XY} \frac{\sigma_Y}{\sigma_X}
%    \label{eq:reg:linear:w_rho_sigma}
%\end{align*}
%
%
%Observe that the fitted
%line must pass through the mean value of $Y$ and $X$;
%plugging in the optimal value of $b$ from \cref{eq:reg:linear:b} into the regression
%equation [\cref{eq:reg:linear:bivariate}], we have
%\begin{align*}
%    \hy_i  = b + w \cdot x_i 
%    = \mu_Y - w\cdot \mu_X + w \cdot x_i
%    = \mu_Y + w (x_i - \mu_X) 
%    %\label{eq:reg:linear:opt-line}
%\end{align*}
%That is, when $x_i = \mu_X$, we have $\hy_i = \mu_Y$. Thus, the point
%$(\mu_X, \mu_Y)$ lies on the regression line.
%
%\vspace{0.7in}
%\caption{Scatterplot: {\tt petal length} ($X$) versus {\tt petal
%width} ($Y$). Solid circle (black) shows the mean point; residual
%error is shown for two sample points: $x_9$ and $x_{35}$.}
%\label{fig:reg:linear:irisplw}
%\end{figure}%\vspace*{12pt}
%
%
\begin{frame}{Bivariate Regression}
    Given two attributes {\tt petal length} ($X$; the predictor variable) and 
    {\tt petal width} ($Y$; the response variable) in the
    Iris dataset ($n=150$). %There are a total of $n=150$ data points.
\readdata{\dataPLW}{REG/linear/figs/iris-plw.dat}
\begin{center}
%\begin{figure}[b!]
% \vspace{-0.1in}
    \centering
\scalebox{0.8}{
    \psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray,arrowscale=2,PointName=none}
    \psset{xAxisLabel=$X$: petal length,yAxisLabel= $Y$: petal width,
        xAxisLabelPos={c,-0.4in},yAxisLabelPos={-0.5in,c}}
    \psgraph[tickstyle=bottom,Dx=1.0,Dy=0.5,Oy=-0.5,subticks=2]{->}(0.0,-0.5)(7.5,3){4in}{2.5in}%
    \dataplot[plotstyle=dots,showpoints=true]{\dataPLW}
    %\psdot[dotstyle=*,dotscale=2](3.76,1.20)
    %\psline[linestyle=dashed](3.76,-0.5)(3.76,1.20)
    %\psline[linewidth=0.5pt](5.6,1.4)(5.6,1.97)
    %\psline[linewidth=0.5pt](3,1.1)(3,0.88)
    %\rput(5.6,1.2){$x_9$}
    %\rput(5.45,1.6){$\epsilon_9$}
    %\rput(3,1.3){$x_{35}$}
    %\rput(2.8,1){$\epsilon_{35}$}
    %\psplot[plotstyle=line]{0}{7.5}{x 0.4164 mul -0.3665 add}
    \endpsgraph
}
\end{center}
\end{frame}

\begin{frame}{Bivariate Regression}
\framesubtitle{Example}
%\begin{example}[Bivariate Regression]
%    \label{ex:reg:linear:bivariate}
    %Figure~\ref{fig:reg:linear:irisplw} shows the scatterplot between

    The mean values for these two variables are
\begin{small}
    \begin{align*}
        \mu_X & = \frac{1}{150} \sum_{i=1}^{150} x_i = \frac{563.8}{150} =
        3.7587\\
        \mu_Y & = \frac{1}{150} \sum_{i=1}^{150} y_i = \frac{179.8}{150} =
        1.1987
    \end{align*}
\end{small}
%    % \enlargethispage{0.2in}
%    % \setlength{\belowdisplayskip}{20pt}
    The variance and covariance is given as
\begin{small}
    \begin{align*}
        \sigma_X^2 & = \frac{1}{150} \sum_{i=1}^{150} (x_i - \mu_X)^2  =
        3.0924\\
        \sigma_Y^2 & = \frac{1}{150} \sum_{i=1}^{150} (y_i - \mu_Y)^2  =
        0.5785\\
        \sigma_{XY} & = \frac{1}{150} \sum_{i=1}^{150} (x_i -
        \mu_X)\cdot (y_i - \mu_Y)  = 1.2877
    \end{align*}
\end{small}
%    % \setlength{\belowdisplayskip}{0pt}
\end{frame}

\begin{frame}{Bivariate Regression}
\framesubtitle{Example}
    Assuming a linear relationship between the response and predictor
    variables, we %use \cref{eq:reg:linear:b} and \cref{eq:reg:linear:w} to 
    obtain the slope and intercept terms as follows
    \begin{align*}
        w & = \frac{\sigma_{XY}}{\sigma_X^2} = \frac{1.2877}{3.0924} =
        0.4164\\
        b & = \mu_Y - w \cdot \mu_X = 1.1987 - 0.4164 \cdot 3.7587 =
        -0.3665
    \end{align*}
    Thus, the fitted regression line is
    \begin{align*}
        \hy = -0.3665 + 0.4164 \cdot x
    \end{align*}
%    \cref{fig:reg:linear:irisplw} plots the best-fitting or regression
%    line; we can observe that the mean point $(\mu_X, \mu_Y) =
%    (3.759,1.199)$ lies on
%    the line. The figure also shows the residual errors, $\epsilon_9$ 
%    and $\epsilon_{35}$, for  
%    the points $x_9$ and $x_{35}$, respectively.
%
    Finally, we can compute the SSE value 
%(see \cref{eq:reg:linear:sse}) 
as follows:
    \begin{align*}
        SSE = \sum_{i=1}^{150} \epsilon_i^2 
        = \sum_{i=1}^{150} (y_i - \hy_i)^2
        = 6.343
    \end{align*}
%
\end{frame}
%\end{example}

\begin{frame}{Bivariate Regression}
\framesubtitle{Example}

{\tt petal length} ($X$) versus {\tt petal
width} ($Y$). Solid circle (black) shows the mean point; residual
error is shown for two sample points: $x_9$ and $x_{35}$.

\readdata{\dataPLW}{REG/linear/figs/iris-plw.dat}
\begin{center}
%\begin{figure}[b!]
% \vspace{-0.1in}
    \centering
\scalebox{0.8}{
    \psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray,arrowscale=2,PointName=none}
    \psset{xAxisLabel=$X$: petal length,yAxisLabel= $Y$: petal width,
        xAxisLabelPos={c,-0.4in},yAxisLabelPos={-0.5in,c}}
    \psgraph[tickstyle=bottom,Dx=1.0,Dy=0.5,Oy=-0.5,subticks=2]{->}(0.0,-0.5)(7.5,3){4in}{2.5in}%
    \dataplot[plotstyle=dots,showpoints=true]{\dataPLW}
    \psdot[dotstyle=*,dotscale=2](3.76,1.20)
    %\psline[linestyle=dashed](3.76,-0.5)(3.76,1.20)
    \psline[linewidth=0.5pt](5.6,1.4)(5.6,1.97)
    \psline[linewidth=0.5pt](3,1.1)(3,0.88)
    \rput(5.6,1.2){$x_9$}
    \rput(5.45,1.6){$\epsilon_9$}
    \rput(3,1.3){$x_{35}$}
    \rput(2.8,1){$\epsilon_{35}$}
    \psplot[plotstyle=line]{0}{7.5}{x 0.4164 mul -0.3665 add}
    \endpsgraph
}
\end{center}
\end{frame}

%\subsection{Geometry of Bivariate Regression}
\begin{frame}{Geometry of Bivariate Regression}
%We now turn to the
%attribute-centric view, which provides important geometric insight for
%bivariate regression. Recall that we are given $n$ equations in
%two unknowns, namely $\hy_i = b + w \cdot x_i$, for $i=1,\cdots,n$. Let
%$X = (x_1, x_2, \cdots, x_n)^T$ be the $n$-dimensional vector denoting the training data sample,
%$Y = (y_1, y_2, \cdots, y_n)^T$ the sample vector for the response
%variable, and 
%$\hY = (\hy_1, \hy_2, \cdots, \hy_n)^T$ the vector of predicted values, then 
We can express the $n$
equations, $y_i = b + w \cdot x_i$ for $i=1,2,\cdots,n$, as:
\begin{align*}
    \hY = b \cdot \bone + w \cdot X
    %\label{eq:reg:linear:hY}
\end{align*}
where $\bone \in \setR^n$ is the $n$-dimensional vector of 1s.
%This equation indicates that t
$\hY$ is a
linear combination of $\bone$ and $X$, i.e., it must lie in the column
space spanned by $\bone$ and $X$, given as $\vspan(\{\bone, X\})$.
$\bepsilon$ captures the deviation between $Y$ and $\hY$.

    \centering
    \vspace{-2.5cm}
    \psset{nameX={$\bx_n$}, nameY={$\bx_2$}, nameZ={$\bx_1$}}
    \psset{unit=0.4in}
    \scalebox{0.70}{%
    \begin{pspicture}(-1.5,-1.5)(4,6)
   \psset{Alpha=81,Beta=17,arrowscale=1.5}
    % \pstThreeDCoor[xMin=0, xMax=8, yMin=0, yMax=4,
    %     zMin=0, zMax=2, linecolor=black]
\psset{IIIDxTicksPlane=xy}
    %\psset{dotstyle=Bo,dotscale=2.5,fillcolor=lightgray}
    %\pstThreeDDot[drawCoor=false,linecolor=gray](5.9,3.0,4.2)
    \pstThreeDCoor[RotZ=-20,xMin=0,xMax=7,yMin=0,yMax=7,zMin=0,zMax=2,
    linecolor=gray,linewidth=0.5pt,linestyle=dashed]
    \pstThreeDLine[linewidth=2pt,arrows=->](0,0,0)(5.9,3.0,4.2)
    %\pstThreeDLine[linewidth=0.5pt,arrows=->](0,0,0)(0,0,2)
    \pstThreeDLine[linewidth=1pt,arrows=->](0,0,0)(0,4.0,0)
    \pstThreeDSquare(-2.5,-1,0)(12,0,0)(0,6,0)
    \pstThreeDLine[linestyle=dashed,linecolor=gray,arrows=->,linewidth=1.5pt](5.9,3.0,0)(5.9,3.0,4.2)
    \pstThreeDLine[linewidth=2pt,arrows=->,linecolor=gray](0,0,0)(5.9,3.0,0)
    %\pstThreeDLine[linewidth=1pt,arrows=->](0,0,0)(5.9,1.5,0)
    \pstThreeDLine[linewidth=1pt,arrows=->](0,0,0)(8,2.03,0)
    \pstThreeDPut[origin=rt](5.9,3.0,4.2){$\quad Y$}
    \pstThreeDPut[origin=rt](5.9,3.0,0){$\quad\hY$}
    %\pstThreeDPut[origin=c](6.6,1.7,0){$X$}
    \pstThreeDPut[origin=c](8.5,2.2,0){$X$}
    \pstThreeDPut[origin=r](0,4,0){$\quad \bone$}
    \psset{beginAngle=0,endAngle=90}
    \pstThreeDEllipse(0,0,0)(1.5,0.375,0)(0,0.75,0)
    %\pstThreeDPut[origin=r](1.5,1,0){$\theta$}
    \pstPlanePut[plane=xy,planecorr=xyrot](1.5,0.9,0){\scriptsize $\theta$}
    \pstThreeDPut[origin=rt](5.9,3.9,3.4){$\bepsilon = Y - \hY$}
    \end{pspicture}
}

\end{frame}



%On the other hand, the response vector $Y$ will not usually lie in the
%same column space. 
%In fact, the residual error vector $\bepsilon = (\epsilon_1,
%\epsilon_2, \cdots, \epsilon_n)^T$ captures the deviation between the
%response and predicted vectors.
%\begin{align*}
%    \bepsilon = Y - \hY
%\end{align*}


\begin{frame}{Geometry of Bivariate Regression}
\framesubtitle{Orthogonal decomposition of $X$ into $\mX$ and $\mu_X \cdot    \bone$.}

Even though $\bone$ and $X$ are linearly independent and form a basis for the column space, they need not be orthogonal. 

\medskip

We can create an orthogonal basis by decomposing $X$ into a component along
$\bone$ and a component orthogonal to $\bone$, $\mX$.
\begin{align*}
    X = \mu_X \cdot \bone + (X - \mu_X \cdot \bone) = \mu_X \cdot \bone
    + \mX
\end{align*}
where $\mX = X - \mu_X \cdot \bone$ is the centered attribute
vector.


% (see
%\cref{eq:reg:linear:sse}), since
%\begin{align*}
%    \norm{\bepsilon}^2 = \norm{Y-\hY}^2 = \sum_{i=1}^n (y_i - \hy_i)^2 
%= \sum_{i=1}^n \epsilon_i^2 = SSE
%\end{align*}
%
%
%\begin{figure}[t!]
%    \centering
%    \vspace{-1.5in}
%    \psset{nameX={$\bx_n$}, nameY={$\bx_2$}, nameZ={$\bx_1$}}
%    \psset{unit=0.4in}
%    \scalebox{0.7}{%
%    \begin{pspicture}(-1.5,-1.5)(4,6)
%   \psset{Alpha=81,Beta=17,arrowscale=1.5}
%    % \pstThreeDCoor[xMin=0, xMax=8, yMin=0, yMax=4,
%    %     zMin=0, zMax=2, linecolor=black]
%\psset{IIIDxTicksPlane=xy}
%    %\psset{dotstyle=Bo,dotscale=2.5,fillcolor=lightgray}
%    %\pstThreeDDot[drawCoor=false,linecolor=gray](5.9,3.0,4.2)
%    \pstThreeDCoor[RotZ=-20,xMin=0,xMax=7,yMin=0,yMax=7,zMin=0,zMax=2,
%    linecolor=gray,linewidth=0.5pt,linestyle=dashed]
%    \pstThreeDLine[linewidth=2pt,arrows=->](0,0,0)(5.9,3.0,4.2)
%    %\pstThreeDLine[linewidth=0.5pt,arrows=->](0,0,0)(0,0,2)
%    \pstThreeDLine[linewidth=1pt,arrows=->](0,0,0)(0,4.0,0)
%    \pstThreeDSquare(-2.5,-1,0)(12,0,0)(0,6,0)
%    \pstThreeDLine[linestyle=dashed,linecolor=gray,arrows=->,linewidth=1.5pt](5.9,3.0,0)(5.9,3.0,4.2)
%    \pstThreeDLine[linewidth=2pt,arrows=->,linecolor=gray](0,0,0)(5.9,3.0,0)
%    %\pstThreeDLine[linewidth=1pt,arrows=->](0,0,0)(5.9,1.5,0)
%    \pstThreeDLine[linewidth=1pt,arrows=->](0,0,0)(8,2.03,0)
%    \pstThreeDPut[origin=rt](5.9,3.0,4.2){$\quad Y$}
%    \pstThreeDPut[origin=rt](5.9,3.0,0){$\quad\hY$}
%    %\pstThreeDPut[origin=c](6.6,1.7,0){$X$}
%    \pstThreeDPut[origin=c](8.5,2.2,0){$X$}
%    \pstThreeDPut[origin=r](0,4,0){$\quad \bone$}
%    \psset{beginAngle=0,endAngle=90}
%    \pstThreeDEllipse(0,0,0)(1.5,0.375,0)(0,0.75,0)
%    %\pstThreeDPut[origin=r](1.5,1,0){$\theta$}
%    \pstPlanePut[plane=xy,planecorr=xyrot](1.5,0.9,0){\scriptsize $\theta$}
%    \pstThreeDPut[origin=rt](5.9,3.9,3.4){$\bepsilon = Y - \hY$}
%    \end{pspicture}
%}
%    \vspace{0.8in}
%\caption{Geometry of bivariate regression: non-orthogonal basis. All
%    the vectors conceptually lie in the $n$-dimensional space spanned by the $n$
%data points. The plane illustrates the subspace spanned by $\bone$ and $X$.}
%\label{fig:reg:linear:bi_geom_nonorth}
%\end{figure}
%
%
%\begin{figure}[t!]
    \vspace{0.1in}
    \centering
    % \hspace{0.3in}
    \psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray}
    \psset{xAxisLabel=$\bone$,yAxisLabel=$\mX$}
    \scalebox{0.90}{%
    \pspicture(-1,-1.5)(0.5,3.5)
    \psgraph[ticks=none,labels=none]{->}(0,0)(7,3.0){1.75in}{1.25in}%
    %\psaxes[ticks=none]{<->}(0,0)(6,4.5)
    \uput[ur](5.9,3.0){$X$}
    \psline[linewidth=2pt,arrowscale=1.5]{->}(0,0)(5.9,3.0)
    \psline[linestyle=dashed]{-}(5.9,0)(5.9,3.0)
    \psline[linestyle=dashed]{-}(0,3.0)(5.9,3.0)
    \psline[linewidth=2pt,arrowscale=1.5,linecolor=gray]{->}(0,0)(5.9,0)
    \psline[linewidth=2pt,arrowscale=1.5,linecolor=gray]{->}(0,0)(0,3.0)
    \rput(3,-0.5){$\underbrace{\hspace{1.45in}}_{\mu_X \cdot \bone}$}
    \rput(-0.6,1.5){\begin{Rotateleft}$\overbrace{\hspace{0.8in}}^{X
                - \mu_X \cdot \bone}$\end{Rotateleft}}
    \psset{PointName=none,PointSymbol=none}
    \pstGeonode(0,0){O}(1,0){A}(0.75,0.4){B}
    \pstMarkAngle[MarkAngleRadius=0.75]{A}{O}{B}{$~$}
    \rput(1.5,0.4){$\theta$}
    \endpsgraph
    \endpspicture
}
\end{frame}
%    \vspace{-0.2in}
%    \caption{Orthogonal decomposition of $X$ into $\mX$ and $\mu_X \cdot
%    \bone$.}
%    \label{fig:reg:linear:decompX}
%\end{figure}
%
%At this point it is worth noting that even though $\bone$ and $X$ are
%linearly independent and form a basis for the column space, they need
%not be
%orthogonal (see \cref{fig:reg:linear:bi_geom_nonorth}). We can
%create an orthogonal basis by decomposing $X$ into a component along
%$\bone$ and a component orthogonal to $\bone$ as shown in
%\cref{fig:reg:linear:decompX}. Recall that the scalar projection of a vector
%$\bb$ onto vector $\ba$ (see \cref{eq:eda:data:scalarproj}) is given as
%\begin{align*}
%    \proj_{\ba}(\ba) = \lB(\frac{\bb^T\ba}{\ba^T\ba} \rB)
%\end{align*}
%and the orthogonal projection of $\bb$ on $\ba$ (see
%\cref{eq:eda:data:proj}) is given as
%\begin{align*}
%    \proj_{\ba}(\ba) \cdot \ba = \lB(\frac{\bb^T\ba}{\ba^T\ba} \rB) \cdot \ba
%\end{align*}
%
%Now, consider the projection of $X$ onto $\bone$; we have
%\begin{align*}
%    \proj_\bone(X) \cdot \bone = \lB(\frac{X^T\bone}{\bone^T\bone}\rB) \cdot \bone = 
%    \lB(\frac{\sum_{i=1}^n x_i}{n}\rB) \cdot \bone = \mu_X \cdot \bone
%\end{align*}
%Thus, we can rewrite $X$ as
%\begin{align*}
%    X = \mu_X \cdot \bone + (X - \mu_X \cdot \bone) = \mu_X \cdot \bone
%    + \mX
%\end{align*}
%where $\mX = X - \mu_X \cdot \bone$ is the centered attribute
%vector, obtained by subtracting the mean $\mu_X$ from all points.
%
%\begin{figure}[t!]
\begin{frame}{Geometry of Regression}
	
The optimal $\hY$ that minimizes the error is 
the orthogonal projection of
$Y$ onto the subspace spanned by $\bone$ and $X$.

\medskip


The residual error vector $\bepsilon$ is thus {\em orthogonal} to the subspace spanned by $\bone$
and $X$, and its squared length (or magnitude) equals the SSE value.

\medskip

	Summarizing: 

	\medskip

	$\mu_Y = \proj_\bone(Y)$ \hspace*{1cm} $w = \proj_{\mX}(Y)$ \hspace*{1cm} $b = \mu_Y - w \cdot \mu_X$ 

\medskip

    \centering
    \vspace{-1.0in}
    \psset{unit=0.4in}
    \scalebox{0.70}{%
    \begin{pspicture}(-1.5,-1.5)(4,6)
   \psset{Alpha=81,Beta=17,arrowscale=1.5}
    %\psset{nameX={$Z$}, nameY={$\bone$}, nameZ={$~$}}
    %\pstThreeDCoor[xMin=0, xMax=8, yMin=0, yMax=4,
    %    zMin=0, zMax=0, linecolor=black]
\psset{IIIDxTicksPlane=xy}
    %\psset{dotstyle=Bo,dotscale=2.5,fillcolor=lightgray}
    %\pstThreeDDot[drawCoor=false,linecolor=gray](5.9,3.0,4.2)
    \psset{nameX={$\bx_n$}, nameY={$\bx_2$}, nameZ={$\bx_1$}}
    \pstThreeDCoor[RotZ=-20,xMin=0,xMax=7,yMin=0,yMax=7,zMin=0,zMax=2,
    linecolor=black,linewidth=0.5pt,linestyle=dashed]
    \pstThreeDLine[linewidth=2pt,arrows=->](0,0,0)(5.9,3.0,4.2)
    \pstThreeDLine[linewidth=2pt](0,0,0)(0,3.0,0)
    \pstThreeDLine[linewidth=2pt](0,0,0)(5.9,0,0)
    \pstThreeDLine[linewidth=1pt,arrows=->](0,0,0)(0,4.0,0)
    \pstThreeDLine[linewidth=1pt,arrows=->](0,0,0)(8,0,0)
    \pstThreeDLine[linestyle=dashed,linecolor=gray](5.9,0,0)(5.9,3.0,0)
    \pstThreeDLine[linestyle=dashed,linecolor=gray](0,3.0,0)(5.9,3.0,0)
    \pstThreeDSquare(-2.5,-1,0)(12,0,0)(0,6,0)
    \psset{dotsep=2pt}
    \pstThreeDLine[linewidth=2pt,arrows=->,linecolor=gray](0,0,0)(5.9,3.0,0)
    \pstThreeDPut[origin=rt](3,1.7,2.7){$Y$}
    \pstThreeDPut[origin=rt](4,1.6,0){$\hY$}
    \pstThreeDPut[origin=c](8.7,0.05,0){$\mX$}
    \pstThreeDPut[origin=r](0,4.2,0){$\bone$}
    %\pstThreeDDot[drawCoor=false,linecolor=gray](5.9,3.0,0)
    \pstPlanePut[plane=xy,planecorr=normal](5.7,-0.01,0){$\overbrace{\hspace{0.7in}}^{w}$}
    \pstPlanePut[plane=xy,planecorr=xyrot](0,0.1,0){$\overbrace{\hspace{1.1in}}^{\mu_Y}$}
    \pstThreeDLine[linewidth=1.5pt,linestyle=dashed,arrows=->,linecolor=gray](5.9,3.0,0)(5.9,3.0,4.2)
    \pstThreeDPut[origin=rt](5.9,3.9,3.4){$\bepsilon = Y - \hY$}
    % \pstPlanePut[plane=xz,planecorr=xyrot](5.9,3.0,1){$\overbrace{\hspace{1.1in}}^{\bepsilon}$}
    % \rput(2.7,0.2){\begin{Rotateleft}$\underbrace{\hspace{1.6in}}_{~}$\end{Rotateleft}}
    % \rput(2.7,0.2){$\bepsilon$}
    \end{pspicture}
}
\end{frame}
%    \vspace{0.8in}
%\caption{Geometry of bivariate regression: orthogonal basis. $\mX = X -
%    \mu_X \cdot \bone$ is the centered attribute vector.
%The plane illustrates the subspace spanned by the orthogonal vectors $\bone$ and $\mX$.}
%\label{fig:reg:linear:bi_geom}
%\end{figure}
%
%
%The two vectors $\bone$ and $\mX$ form an {\em orthogonal basis} for
%the subspace. We can thus obtain the predicted vector $\hY$ by projecting
%$Y$ onto $\bone$ and $\mX$, and summing up these two components,
%as shown in \cref{fig:reg:linear:bi_geom}. That is,
%\begin{align*}
%    \hY & = \proj_\bone(Y) \cdot \bone + \proj_{\mX}(Y) \cdot \mX
%     = \lB(\tfrac{Y^T\bone}{\bone^T\bone} \rB)\bone +
%     \lB(\tfrac{Y^T\;\mX}{\mX^T\; \mX}\rB) \mX
%     = \mu_Y \cdot \bone + \lB(\tfrac{Y^T\;\mX}{\mX^T\; \mX}\rB) \mX
%     \label{eq:reg:linear:hY1}
%\end{align*}
%
%On the other hand, from \cref{eq:reg:linear:hY}, we know that
%\begin{align*}
%\hY = b \cdot \bone + w \cdot X = b \cdot \bone + w \lB(\mu_X \cdot \bone +
%\mX\rB) = (b + w \cdot \mu_X) \cdot \bone + w \cdot \mX
%     \label{eq:reg:linear:hY2}
%\end{align*}
%Since both \cref{eq:reg:linear:hY1} and \cref{eq:reg:linear:hY2} 
%are expressions for $\hY$, we can equate them to obtain
%\begin{align*}
%    \mu_Y & = b + w \cdot \mu_X \;\;\text{ or }\;\; b = \mu_Y - w\cdot\mu_X &
%      w & = \tfrac{Y^T\;\mX}{\mX^T\;\mX} 
%\end{align*}
%where the bias term $b = \mu_Y - w \cdot \mu_X$ matches
%\cref{eq:reg:linear:b}, and the weight $w$ also matches
%\cref{eq:reg:linear:w}, since
%\begin{align*}
%w  = \frac{Y^T\;\mX}{\mX^T\;\mX} = \frac{Y^T\;\mX}{\norm{\mX}^2} = 
%\frac{Y^T (X - \mu_X \cdot
%    \bone)}{\norm{X-\mu_X \cdot \bone}^2} = 
%    \frac{\lB(\sum_{i=1}^n y_i x_i\rB) - n \cdot \mu_X \cdot \mu_Y}{\lB(\sum_{i=1}^n
%        x_i^2\rB) - n \cdot
%    \mu_X^2}
%\end{align*}
%
%% In other words, the slope $w$ of the best fitting line is the length of the
%% projection of $Y$ in terms of $X$. It is also proportional to the cosine
%% of the angle between $Y$ and the centered vector $\mX = X - \mu_X \cdot
%% \bone$, since
%% \begin{align*}
%%     w = \frac{Y^T \mX}{\mX^T \mX} = \frac{\norm{Y} \cdot \norm{\mX}
%%         \cos\theta}{\norm{\mX}^2}  = \frac{\norm{Y}}{\norm{\mX}} \cos\theta
%% \end{align*}
%
%
%\begin{example}[Geometry of Regression]
\begin{frame}{Geometry of Regression}
\framesubtitle{Example}
    Let us consider the regression of \texttt{petal length} ($X$) on 
    \texttt{petal width} ($Y$) for the Iris dataset, with $n=150$. 
    First,
    we center $X$ by subtracting the mean $\mu_X = 3.759$.
    Next, we compute the scalar projections of $Y$ onto $\bone$ and $\mX$, 
    to obtain
    \begin{align*}
        \mu_Y & = \proj_\bone(Y) = \lB(\frac{Y^T \bone}{\bone^T\bone} \rB)
        = \frac{179.8}{150} = 1.1987\\
        w & = \proj_{\mX}(Y) = \lB(\frac{Y^T \mX}{\mX^T \mX} \rB) =
        \frac{193.16}{463.86} = 0.4164
    \end{align*}
    Thus, the bias term $b$ is given as
    \begin{align*}
        b = \mu_Y - w \cdot \mu_X = 1.1987 - 0.4164 \cdot 3.7587 =
        -0.3665
    \end{align*}
    %These values for $b$ and $w$ match those in
    %\cref{ex:reg:linear:bivariate}.
    We can compute the SSE value %(see \cref{eq:reg:linear:sse}) 
    as the squared length of the residual error vector
    \begin{align*}
        SSE = \norm{\bepsilon}^2 = \norm{Y - \hY}^2 = (Y - \hY)^T(Y-\hY)
        = 6.343
    \end{align*}
\end{frame}
%\end{example}
%
%
%\section{Multiple Regression}
%\index{multiple regression}
%\index{regression!multiple regression}
%\index{linear regression!multiple regression}

\ifdefined\wox \begin{frame} \titlepage \end{frame} \fi

\begin{frame}{Multiple Regression}

{\em Multiple regression:}
%\footnote{We follow the usual terminology and reserve
%    the term {\em multivariate regression} for the case when there are multiple
%    response attributes $Y_1, Y_2, \cdots, Y_q$ and multiple predictor
%    attributes $X_1, X_2, \cdots, X_d$.}
%where we have 
multiple predictor attributes $X_1, X_2, \cdots, X_d$ and a single response
attribute $Y$. 
%%Let $\bX = (X_1, X_2, \cdots, X_d)^T$
%%denote the multivariate random variable corresponding to the independent
%%attributes.

\medskip

The training data sample $\bD \in \setR^{n \times d}$
comprises $n$ points $\bx_i = (x_{i1}, x_{i2}, \cdots, x_{id})^T$ in a $d$-dimensional space, along with the
corresponding observed response value $y_i$. 

\medskip

%The vector $Y = (y_1, y_2, \cdots, y_n)^T$ denotes the observed response
%vector.
%The predicted response value
%for input $\bx_i$ is
%given as
%\begin{align*}
%    \hy_i = b + w_1 x_{i1} + w_2 x_{i2} + \cdots + w_d x_{id} = b + \bw^T
%    \bx_i
%    \label{eq:reg:linear:multiple_w_noaugment}
%\end{align*}
%where $\bw = (w_1, w_2, \cdots, w_d)^T$ is the weight vector comprising the
%regression coefficients or weights $w_{\!j}$ along each attribute
%$X_{\!j}$.
%\cref{eq:reg:linear:multiple_w_noaugment} defines a hyperplane in $\setR^{d+1}$ 
%with bias term $b$ and normal
%vector $\bw$.
%
Instead of dealing with the bias $b$ separately from the weights $w_i$,
we can introduce a new ``constant'' valued attribute $X_0$ whose value is
always fixed at $1$. 
%Thus, each input point $\bx_i = (x_{i1}, x_{i2}, \cdots,
%x_{id})^T \in \setR^d$ is mapped to the point $\abx_i = (x_{i0}, 
%x_{i1}, x_{i2}, \cdots, x_{id})^T \in \setR^{d+1}$, where $x_{i0} =1$. 
%Likewise, the weight vector $\bw = (w_1, w_2, \cdots, w_d)^T$ is mapped
%to an augmented weight vector $\abw = (w_0, w_1, w_2, \cdots, w_d)^T$,
%where $w_0 = b$.

\medskip

The predicted response value for an augmented 
$(d+1)$ dimensional point $\abx_i$ can be written as
\begin{empheq}[box=\tcbhighmath]{align*}
    \hy_i = w_0 x_{i0} + w_1 x_{i1} + w_2 x_{i2} + \cdots + w_d x_{id} =
    \abw^T \abx_i
\end{empheq}
\end{frame}
%Since there are $n$ points, in fact we have $n$ such equations, one per
%point, and there are $(d+1)$ unknowns, namely the elements of the
%augmented weight vector
%$\abw$. We can compactly write all these $n$ equations as a single matrix
%equation, given as
%\begin{align*}
%    \hY = \abD \abw
%    \label{eq:reg:linear:multiple_hY}
%\end{align*}
%where $\abD \in \setR^{n \times (d+1)}$ is the
%{\em augmented data
%matrix}, which includes the constant attribute $X_0$ in addition to the
%% \index{multiple regression!augmented data matrix}
%predictor attributes $X_1, X_2, \cdots, X_d$, and $\hY = (\hy_1,
%\hy_2, \cdots, \hy_n)^T$ is the vector of predicted responses.
%
\begin{frame}{Multiple Regression}
The multiple regression task is to find the {\em best fitting hyperplane} defined
by  $\abw$ that minimizes the SSE:
\begin{align*}
    \min_{\abw} SSE & = \sum_{i=1}^{n} \epsilon_i^2 =
    \norm{\bepsilon}^2 = \norm{Y - \hY}^2 \notag\\
    & = (Y -\hY)^T (Y - \hY) = Y^T Y - 2 Y^T\; \hY +
    \hY^T\; \hY\notag\\
    & = Y^T Y -2 Y^T (\abD \abw) + (\abD \abw)^T (\abD \abw)\notag\\
    & = Y^T Y - 2 \abw^T (\abD^T Y) + \abw^T (\abD^T \abD) \abw
%\label{eq:reg:linear:multiple_SSEobj}
\end{align*}
%where we substituted $\hY = \abD \abw$ from
%\cref{eq:reg:linear:multiple_hY}, and we used the fact that $Y^T
%(\abD \abw) = (\abD \abw)^T Y = \abw^T (\abD^T Y)$.
%
%To solve the objective, we differentiate the expression in
%\cref{eq:reg:linear:multiple_SSEobj} with
%respect to $\abw$ and set the result to $\bzero$, to obtain
%\begin{align*}
%    \frac{\partial}{\partial \abw} SSE & = -2 \abD^T Y + 2 (\abD^T\abD) \abw
%    = \bzero\\
%    & \implies (\abD^T \abD) \abw = \abD^T Y
%\end{align*}
Therefore, the optimal weight vector is given as
\begin{align*}
    \tcbhighmath{
    \abw = (\abD^T \abD)^{-1} \abD^T Y}
%\label{eq:reg:linear:multiple_w}
\end{align*}
\end{frame}
%
%Substituting the optimal value of $\abw$ into
%\cref{eq:reg:linear:multiple_hY}, we get
%\begin{align*}
%    \hY = \abD \abw = \abD (\abD^T \abD)^{-1} \abD^T Y = \bH Y
%\end{align*}
%where $\bH = \abD (\abD^T \abD)^{-1} \abD^T$ is called the {\em hat matrix},
%\index{multiple regression!hat matrix}
%since it puts the ``hat'' on $Y$!
%Notice also that $\abD^T\abD$ is the uncentered scatter matrix for the
%training data.
%
%% \begin{figure}[t!]
%%     \centering
%% \vspace*{12pt}
%% \readdata{\dataIrisP}{REG/linear/figs/iris-3d-pos.dat}
%% \readdata{\dataIrisN}{REG/linear/figs/iris-3d-neg.dat}
%% \psset{unit=0.5in}
%% \psset{arrowscale=2,dotscale=1.75}
%% \psset{Alpha=70,Beta=20}
%% \psset{nameX=$X_1$, nameY=$X_2$, nameZ=$Y$}
%% \psset{IIIDOffset={(3,0,0)}}
%% \begin{pspicture}(-2,-4.5)(2,4.5)
%% \pstThreeDCoor[xMin=3,xMax= 7.9,%
%%     yMin=0,yMax=6.9,%
%%     zMin=0,zMax=2.5,% 
%%     %Dx=0.5, Dy=0.5, Dz=0.5,%
%%         %IIIDticks,IIIDlabels,%
%%         linewidth=1pt,linecolor=black,arrowscale=2]
%% %\multido{\iA=1+1}{8}{\footnotesize%
%% %\pstThreeDPut(-0.01,\iA-1, -0.01){\iA}}
%% %\multido{\nx=3.0+1.0}{5}{\scriptsize%
%% %        \pstThreeDPut(\nx\space,0.1,-0.1){\nx}}
%% % \multido{\ny=1.0+1.0}{8}{%
%% %         \pstThreeDPut(0,\ny\space,-1){\scriptsize{\ny}}}
%% \psset{dotstyle=Bo,fillcolor=lightgray}
%% \dataplotThreeD[plotstyle=dots,showpoints=true]{\dataIrisN}
%% \pstThreeDSquare[fillstyle=vlines,
%%     hatchcolor=black, hatchwidth=0.1\pslinewidth,
%%     hatchsep=3\pslinewidth]
%% %(0,0,-0.0139)(1,0,-0.082)(0,1, 0.45)
%% %(0,0,-0.0139)(8,0,-0.65)(0,8,3.6)
%% (3.5,0.5,-0.079)(5,0,-0.41)(0,7,3.15)
%% \input{REG/linear/figs/iris-3d-lines.tex}
%% \psset{dotstyle=Bo,fillcolor=white}
%% \dataplotThreeD[plotstyle=dots,showpoints=true]{\dataIrisP}
%% \end{pspicture}
%% \end{figure}
%
%\begin{figure}[t!]
%\vspace*{-0.2in}

\begin{frame}{Multiple Regression}
\framesubtitle{Example}

Given  
{\tt sepal length} ($X_1$) and {\tt petal
    length} ($X_2$) on the response attribute {\tt petal width} ($Y$)
for the Iris dataset with $n=150$ points, we want to learn the multiple regression. 

\centering
\psset{unit=0.25in}
\scalebox{0.95}{%
\psset{viewpoint=20 10 10 rtp2xyz,Decran=30}
\psset{lightsrc=viewpoint}
\psset{incolor=white}
\psset{opacity=0.2}
\psset{fillcolor=white}
\begin{pspicture}(3,-5)(8,7)
    \axesIIID[axisnames={X_1, X_2, Y}](0,0,0)(9,8,3)
\psset{linewidth=0pt, dotsize=0.3}
\psset{dotstyle=Bo,fillcolor=white}
\input{REG/linear/figs/iris-3d-points.tex}
\end{pspicture}
}
\end{frame}


\begin{frame}{Multiple Regression}
\framesubtitle{Example}
    %\label{ex:reg:linear:multiple}
    %\cref{fig:reg:linear:multi} 
We and $X_0 = \bone_{150}$ and 
$\abD \in
\setR^{150\times 3}$.
We then compute 
$\abD^T\abD$ and its inverse
\begin{small}
\begin{align*}
    \abD^T\abD & = \matr{
150.0 & 876.50  & 563.80\\
876.5 & 5223.85 & 3484.25\\
563.8 & 3484.25 & 2583.00 }
&
(\abD^T\abD)^{-1} & = \amatr{r}{
0.793 & -0.176 & 0.064\\
-0.176 &  0.041 & -0.017\\
0.064 & -0.017 &  0.009}
\end{align*}
\end{small}
We also compute $\abD^T Y$, given as
\begin{small}
\begin{align*}
    \abD^TY & = \matr{
 179.80\\
 1127.65\\
 868.97}
\end{align*}
\end{small}
The augmented weight vector $\abw$ is then given as
\begin{small}
\begin{align*}
    \abw = \matr{w_0\\w_1\\w_2} = (\abD^T\abD)^{-1} \cdot (\abD^T Y) = 
    \amatr{r}{-0.014\\ -0.082\\ 0.45}
\end{align*}
\end{small}
Therefore $b=w_0=-0.014$, and
%the fitted model is 
$\hY = -0.014 -0.082 \cdot X_1 + 0.45\cdot X_2$

%\cref{fig:reg:linear:multi} 
\end{frame}

\begin{frame}{Multiple Regression}
\framesubtitle{Example}
Figure shows the fitted hyperplane and 
the residual error for each point. Positive 
residuals (i.e., $\epsilon_i > 0$ or $\hy_i > y_i$) are white,
while negative residuals (i.e., $\epsilon_i < 0$ or $\hy_i < y$) are gray. The SSE value for the
model is $6.18$.

\centering
\psset{unit=0.25in}
\scalebox{0.75}{%
%\psset{viewpoint=20 7 7 rtp2xyz,Decran=30}
\psset{viewpoint=20 10 10 rtp2xyz,Decran=30}
\psset{lightsrc=viewpoint}
\psset{incolor=white}
\psset{opacity=0.2}
\psset{fillcolor=white}
\begin{pspicture}(3,-5)(8,7)
    \axesIIID[axisnames={X_1, X_2, Y}](0,0,0)(9,8,3)
\psset{linewidth=0.5pt, dotsize=0.3}
\psset{dotstyle=Bo,fillcolor=gray}
\input{REG/linear/figs/iris-3d-linesN.tex}
\psSurface[showAxes=false,ngrid=.2 .2,fillcolor=lightgray,%axesboxed,
        linewidth=0.5\pslinewidth,
        color1 = {[rgb]{0 0 0}},
        color2 = {[rgb]{1 1 1}},
        hue=(color1) (color2), lightintensity=5,
    algebraic,Zmin=0,Zmax=3](0,0)(8,7){%
    (-0.0819*x) + (0.45*y) - 0.0139 }
\psset{dotstyle=Bo,fillcolor=white}
\input{REG/linear/figs/iris-3d-linesP.tex}
\end{pspicture}
}
\end{frame}
%\vspace{0.2in}
%\caption{Multiple regression: {\tt sepal length} ($X_1$) and {\tt petal
%    length} ($X_2$) with response attribute {\tt petal width} ($Y$).
%The vertical bars show the residual error for the points. Points in
%white are above the plane, whereas points in gray are below the plane.}
%    \label{fig:reg:linear:multi}
%\end{figure}
%
%\begin{example}[Multiple Regression]

%\label{ex:reg:linear:multiple_basic}
%\end{example}
%
% WMJ Skipped Geometry of Multiple Regression
%\subsection{Geometry of Multiple Regression}
%\label{sec:reg:linear:orthogonalization}
%Let 
%$\abD$ be the augmented data matrix comprising the $d$ independent
%attributes $X_i$, along with the new constant attribute $X_0 = \bone \in
%\setR^n$, given as
%\begin{align*}
%\abD = \matr{
%    | & | & | &  & | \\
%    X_0 & X_1 & X_2 & \cdots & X_d\\
%    | & | & | &  & | \\
%}
%\end{align*}
%Let 
%$\abw = (w_0, w_1, \cdots, w_d)^T \in \setR^{(d+1)}$ be the augmented weight vector that
%incorporates the bias term $b = w_0$. 
%%Further, let $\bX = (X_0, X_1,
%%\cdots, X_d)$ denote the augmented multivariate random variable, such that
%%each data point $\bx_i$ is a sample drawn from $\bX$.
%Recall that the predicted response vector is given as
%\begin{align*}
%    \hY & = b \cdot \bone + w_1 \cdot X_1 + w_2 \cdot X_2 + \cdots + w_d 
%    \cdot X_d = \sum_{i=0}^d w_i \cdot X_i = \abD \abw
%\end{align*}
%This equation makes it clear that the
%predicted vector must lie in the column space of the augmented data
%matrix $\abD$, denoted $col(\abD)$, i.e.,
%it must be a linear combination of the attribute vectors $X_i$, $i=0,\cdots,d$.
%
%To minimize the error in prediction, $\hY$ must be the
%orthogonal projection of $Y$ onto the subspace $col(\abD)$. 
%The residual error vector $\bepsilon = Y - \hY$ is thus orthogonal to the subspace
%$col(\abD)$, which means that it is orthogonal to each attribute
%vector $X_i$.
%That is,
%\begin{align*}
%    ~ & X_i^T \bepsilon  = 0\\
%    \implies & X_i^T(Y - \hY)  = 0\\
%    \implies & X_i^T\hY  = X_i^TY\\
%    \implies & X_i^T(\abD \abw)  = X_i^T Y\\
%    \implies & w_0 \cdot X_i^T X_0 + w1 \cdot X_i^T X_1 + \cdots + w_d
%    \cdot X_i^T X_d  = X_i^T Y
%\end{align*}
%We thus have $(d+1)$ equations, called the {\em normal equations}, in $(d+1)$ unknowns, namely the
%\index{multiple regression!normal equations}
%regression coefficients or weights $w_i$ (including the bias term $w_0$). The solution to these
%simultaneous equations yields the weight vector $\abw = (w_0, w_1, \cdots,
%w_d)^T$. The $(d+1)$ normal equations are
%\begin{align*}
%    \begin{aligned}    
%    w_0 \cdot X_0^T X_0 + w1 \cdot X_0^T X_1 + \cdots + w_d \cdot X_0^T X_d & = X_0^T Y\\
%    w_0 \cdot X_1^T X_0 + w1 \cdot X_1^T X_1 + \cdots + w_d \cdot X_1^T X_d & = X_1^T Y\\
%    \vdots\qquad\qquad\qquad & = \;\;\vdots\\
%    w_0 \cdot X_d^T X_0 + w1 \cdot X_d^T X_1 + \cdots + w_d \cdot X_d^T X_d & = X_d^T Y
%    \end{aligned}
%    \label{eq:reg:linear:multiple_normal_eqs}
%\end{align*}
%which can be written compactly in matrix notation to solve for $\abw$ as
%follows
%\begin{align*}
%    \matr{
%        X_0^TX_0 & X_0^TX_1 & \cdots & X_0^T X_d\\
%        X_1^TX_0 & X_1^TX_1 & \cdots & X_1^T X_d\\
%        \vdots   & \vdots   & \cdots & \vdots\\
%        X_d^TX_0 & X_d^TX_1 & \cdots & X_d^T X_d\\
%    } %
%    \abw & = \abD^T Y \notag\\
%    (\abD^T \abD) \abw & = \abD^T Y \notag \\
%    \abw & = (\abD^T\abD)^{-1} (\abD^T Y)
%    \label{eq:reg:linear:multiple_normal_compact}
%\end{align*}
%This matches the expression in \cref{eq:reg:linear:multiple_w}.
%
%More insight can be obtained by noting that the attribute vectors
%comprising the column space of $\abD$ are not necessarily orthogonal,
%even if we assume they are linearly independent.  To obtain the
%projected vector $\hY$, we first need to construct
%an orthogonal basis for $col(\abD)$. 
%
%Let $U_0, U_1, \cdots, U_d$ denote the set of orthogonal basis vectors
%for $col(\abD)$. We can construct these vectors in a step-wise manner 
%via {\em
%    Gram--Schmidt orthogonalization}, as follows
%\index{multiple regression!Gram--Schmidt orthogonalization}
%\begin{align*}
%    U_0 & = X_0\\
%    U_1 & = X_1 - p_{10} \cdot U_0\\
%    U_2 & = X_2 - p_{20} \cdot U_0 - p_{21} \cdot
%    U_1\\
%    \vdots\;\; & = \qquad\qquad\vdots\\
%    U_d & = X_d - p_{d0} \cdot U_0 -
%    p_{d1} \cdot U_1 - \cdots - p_{d,d-1} \cdot U_{d-1}
%\end{align*}
%where 
%$$p_{\!ji} = \proj_{U_i}(X_{\!j}) =
%\frac{X_{\!j}^TU_i}{\norm{U_i}^2}$$
%denotes the scalar projection of attribute $X_{\!j}$ onto the basis vector $U_i$.
%Essentially, to obtain $U_{\!j}$, 
%we subtract from vector $X_{\!j}$ its scalar projections along all
%previous basis vectors $U_0, U_1, \cdots, U_{\!j-1}$. 
%
%Rearranging the equations above, so that $X_{\!j}$ is on the left hand
%side, we get
%\begin{align*}
%    X_0 & = U_0\\
%    X_1 & = p_{10} \cdot U_0 + U_1\\
%    X_2 & = p_{20} \cdot U_0 + p_{21} \cdot
%    U_1 + U_2\\
%    \vdots\;\; & = \qquad\qquad\vdots\\
%    X_d & = p_{d0} \cdot U_0 +
%    p_{d1} \cdot U_1 + \cdots + p_{d,d-1} \cdot U_{d-1} + U_d
%\end{align*}
%
%The Gram--Schmidt method thus results in the so-called {\em
%QR-factorization}\footnote{In QR-factorization, the matrix $\bQ$ is
%\index{multiple regression!QR-factorization}
%    orthogonal, with orthonormal columns, i.e., with orthogonal columns that are normalized to be unit length. However, we
%keep the basis vectors un-normalized for ease of presentation.}
%of the data matrix, namely $\abD = \bQ \bR$, where by construction $\bQ$
%is a $n \times (d+1)$ matrix with orthogonal columns
%\begin{align*}
%    \bQ = \matr{
%      | & |   & | &        & | \\
%    U_0 & U_1 & U_2 & \cdots & U_d\\
%    |   & |   & | &        & | \\
%    }
%\end{align*}
%and $\bR$ is the $(d+1) \times (d+1)$ upper-triangular matrix
%\begin{align*}
%    \bR = \matr{
%    1      & p_{10} & p_{20}& \cdots   & p_{d0}\\
%    0      & 1      & p_{21}& \cdots   & p_{d1}\\
%    0      & 0      & 1     & \cdots   & p_{d2}\\
%    \vdots & \vdots & \vdots& \ddots & \vdots\\
%    0      & 0      & 0     & 1        & p_{d,d-1}\\
%    0      & 0      & 0     & 0        & 1
%    }
%\end{align*}
%So that, in the column view the QR-factorization 
%of the augmented data matrix is given as:
%\begin{align*}
%    \underbrace{
%    \matr{
%      | & |   & | &       & | \\
%    X_0 & X_1 & X_2 &\cdots & X_d\\
%    |   & |   & | &       & | \\
%    }}_{\abD}
%    & = 
%    \underbrace{
%    \matr{
%      | & |   & | &        & | \\
%    U_0 & U_1 & U_2 & \cdots & U_d\\
%    |   & |   & | &        & | \\
%    }}_{\bQ}
%    \cdot 
%    \underbrace{
%    \matr{
%    1      & p_{10} & p_{20} & \cdots      & p_{d0}\\
%    0      & 1      & p_{21} & \cdots      & p_{d1}\\
%    0      & 0      & 1      & \cdots      & p_{d2}\\
%    \vdots & \vdots & \vdots & \ddots & \vdots \\
%    0      & 0      & 0   & 1      & p_{d,d-1}\\
%    0      & 0      & 0   & 0      & 1
%    }}_{\bR}
%\end{align*}
%
%Since the new basis vectors $U_0,
%U_1, \cdots, U_d$ form an orthogonal basis for the column space of $\abD$,
%we can obtain the predicted response vector as a sum of the
%projections of $Y$ along each new basis vector, given as
%\begin{align*}
%    \hY & = \proj_{U_0}\!(Y) \cdot U_0 + \proj_{U_1}\!(Y) \cdot U_1 
%    + \cdots + \proj_{U_d}\!(Y) \cdot U_d
%    \label{eq:reg:linear:multiple_proj_hY}
%\end{align*}
%
%\paragraph{Bias Term}
%The geometric approach makes it easy to derive an expression for the
%bias term $b$.
%Note that each of the predictor
%attributes can be centered by removing its projection along the
%vector $\bone$. Define $\mX_{\!i}$ to be the centered attribute vector
%\begin{align*}
%    \mX_{\!i} = X_i - \mu_{X_i} \cdot \bone
%\end{align*}
%All the centered vectors $\mX_{\!i}$ lie in the $n-1$ dimensional
%space, comprising the orthogonal complement of the span of $\bone$.
%
%From the expression of $\hY$, we have
%\begin{align*}
%    \hY & = b \cdot \bone + w_1 \cdot X_1 + w_2 \cdot X_2 + \cdots + w_d 
%    \cdot X_d \notag\\
%    & = b \cdot \bone + w_1 \cdot \lB(\mX_{\!1} + \mu_{X_1} \cdot \bone\rB) +
%    \cdots + w_d \cdot \lB(\mX_{\!d} + \mu_{X_d} \cdot \bone\rB) \notag\\
%    & = \lB(b + w_1 \cdot \mu_{X_1} + \ldots + w_d \cdot \mu_{X_d}\rB)
%    \cdot \bone + w_1 \cdot \mX_{\!1} + \ldots + w_d \cdot \mX_{\!d}
%    \label{eq:reg:linear:multiple_hYgeom}
%\end{align*}
%
%On the other hand, since $\bone$ is orthogonal to all $\mX_{\!i}$, we can obtain 
%another
%expression for $\hY$ in terms of the projection of $Y$ onto the subspace
%spanned by the vectors $\{\bone,\mX_{\!1}, \cdots, \mX_{\!d}\}$.
%Let the new orthogonal basis for these centered attribute vectors be 
%$\{\mU_0, \mU_1, \cdots, \mU_d\}$, where $\mU_0 = \bone$.
%Thus, $\hY$ can also be written as
%\begin{align*}
%    \hY = \proj_{\mU_0}\!(Y) \cdot \mU_0 + 
%    \sum_{i=1}^d \proj_{\mU_i}\!(Y) \cdot \mU_i 
%    = \proj_{\bone}(Y) \cdot \bone + 
%    \sum_{i=1}^d \proj_{\mU_i}\!(Y) \cdot \mU_i 
%    \label{eq:reg:linear:multiple_hY2}
%\end{align*}
%In particular, equating the scalar projections along $\bone$ 
%in \cref{eq:reg:linear:multiple_hYgeom} and
%\cref{eq:reg:linear:multiple_hY2}, we
%get:
%\begin{align*}
%    \proj_{\bone}(Y) = \mu_Y & 
%    = (b + w_1 \cdot \mu_{X_1} + \ldots + w_d \cdot \mu_{X_d}),
%    \text{ which implies} \notag\\
%    b & = \mu_Y - w_1 \cdot \mu_{X_1} - \ldots - w_d \cdot \mu_{X_d} = 
%    \mu_Y - \sum^{n}_{i=1} w_i \cdot \mu_{X_i}
%     \label{eq:reg:linear:multiple_b}
%\end{align*}
%where we use the fact that the scalar projection of any attribute vector onto 
%$\bone$ yields the mean value of that attribute. For example,
%\begin{align*}
%    \proj_{\bone}(Y) & = \frac{Y^T \bone}{\bone^T\bone} =
%    \frac{1}{n}\sum_{i=1}^n y_i = \mu_Y
%\end{align*}
%
%% \begin{align*}
%%     w_0 & = \proj_{U_0}(Y) - \sum_{j=1}^d w_{\!j}
%%     \cdot\proj_{U_0}(X_{\!j}) 
%%     = \mu_Y - \sum_{j=1}^d w_{\!j} \cdot \mu_{X_{\!j}} \notag\\
%%     & = \mu_Y - w_1 \cdot \mu_{X_1} - w_2 \cdot \mu_{X_2} 
%%     - \cdots - w_d \cdot \mu_{X_d}
%%     \label{eq:reg:linear:multiple_b}
%% \end{align*}
%% Here, we used the fact that the projection of any attribute vector onto 
%% $U_0 = X_0
%% = \bone$ yields the mean value of that attribute, since
%% \begin{align*}
%%     \proj_{U_0}(Y) & = \frac{Y^T \bone}{\bone^T\bone} =
%%     \frac{1}{n}\sum_{i=1}^n y_i = \mu_Y\\
%%     \proj_{U_0}(X_{\!j}) & = \frac{X_{\!j}^T \bone}{\bone^T\bone} =
%%     \frac{1}{n}\sum_{i=1}^n x_{\!ji} = \mu_{X_{\!j}}\\
%% \end{align*}
%
%
\begin{frame}{Multiple-Regression Algorithm}
The algorithm is based on the QR-factorization, which expresses a matrix as a product of two separate matrices, Q (an orthogonal matrix), and R (an upper/right triangular matrix). 

\medskip

\begin{tightalgo}[H]{\textwidth-18pt}
    \SetKwInOut{Algorithm}{\textsc{Multiple-Regression} ($\bD, Y$)}
\Algorithm{} 
$\abD \assign \Bigl(\bone \qquad \bD\Bigr)$ \tcp*[h]{augmented data with
$X_0 = \bone \in \setR^n$}\;
$\{\bQ, \bR\} \assign $ QR-factorization$(\abD)$ \tcp*[h]{$\bQ=\matr{U_0 &
U_1 & \cdots & U_d}$}\;
$\bDelta^{-1} \assign 
\scriptsize
\matr{
    \frac{1}{\norm{U_0}^2} & 0 & \cdots & 0\\
    0 & \frac{1}{\norm{U_1}^2} & \cdots & 0\\
        0 & 0 & \ddots & 0\\
        0 & 0 & \cdots & \frac{1}{\norm{U_d}^2}\\
    }$ \tcp*[h]{squared norms}\;
$\bR \bw \assign \bDelta^{-1} \bQ^T Y$ \tcp*[h]{solve for $\bw$ by back-substitution}\;
$\hY \assign \bQ \bDelta^{-1} \bQ^T Y$\;
%\caption{Multiple Regression Algorithm}
%\label{alg:reg:linear:regression}
\end{tightalgo}
\end{frame}
%
%\subsection{Multiple Regression Algorithm}
%\label{sec:reg:linear:multiple_alg}
%The pseudo-code for multiple regression is shown in
%\cref{alg:reg:linear:regression}. The algorithm starts by the
%QR-decomposition of $\abD = \bQ \bR$, where $\bQ$ is a 
%matrix with orthogonal columns that make up an orthogonal basis, 
%and $\bR$ is an upper triangular
%matrix, which can be obtained via Gram--Schmidt orthogonalization.
%Note that, by construction, the matrix $\bQ^T\bQ$ is given as
%\begin{align*}
%    \bQ^T\bQ = \matr{
%        \norm{U_0}^2 & 0 & \cdots & 0\\
%        0 & \norm{U_1}^2 & \cdots & 0\\
%        0 & 0 & \ddots & 0\\
%        0 & 0 & \cdots & \norm{U_d}^2\\
%    } = \bDelta
%\end{align*}
%We can observe that the matrix $\bQ^T\bQ$, denoted $\bDelta$, is a
%diagonal matrix that
%contains the squared norms of the new orthogonal basis vectors
%$U_0, U_1, \cdots, U_d$.
%
%Recall that the solution to multiple regression is given via the normal
%equations [\cref{eq:reg:linear:multiple_normal_eqs}], which can
%be compactly written as
%$(\abD^T\abw)\abw = \abD^T Y$ [\cref{eq:reg:linear:multiple_normal_compact}]; plugging in the QR-decomposition, we get
%\begin{align*}
%    (\abD^T\abD) \abw = \abD^T Y\\
%(\bQ \bR)^T (\bQ \bR) \abw = (\bQ \bR)^T Y\\
%\bR^T (\bQ^T\bQ) \bR \abw = \bR^T \bQ^T Y\\
%\bR^T\bDelta\bR\abw = \bR^T\bQ^T Y\\
%\bDelta\bR\abw = \bQ^T Y\\
%\bR\abw = \bDelta^{-1} \bQ^T Y
%\end{align*}
%Note that $\bDelta^{-1}$ is a diagonal matrix that records the
%reciprocal of the squared norms of new basis vectors $U_0, U_1, \cdots, U_d$.
%Furthermore, since $\bR$ is upper-triangular, it is straightforward to 
%solve for $\abw$ by back-substitution.
%Note also that we can obtain the predicted vector $\hY$ as follows
%\begin{align*}
%    \hY = \abD \abw = \bQ \bR \bR^{-1} \bDelta^{-1} \bQ^T Y = 
%    \bQ (\bDelta^{-1}\bQ^T Y)
%\end{align*}
%It is interesting to note that $\bDelta^{-1} \bQ^T Y$ gives the vector
%of scalar projections of $Y$ onto each of the orthogonal basis vectors
%\begin{align*}
%    \bDelta^{-1}\bQ^T Y = \matr{\proj_{U_0}\!(Y)\\
%        \proj_{U_1}\!(Y)\\ \vdots \\ \proj_{U_d}\!(Y)}
%\end{align*}
%Therefore, $\bQ (\bDelta^{-1}\bQ^T Y)$, yields the projection formula in
%\cref{eq:reg:linear:multiple_proj_hY}
%\begin{align*}
%    \hY = \bQ \matr{\proj_{U_0}\!(Y)\\
%        \proj_{U_1}\!(Y)\\ \vdots \\ \proj_{U_d}\!(Y)}
%    = \proj_{U_0}\!(Y) \cdot U_0 + \proj_{U_1}\!(Y) \cdot U_1 
%    + \cdots + \proj_{U_d}\!(Y) \cdot U_d
%\end{align*}
%
%
%
%% $ Rscript iris-3d.R
%% During startup - Warning message:
%% Setting LC_CTYPE=en_US.UTF-8 failed
%% means 1 5.843333 3.758667 1.198667
%%       ones      V1      V3
%% ones 150.0  876.50  563.80
%% V1   876.5 5223.85 3484.25
%% V3   563.8 3484.25 2583.00
%%             ones          V1           V3
%% ones  0.79300145 -0.17555847  0.063722562
%% V1   -0.17555847  0.04077478 -0.016682017
%% V3    0.06372256 -0.01668202  0.008980851
%%         [,1]
%% ones  179.80
%% V1   1127.65
%% V3    868.97
%% w -0.01385201 -0.08190841 0.44993
%% QR 5.843333 3.758667 1.85751
%% norms 150 102.1683 111.348
%% Inorms 0.006666667 0 0 0 0.009787769 0 0 0 0.008980851
%% QtY 150 3
%%  179.8 77.01867 50.09882
%% IQtY 1.198667 0.7538409 0.44993
%%      [,1]     [,2]     [,3]
%% [1,]    1 5.843333 3.758667
%% [2,]    0 1.000000 1.857510
%% [3,]    0 0.000000 1.000000
%% diff 7.771561e-15
%% back_w -0.01385201 -0.08190841 0.44993 0.8357493 -0.4786182 1.691137 1.212519
%%      [,1]      [,2]      [,3]
%% [1,]    1 -5.843333  7.095381
%% [2,]    0  1.000000 -1.857510
%% [3,]    0  0.000000  1.000000
%% sums 179.8 876.5 563.8 1.198667 5.843333 3.758667
%% orthogonal -5.506706e-14 1.762201e-13 2.011004e-13
%% projY 1.198667 0.7538409 0.44993
%% r-vals -5.843333 7.095381 -1.85751
%% w's -0.01385201 -0.08190841 0.44993
%
%
%
\begin{frame}{QR-Factorization and Geometric Approach}
\framesubtitle{Example}
%\begin{example}[Multiple Regression: QR-Factorization and Geometric
%Approach]
Consider the multiple regression of
{\tt sepal length} ($X_1$) and {\tt petal
    length} ($X_2$) on {\tt petal width} ($Y$)
for the Iris dataset with $n=150$ points.
%, as shown in \cref{fig:reg:linear:multi}.

%The augmented dataset $\abD \in
%\setR^{150\times 3}$ comprises $n=150$ points along
%three attributes $X_0$, $X_1$, and $X_2$, where $X_0 = \bone$.

\medskip

The Gram--Schmidt orthogonalization results in the following
QR-factorization:
\begin{align*}
    \underbrace{
    \matr{
      | & |   &  | \\
      X_0 & X_1 & X_2\\
    |   & |   & | \\
    }}_{\abD} 
    = 
    \underbrace{
    \matr{
      | & |   &  | \\
      U_0 & U_1 & U_2\\
    |   & |   & | \\
    }}_{\bQ} 
    \cdot
    \underbrace{
    \matr{
        1 & 5.843 & 3.759\\
        0 & 1 & 1.858\\
        0 & 0 & 1
    }}_{\bR} 
\end{align*}
$\bQ \in \setR^{150\times 3}$ 
and $\bDelta$, the squared norms of the basis vectors, and
its inverse are
\begin{align*}
    \bDelta & = 
    \matr{
   150 & 0 & 0\\
   0 & 102.17 & 0\\
   0 & 0 & 111.35 } &
   \bDelta^{-1} = &
    \matr{
        0.00667 & 0 & 0\\
        0 & 0.00979 & 0\\
    0 & 0 & 0.00898 }
\end{align*}
\end{frame}

\begin{frame}{QR-Factorization and Geometric Approach}
\framesubtitle{Example}
We can use back-substitution to solve for $\abw$, as follows
\begin{gather*}
\bR\abw  = \bDelta^{-1} \bQ^T Y\\
    \matr{
        1 & 5.843 & 3.759\\
        0 & 1 & 1.858\\
        0 & 0 & 1
    } \matr{w_0\\w_1\\w_2}  = \matr{1.1987\\ 0.7538\\ 0.4499}
\end{gather*}
Back-substitution starts with $w_2$:
\begin{align*}
    w_2 = 0.4499
\end{align*}
Next, $w_1$ is given as:
\begin{align*}
    & w_1 + 1.858 \cdot w_2 = 0.7538\\
    \implies & w_1 = 0.7538 - 0.8358 = -0.082
\end{align*}
Finally, $w_0$ can be computed as
\begin{align*}
    & w_0 + 5.843 \cdot w_1 + 3.759 \cdot w_2 = 1.1987\\
    \implies & w_0 = 1.1987 + 0.4786 - 1.6911 = -0.0139
\end{align*}
\end{frame}

\begin{frame}{QR-Factorization and Geometric Approach}
\framesubtitle{Example}
The multiple regression model is given as
\begin{align*}
       \hY & = -0.014 \cdot X_0 -0.082 \cdot X_1 + 0.45 \cdot X_2
       %\label{eq:reg:linear:multiple_sol}
\end{align*}
%which matches the model in \cref{ex:reg:linear:multiple}. 

It is also instructive to construct the new basis vectors $U_0, U_1,
\cdots, U_d$ in terms of
$X_0, X_1, \cdots, X_d$. 
Since $\abD = \bQ \bR$, we have $\bQ = \abD \bR^{-1}$.
The inverse of $\bR$ is also upper-triangular, and is given as
\begin{align*}
    \bR^{-1} = \matr{
1 & -5.843 & 7.095\\
0 &   1  & -1.858\\
0 &   0  &     1
}
\end{align*}
$\bQ$ can be written as:
\begin{align*}
    \underbrace{
    \matr{
      | & |   &  | \\
      U_0 & U_1 & U_2\\
    |   & |   & | \\
    }}_{\bQ} &
    = 
    \underbrace{
    \matr{
      | & |   &  | \\
      X_0 & X_1 & X_2\\
    |   & |   & | \\
}}_{\abD} 
\underbrace{
\matr{
1 & -5.843 & 7.095\\
0 &   1  & -1.858\\
0 &   0  &     1
}}_{\bR^{-1}}
\end{align*}
\end{frame}

\begin{frame}{QR-Factorization and Geometric Approach}
\framesubtitle{Example}
This expression allows us to
\begin{align*}
      U_0 & = X_0\\
      U_1 & = -5.843 \cdot X_0 + X_1\\
      U_2 & = 7.095 \cdot X_0 - 1.858 \cdot X_1 + X_2
\end{align*}
The scalar projections of $Y$ onto $U_i$ are:
\begin{align*}
   \proj_{U_0}\!(Y) & = 1.199 &
   \proj_{U_1}\!(Y) & = 0.754 &
   \proj_{U_2}\!(Y) & = 0.45 
\end{align*}
The fitted response vector $\hY$ is given as:
\begin{align*}
    \hY & = \proj_{U_0}\!(Y) \cdot U_0 + \proj_{U_1}\!(Y) \cdot U_1 + 
    \proj_{U_2}\!(Y) \cdot U_2\\
        & = 1.199 \cdot X_0 + 0.754 \cdot (-5.843 \cdot X_0 + X_1) +
        0.45\cdot (7.095 \cdot X_0 - 1.858 \cdot X_1 + X_2)\\
        & = (1.199 - 4.406 + 3.193) \cdot X_0 + (0.754 - 0.836) \cdot
        X_1 + 0.45
        \cdot X_2\\
        & = -0.014 \cdot X_0 -0.082 \cdot X_1 + 0.45 \cdot X_2
\end{align*}
%which matches \cref{eq:reg:linear:multiple_sol}.
% Further the mean values of all the
% attributes are
% \begin{align*}
%     \mu_Y & = 1.199 & \mu_{X_1} & = 5.843 & \mu_{X_2} & = 3.759\\
% \end{align*}
% Therefore, the bias term can also be verified as
% \begin{align*}
%     b & = w_0 = 1.199 - 0.754 \cdot 5.843 - 0.45 \cdot 3.759\\
%     & = 1.199 - 4.406 - 1.692\\
% \end{align*}
\end{frame}
%\label{ex:reg:linear:multiple_qr}
%\end{example}
%
%\subsection{Multiple Regression: Stochastic Gradient Descent}
%
\begin{frame}{Multiple Regression: Stochastic Gradient Descent}
Instead of using the QR-factorization approach to exactly solve the
multiple regression problem, we can also employ the simpler stochastic
gradient algorithm. 
%\index{multiple regression!gradient descent}
%Consider the SSE objective given in
%\cref{eq:reg:linear:multiple_SSEobj} (multiplied by 1/2):
%\begin{align*}
%    \min_{\abw} SSE = \frac{1}{2} \lB(Y^T Y - 2 \abw^T (\abD^T Y) +
%    \abw^T (\abD^T \abD) \abw\rB)
%    \label{eq:reg:linear:multiple_obj_gd}
%\end{align*}
The gradient of the SSE objective is given as
\begin{align*}
    \grad_{\abw} = 
    \frac{\partial}{\partial \abw} SSE & = -\abD^T Y + (\abD^T\abD) \abw
\end{align*}
%
From an initial weight vector 
$\abw^{\;0}$, we update $\abw$ as:
\begin{align*}
    \abw^{\;t+1}  &= \abw^{\;t} - \eta \cdot \grad_{\abw}
    = \abw^{\;t} + \eta \cdot \abD^T(Y - \abD \cdot \abw^{\;t})
\end{align*}
where $\abw^{t}$ is the estimate of the weight vector at step $t$.
%
%\index{multiple regression!stochastic gradient descent}
%\index{multiple regression!SGD}
We update the weight vector by
considering only one (random) point at each iteration. 
%Restricting \cref{eq:reg:linear:multiple_obj_gd} to a single point
%$\abx_k$ in the training data $\abD$,
%the gradient at the
%point $\abx_k$ is given as
%\begin{align*}
%    \grad_{\abw}(\abx_k) = -\abx_k y_k + \abx_k \abx_k^T \abw = 
%    - (y_k - \abx_k^T\abw) \abx_k
%\end{align*}
%Therefore, the stochastic gradient update rule is given as
\begin{align*}
    \abw^{\;t+1}  &= \abw^{\;t} - \eta \cdot \grad_{\abw}(\abx_k)\\
    & = \abw^{\;t} + \eta \cdot (y_k - \abx_k \cdot \abw^{\;t}) \cdot
    \abx_k
\end{align*}
\end{frame}
%
\begin{frame}{Multiple Regression: SGD Algorithm}
\begin{tightalgo}[H]{\textwidth-18pt}
    \SetKwInOut{Algorithm}{\textsc{Multiple Regression: SGD} ($\bD, Y, \eta, \epsilon$)}
\Algorithm{} 
$\abD \assign \bigl(\bone \quad \bD\bigr)$ \tcp*[h]{augment data}\;
$t \assign 0$ \tcp*[h]{step/iteration counter}\; 
$\abw^{\;t} \assign $ random vector in $\setR^{d+1}$ \tcp*[h]{initial weight vector}\;
\Repeat{$\norm{\bw^{\;t}-\bw^{\;t-1}} \le \epsilon$}
{%
    \ForEach{$k = 1,2,\cdots,n$ (in random order)}{%
        $\grad_{\abw}(\abx_k) \assign - (y_k - \abx_k^T\abw^{\;t}) \cdot \abx_k$ 
        \tcp*[h]{compute gradient at $\abx_k$}\;
        $\abw^{\;t+1} \assign \abw^{\;t} - \eta \cdot \grad_{\abw}(\abx_k)$
        \tcp*[h]{update estimate for $w_k$}\;
    }
    $t \assign t+1$\;
}%
%\caption{Multiple Regression: Stochastic Gradient Descent}
%\label{alg:reg:linear:l2_regression}
\end{tightalgo}
\end{frame}
%
%%l2-regression
%% python3 ./l2-regression.py iris-plwsl.dat iris-plwsl.dat 0 0.001 0.0001 0 123
%% ['./l2-regression.py', 'iris-plwsl.dat', 'iris-plwsl.dat', '0', '0.001', '0.0001', '0', '123']
%
%% norm: L2, L1 0.21262997275242174 0.5441481018366252
%% weights [-0.08264871  0.45358183  0.00791756]
%% accuracy, sse 0.0 6.3299040381982445
%
% % python3 ./l2-regression.py iris-plwsl.dat iris-plwsl.dat 0 0.001 0.0001 0 12345
%% norm: L2, L1 0.20856480315392606 0.557817440378845
%% weights [-0.07786804  0.44893122 -0.03101817]
%% accuracy, sse 0.0 6.180637811060918
%
%
%\cref{alg:reg:linear:l2_regression} shows the pseudo-code for the
%stochastic gradient descent algorithm for multiple regression. After
%augmenting the data matrix, in each iteration it updates the weight
%vector by considering the gradient at each point in random order. The
%method stops when the weight vector converges based on the tolerance
%$\epsilon$. 
%
%\begin{example}[Multiple Regression: SGD]
\begin{frame}{Multiple Regression: SGD}
\framesubtitle{Example}
    %We continue \cref{ex:reg:linear:multiple_qr} 
    Multiple regression of 
{\tt sepal length} ($X_1$) and {\tt petal
    length} ($X_2$) on the response attribute {\tt petal width} ($Y$)
for the Iris dataset with $n=150$ points. 

\medskip
    
    Using
    the exact approach the multiple regression model was given as
    \begin{align*}
        \hY = -0.014 \cdot X_0 -0.082 \cdot X_1 + 0.45 \cdot X_2 \end{align*}
    Using SGD we obtain the following model with
    $\eta=0.001$ and $\epsilon=0.0001$:
    \begin{align*}
        \hY = -0.031 \cdot X_0 -0.078 \cdot X_1 + 0.45 \cdot X_2
    \end{align*}
    The results from the SGD approach are essentially the same as the
    exact method, with a slight difference in the bias term.

    \medskip

    The SSE value for the exact
    method is 6.179, whereas for SGD it is 6.181.
\end{frame}
%\end{example}
%
%
%\section{Ridge Regression}
%\index{regression!ridge regression}
%\index{ridge regression}
%\index{linear regression!ridge regression}
%\index{L$_2$ regularized regression|see{ridge regression}}
%\index{L$_2$ regularization!linear regression}
%\index{L$_2$ regularization}

\ifdefined\wox \begin{frame} \titlepage \end{frame} \fi

\begin{frame}{Ridge Regression}
For linear regression, 
$\hY$ lies in the span of the column vectors comprising the augmented 
data matrix $\abD$. 

\medskip

Often the data is noisy and
uncertain, and, therefore, instead of fitting the model to the data
exactly, it may be better to fit a more robust model.

\medskip

Regularization constrains the solution vector
$\abw$ to have a small norm. 


\medskip

Besides minimizing $\norm{Y - \hY}^2$, we add a
regularization term ($\norm{\abw}^2$):
\begin{empheq}[box=\tcbhighmath]{align*}
    \min_{\abw}\;\; J(\abw) = \norm{Y - \hY}^2 + \alpha \cdot \norm{\abw}^2
    = \norm{Y - \abD\abw}^2 + \alpha \cdot \norm{\abw}^2
    %\label{eq:reg:linear:ridgeobj}
\end{empheq}
$\alpha \ge 0$ controls the
tradeoff between minimizing the squared
norm of the weight vector and the squared error. 

\end{frame}

\begin{frame}{Ridge Regression}

%Recall that $\norm{\abw}^2 = \sum^{d}_{i=1} w_i^2$ is the $L_2$-norm of
%$\abw$. For this reason ridge regression is also called $L_2$ regularized
%regression.
%When $\alpha = 0$,
%there is no regularization, but as $\alpha$ increases there is more
%emphasis on minimizing the regression coefficients.
%
%The solve the new regularized objective we differentiate
%\cref{eq:reg:linear:ridgeobj} 

We differentiate w.r.t. $\abw$ and set the results to
$\bzero$ to obtain
%\begin{align*}
%    \frac{\partial}{\partial \abw} J(\abw) & = 
%    \frac{\partial}{\partial \abw} \lB\{ 
%    \norm{Y - \abD\abw}^2 + \alpha \cdot \norm{\abw}^2  \rB\} = \bzero \notag\\
%    \implies & \frac{\partial}{\partial \abw} \lB\{ Y^TY - 2 \abw^T (\abD^TY) + \abw^T
%    (\abD^T\abD) \abw + \alpha \cdot \abw^T\abw \rB\} = \bzero \notag\\
%    \implies & -2\abD^T Y + 2 (\abD^T\abD) \abw + 2\alpha \cdot \abw = \bzero
%    \label{eq:reg:linear:ridge_w1}\\
%    \implies & (\abD^T\abD + \alpha\cdot\bI) \abw = \abD^T Y
%\end{align*}
%Therefore, the optimal solution is
\begin{align*}
   \abw = (\abD^T\abD + \alpha\cdot\bI)^{-1} \abD^T Y
    %\label{eq:reg:linear:ridge_w}
\end{align*}
%where $\bI \in \setR^{(d+1) \times (d+1)}$ is the identity matrix.
The matrix $(\abD^T\abD + \alpha\cdot\bI)$ is always invertible (or
non-singular) for
$\alpha >
0$ even if $\abD^T\abD$ is not invertible (or singular). 

\medskip

If
$\lambda_i$ is an eigenvalue of $\abD^T\abD$, then $\lambda_i + \alpha$ is an
eigenvalue of $(\abD^T\abD + \alpha\cdot\bI)$. Since $\abD^T\abD$ is positive
semi-definite it has non-negative eigenvalues. 
Even if an $\lambda_i = 0$, the
corresponding eigenvalue of $(\abD^T\abD + \alpha\cdot \bI)$ is
$\lambda_i+\alpha = \alpha > 0$. 
\medskip

Regularized regression is called
{\em ridge regression} because it
adds a ``ridge'' along the main diagonal of the $\abD^T\abD$ matrix, i.e.,
the solution depends on $(\abD^T\abD +
\alpha\cdot\bI)$. 

\medskip

If we
choose a small positive $\alpha$ we are always guaranteed a solution.

\end{frame}
%
%\begin{figure}[t!]
%\vspace{0.6in}
%\caption{Scatterplot: {\tt petal length} ($X$) versus {\tt petal
%width} ($Y$). Ridge regression lines for $\alpha = 0, 10, 100$.}
%\label{fig:reg:linear:iris-ridge}
%\end{figure}\vspace*{12pt}
%
%
%% zaki@Rpoly128691:~/research/DataMiningBook/dmbook-2ed/REG/linear/figs (master)
%% $ python ./ridge-regression.py iris-plw.dat iris-plw.dat 0 0.001 0.001 0
%% ['./ridge-regression.py', 'iris-plw.dat', 'iris-plw.dat', '0', '0.001', '0.001', '0']
%% 150 2
%% classes [0.10000000000000001, 0.20000000000000001, 0.29999999999999999, 0.40000000000000002, 0.5, 0.59999999999999998, 1.0, 1.1000000000000001, 1.2, 1.3, 1.3999999999999999, 1.5, 1.6000000000000001, 1.7, 1.8, 1.8999999999999999, 2.0, 2.1000000000000001, 2.2000000000000002, 2.2999999999999998, 2.3999999999999999, 2.5]
%% weights [ 0.41641913 -0.36651405] 0.307737439074
%% accuracy, sse 0.0 6.34349194792
%
%
%% zaki@Rpoly128691:~/research/DataMiningBook/dmbook-2ed/REG/linear/figs (master)
%% $ python ./ridge-regression.py iris-plw.dat iris-plw.dat 10 0.001 0.001 0
%% ['./ridge-regression.py', 'iris-plw.dat', 'iris-plw.dat', '10', '0.001', '0.001', '0']
%% 150 2
%% classes [0.10000000000000001, 0.20000000000000001, 0.29999999999999999, 0.40000000000000002, 0.5, 0.59999999999999998, 1.0, 1.1000000000000001, 1.2, 1.3, 1.3999999999999999, 1.5, 1.6000000000000001, 1.7, 1.8, 1.8999999999999999, 2.0, 2.1000000000000001, 2.2000000000000002, 2.2999999999999998, 2.3999999999999999, 2.5]
%% weights [ 0.38824998 -0.24434588] 0.210442956661
%% accuracy, sse 0.0 6.75137154825
%
%% zaki@Rpoly128691:~/research/DataMiningBook/dmbook-2ed/REG/linear/figs (master)
%% $ python ./ridge-regression.py iris-plw.dat iris-plw.dat 100 0.001 0.001 0
%% ['./ridge-regression.py', 'iris-plw.dat', 'iris-plw.dat', '100', '0.001', '0.001', '0']
%% 150 2
%% classes [0.10000000000000001, 0.20000000000000001, 0.29999999999999999, 0.40000000000000002, 0.5, 0.59999999999999998, 1.0, 1.1000000000000001, 1.2, 1.3, 1.3999999999999999, 1.5, 1.6000000000000001, 1.7, 1.8, 1.8999999999999999, 2.0, 2.1000000000000001, 2.2000000000000002, 2.2999999999999998, 2.3999999999999999, 2.5]
%% weights [ 0.32835923 -0.02131573] 0.108274143217
%% accuracy, sse 0.0 9.97083562118
%
%
%
%\begin{example}[Ridge Regression]
%    \label{ex:reg:linear:ridge}

\begin{frame}{Ridge Regression}
\framesubtitle{Example}

Given  
{\tt sepal length} ($X_1$) and {\tt petal
    length} ($X_2$) on the response attribute {\tt petal width} ($Y$)
for the Iris dataset with $n=150$ points, we want to learn the ridge regression. 

\centering
\psset{unit=0.25in}
\scalebox{0.75}{%
\psset{viewpoint=20 10 10 rtp2xyz,Decran=30}
\psset{lightsrc=viewpoint}
\psset{incolor=white}
\psset{opacity=0.2}
\psset{fillcolor=white}
\begin{pspicture}(3,-5)(8,7)
    \axesIIID[axisnames={X_1, X_2, Y}](0,0,0)(9,8,3)
\psset{linewidth=0pt, dotsize=0.3}
\psset{dotstyle=Bo,fillcolor=white}
\input{REG/linear/figs/iris-3d-points.tex}
\end{pspicture}
}
\end{frame}

\begin{frame}{Ridge Regression}
\framesubtitle{Example}
    The uncentered scatter matrix is given as
    \begin{align*}
        \abD^T\abD = \matr{
        150.0 &  563.8\\
        563.8 & 2583.0\\
        }
    \end{align*}
    %Using \cref{eq:reg:linear:ridge_w} 
We obtain different lines of best
    fit for different values of the regularization constant $\alpha$:
\begin{small}
    \begin{align*}
        \alpha = 0: & \hY = -0.367 + 0.416 \cdot X, 
        & \norm{\abw}^2 & = \norm{(-0.367, 0.416)^T}^2 = 0.308, 
        & SSE & = 6.34\\
        \alpha = 10: & \hY = -0.244 + 0.388 \cdot X,
        & \norm{\abw}^2 & = \norm{(-0.244, 0.388)^T}^2 = 0.210,
       & SSE & = 6.75 \\
       \alpha = 100: & \hY = -0.021 + 0.328 \cdot X,
       & \norm{\abw}^2 & = \norm{(-0.021, 0.328)^T}^2 = 0.108,
        & SSE & = 9.97
    \end{align*}
\end{small}
    % \enlargethispage{20pt}
    %\cref{fig:reg:linear:iris-ridge} 
\end{frame}

\begin{frame}{Ridge Regression}
\framesubtitle{Example}
    As $\alpha$ increases there is more emphasis on
    minimizing the squared norm of $\abw$. 

	\medskip

	Since $\norm{\abw}^2$ is more
    constrained as $\alpha$ increases, the fit of the model decreases,
    as seen from the increase in SSE values.

	\medskip

    \centering
\scalebox{0.7}{
    \psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray,arrowscale=2,PointName=none}
    \psset{xAxisLabel=$X$: petal length,yAxisLabel= $Y$: petal width,
        xAxisLabelPos={c,-0.4in},yAxisLabelPos={-0.5in,c}}
    \psgraph[tickstyle=bottom,Dx=1.0,Dy=0.5,Oy=-0.5,subticks=2]{->}(0.0,-0.5)(7.5,3){4in}{2.5in}%
    \dataplot[plotstyle=dots,showpoints=true]{\dataPLW}
    \psdot[dotstyle=*,dotscale=2](3.76,1.20)
    %\psline[linestyle=dashed](3.76,-0.5)(3.76,1.20)
    \psplot[plotstyle=line,linewidth=1pt]{0}{7.5}{x 0.416 mul -0.365
        add} %a=0
    \psplot[plotstyle=line,linewidth=1pt,linestyle=dashed]{0}{7.5}{x
        0.388 mul -0.244 add} %a = 10
    \psplot[plotstyle=line,linecolor=gray,linewidth=1pt]{0}{7.5}{x 0.328
        mul -0.021 add} %a=100
   % \psplot[algebraic,plotstyle=line,linecolor=gray,linewidth=2pt]{0}{7.5}%
   % {0.214 + 0.0572 * x^2}
    \rput(7.9,2.9){$\alpha=0$}
    \rput(7.95,2.7){$\alpha=10$}
    \rput(8,2.45){$\alpha=100$}
    \endpsgraph
}
\end{frame}
%\end{example}
%
%%\begin{figure}[t!]
%%    \centering
%%    \psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray,arrowscale=2,PointName=none}
%%    \psset{xAxisLabel=$X$: petal length,yAxisLabel= $Y$: petal width,
%%        xAxisLabelPos={c,-0.4in},yAxisLabelPos={-0.5in,c}}
%%    \psgraph[tickstyle=bottom,Dx=1.0,Dy=0.5,Oy=-0.5,subticks=2]{->}(0.0,-0.5)(7.5,3){4in}{2.5in}%
%%    \dataplot[plotstyle=dots,showpoints=true]{\dataPLW}
%%    \psdot[dotstyle=*,dotscale=2](3.76,1.20)
%%    %\psline[linestyle=dashed](3.76,-0.5)(3.76,1.20)
%%    \psplot[plotstyle=line,linewidth=1pt]{0}{7.5}{x
%%    0.408 mul -0.333 add} %a = 10, unpenalized
%%    \psplot[plotstyle=line,linewidth=1pt,linestyle=dashed]{0}{7.5}{x
%%    0.388 mul -0.244 add} %a = 10, penalized
%%   % \psplot[algebraic,plotstyle=line,linecolor=gray,linewidth=2pt]{0}{7.5}%
%%   % {0.214 + 0.0572 * x^2}
%%    \rput(7.9,2.9){unpenalized}
%%    \rput(7.95,2.7){penalized}
%%    \endpsgraph
%%\vspace{0.6in}
%%\caption{Scatterplot: {\tt petal length} ($X$) versus {\tt petal
%%width} ($Y$). Ridge regression lines for penalized and unpenalized bias
%%term, with $\alpha =10$.}
%%\label{fig:reg:linear:iris-ridge-unpenalized}
%%\end{figure}\vspace*{12pt}
%
%
%% zaki@Rpoly128691:~/research/DataMiningBook/dmbook-2ed/REG/linear/figs (master)
%% $ python ./ridge-regression.py iris-plw.dat iris-plw.dat 0 0.001 0.001 1
%% ['./ridge-regression.py', 'iris-plw.dat', 'iris-plw.dat', '0', '0.001', '0.001', '1']
%% 150 2
%% classes [0.10000000000000001, 0.20000000000000001, 0.29999999999999999, 0.40000000000000002, 0.5, 0.59999999999999998, 1.0, 1.1000000000000001, 1.2, 1.3, 1.3999999999999999, 1.5, 1.6000000000000001, 1.7, 1.8, 1.8999999999999999, 2.0, 2.1000000000000001, 2.2000000000000002, 2.2999999999999998, 2.3999999999999999, 2.5]
%% weights [ 0.41641913] 0.173404893733
%% bias -0.366514045217
%% accuracy, sse 0.0 6.34349194792
%
%% zaki@Rpoly128691:~/research/DataMiningBook/dmbook-2ed/REG/linear/figs (master)
%% $ python ./ridge-regression.py iris-plw.dat iris-plw.dat 10 0.001 0.001 1
%% ['./ridge-regression.py', 'iris-plw.dat', 'iris-plw.dat', '10', '0.001', '0.001', '1']
%% 150 2
%% classes [0.10000000000000001, 0.20000000000000001, 0.29999999999999999, 0.40000000000000002, 0.5, 0.59999999999999998, 1.0, 1.1000000000000001, 1.2, 1.3, 1.3999999999999999, 1.5, 1.6000000000000001, 1.7, 1.8, 1.8999999999999999, 2.0, 2.1000000000000001, 2.2000000000000002, 2.2999999999999998, 2.3999999999999999, 2.5]
%% weights [ 0.40763139] 0.16616335191
%% bias -0.333483859495
%% accuracy, sse 0.0 6.37931353516
%
%% zaki@Rpoly128691:~/research/DataMiningBook/dmbook-2ed/REG/linear/figs (master)
%% $ python ./ridge-regression.py iris-plw.dat iris-plw.dat 100 0.001 0.001 1
%% ['./ridge-regression.py', 'iris-plw.dat', 'iris-plw.dat', '100', '0.001', '0.001', '1']
%% 150 2
%% classes [0.10000000000000001, 0.20000000000000001, 0.29999999999999999, 0.40000000000000002, 0.5, 0.59999999999999998, 1.0, 1.1000000000000001, 1.2, 1.3, 1.3999999999999999, 1.5, 1.6000000000000001, 1.7, 1.8, 1.8999999999999999, 2.0, 2.1000000000000001, 2.2000000000000002, 2.2999999999999998, 2.3999999999999999, 2.5]
%% weights [ 0.34256811] 0.117352909877
%% bias -0.0889326688387
%% accuracy, sse 0.0 8.8733924649
%
%
%
\begin{frame}{Ridge Regression: Unpenalized Bias Term}
%\paragraph{Unpenalized Bias Term}

    Often in $L_2$ regularized regression we do not want to penalize the
    bias term $w_0$, since it simply provides the intercept
    information.

    \medskip

    Consider the new regularized
    objective where $\bw = (w_1, w_2, \cdots, w_d)^T$ without $w_0$:
\begin{align*}
    \min_\bw\;\; J(\bw) & = 
    \norm{Y - w_0 \cdot \bone - \bD\bw}^2 + \alpha \cdot
    \norm{\bw}^2\\
    %\label{eq:reg:linear:l2_unpenalized_bias}\\
    & = \biggl\lVert Y - w_0 \cdot \bone - \sum_{i=1}^d w_i \cdot
        X_i \biggr\rVert^2 +
    \alpha \cdot \lB( \sum^{d}_{i=1} w_i^2 \rB) \notag
\end{align*}
%    
%    Recall from \cref{eq:reg:linear:multiple_b} that the bias $w_0 = b$ is given as
%    \begin{align*}
%        w_0 = b = \mu_Y - \sum_{i=1}^d w_i \cdot \mu_{X_i} = \mu_Y -
%        \bmu^T \bw 
%    \end{align*}
%    where 
%    $\bmu = (\mu_{X_1}, \mu_{X_2}, \cdots, \mu_{X_d})^T$ is the
%    multivariate mean of (unaugmented) $\bD$.
%    Substituting $w_0$ into the new $L_2$ objective in
%    \cref{eq:reg:linear:l2_unpenalized_bias}, we get
%    \begin{align*}
%     \min_\bw\;\; J(\bw) & =
%     \norm{Y - w_0 \cdot \bone - \bD\bw}^2 + \alpha \cdot \norm{\bw}^2\\
%     & = \norm{Y - (\mu_Y - \bmu^T\bw) \cdot \bone - \bD\bw}^2 + \alpha \cdot \norm{\bw}^2\\
%     & = \norm{(Y - \mu_Y \cdot \bone) - (\bD - \bone \bmu^T)\bw}^2 + \alpha \cdot \norm{\bw}^2
%    \end{align*}
%
Therefore, we have
\begin{align*}
    \tcbhighmath{
     \min_\bw\;\; J(\bw) = \norm{\mY - \mbD\;\bw}^2 + \alpha \cdot
 \norm{\bw}^2}
 %\label{eq:reg:linear:ridgeobj_unpenalizedbias}
\end{align*}
    where $\mY = Y - \mu_Y \cdot \bone$ 
    is the centered $Y$, and $\mbD = \bD - \bone \bmu^T$ is the
    centered $\bD$.

	\medskip

 We can exclude $w_0$ from the $L_2$ regularization
    objective by centering the response vector and the
    unaugmented data matrix. 
\end{frame}
%
%
%    % In the development above, we penalized the entire
%    % weight vector $\bw = (w_0, w_1, w_2, \cdots, w_d)^T$. A simple trick
%    % allows one to exclude the bias term $w_0$. 
%    % First, we do not augment the
%    % data by adding $X_0 = \bone$, but rather we keep the original data
%    % $\bD$ comprising the independent attributes $X_1, X_2, \cdots, X_d$.
%    % Next, we center the data matrix $\bD$ by subtracting the mean $\bmu$
%    % from each point, and we also center the response vector $Y$ by
%    % subtracting its mean $\mu_Y$. That is
%    % \begin{align*}
%    %     \bD & = \bD - \bone\cdot \bmu^T & Y & Y - \mu_Y
%    % \end{align*}
%    % where 
%
%\begin{example}[Ridge Regression: Unpenalized Bias]
%    \label{ex:reg:linear:ridge_unpenalized_bias}
\begin{frame}{Ridge Regression: Unpenalized Bias}
\framesubtitle{Example}
%    We continue from \cref{ex:reg:linear:ridge}. 
When we do not penalize
    $w_0$, we obtain the following 
    lines of best
    fit for different values of the regularization constant $\alpha$:
    \begin{align*}
        \alpha = 0: & \hY = -0.365 + 0.416 \cdot X 
        & w_0^2+w_1^2 & = 0.307 
        & SSE & = 6.34\\
        \alpha = 10: & \hY = -0.333 + 0.408 \cdot X
        & w_0^2 +w_1^2 & = 0.277
       & SSE & = 6.38 \\
       \alpha = 100: & \hY = -0.089 + 0.343 \cdot X
       & w_0^2+w_1^2 & = 0.125
       & SSE & = 8.87
    \end{align*}

    %From \cref{ex:reg:linear:ridge}, 
We observe that for $\alpha=10$,
    when we penalize $w_0$, we obtain the following model:
    \begin{align*}
        \alpha = 10: & \hY = -0.244 + 0.388 \cdot X
        & w_0^2+w_1^2 & = 0.210
       & SSE & = 6.75
    \end{align*}
    As expected, we obtain a higher bias term when we
    do not penalize $w_0$.

    % \cref{fig:reg:linear:iris-ridge-unpenalized} contrasts the regularized regression
    % lines with and without penalization for $w_0$ for $\alpha=10$.
\end{frame}
%\end{example}
%
\begin{frame}{Ridge Regression: Stochastic Gradient Descent}
%\subsection{Ridge Regression: Stochastic Gradient Descent}
%
Instead of inverting the matrix $(\abD^T\abD + \alpha \cdot \bI)$ as
called for in the exact ridge regression solution, %in 
%\cref{eq:reg:linear:ridge_w}, 
we can employ the stochastic
gradient descent algorithm.

\medskip

The gradient of $\abw$
% given in \cref{eq:reg:linear:ridge_w1}, 
multiplied by 1/2 for convenience is:
\begin{align*}
    \grad_{\abw} = \frac{\partial}{\partial \abw} J(\abw) = 
     -\abD^T Y + (\abD^T\abD) \abw + \alpha \cdot \abw
\end{align*}
Using (batch) gradient descent, we can iteratively compute $\abw$ as follows
\begin{align*}
    \abw^{\;t+1}  &= \abw^{\;t} - \eta \cdot \grad_{\abw}
    = (1 - \eta \cdot \alpha) \abw^{\;t} + \eta \cdot \abD^T(Y - \abD
    \cdot \abw^{\;t})
\end{align*}
%
%\index{ridge regression!stochastic gradient descent}
%\index{ridge regression!SGD}
In SGD, we update the weight vector by
considering only one (random) point at each time: 
%Restricting \cref{eq:reg:linear:ridge_obj_gd} to a single point
%$\abx_k$, the gradient at the
%point $\abx_k$ is given as
%\begin{align*}
%    \grad_{\abw}(\abx_k) = -\abx_k y_k + \abx_k \abx_k^T \abw +
%    \frac{\alpha}{n} \abw = 
%    - (y_k - \abx_k^T\abw) \abx_k + \frac{\alpha}{n} \abw
%    \label{eq:reg:linear:ridge_obj_gd}
%\end{align*}
%Here, we scale the regularization constant 
%$\alpha$ by dividing it by $n$, the number of
%points in the training data, 
%since the original ridge value $\alpha$ is for all the $n$ points,
%whereas we are now considering only one point at a time.
%Therefore, the stochastic gradient update rule is given as
\begin{align*}
    \abw^{\;t+1}  = \abw^{\;t} - \eta \cdot \grad_{\abw}(\abx_k)
     = \lB(1 - \frac{\eta \cdot \alpha}{n}\rB) \abw^{\;t} 
    + \eta \cdot (y_k - \abx_k \cdot \abw^{\;t}) \cdot
    \abx_k
\end{align*}
\end{frame}
%
\begin{frame}{Ridge Regression: SGD Algorithm}
\begin{tightalgo}[H]{\textwidth-18pt}
    \SetKwInOut{Algorithm}{\textsc{Ridge Regression: SGD} ($\bD, Y, \eta, \epsilon$)}
\Algorithm{} 
$\abD \assign \bigl(\bone \quad \bD\bigr)$ \tcp*[h]{augment data}\;
$t \assign 0$ \tcp*[h]{step/iteration counter}\; 
$\abw^{\;t} \assign $ random vector in $\setR^{d+1}$ \tcp*[h]{initial weight vector}\;
\Repeat{$\norm{\bw^{\;t}-\bw^{\;t-1}} \le \epsilon$}
{%
    \ForEach{$k = 1,2,\cdots,n$ (in random order)}{%
        $\grad_{\abw}(\abx_k) \assign - (y_k - \abx_k^T\abw^{\;t}) \cdot
        \abx_k +
        \frac{\alpha}{n} \cdot \abw$ 
        \tcp*[h]{gradient at $\abx_k$}\;
        $\abw^{\;t+1} \assign \abw^{\;t} - \eta \cdot \grad_{\abw}(\abx_k)$
        \tcp*[h]{update estimate for $w_k$}\;
    }
    $t \assign t+1$\;
}%
% \vspace{-0.1in}
%\caption{Ridge Regression: Stochastic Gradient Descent}
%\label{alg:reg:linear:ridge_regression_sgd}
\end{tightalgo}
\end{frame}

%
%%l2-regression
%% python3 ./l2-regression.py iris-plwsl.dat iris-plwsl.dat 0 0.001 0.0001 0 123
%% ['./l2-regression.py', 'iris-plwsl.dat', 'iris-plwsl.dat', '0', '0.001', '0.0001', '0', '123']
%
%% norm: L2, L1 0.21262997275242174 0.5441481018366252
%% weights [-0.08264871  0.45358183  0.00791756]
%% accuracy, sse 0.0 6.3299040381982445
%
% % python3 ./l2-regression.py iris-plwsl.dat iris-plwsl.dat 0 0.001 0.0001 0 12345
%% norm: L2, L1 0.20856480315392606 0.557817440378845
%% weights [-0.07786804  0.44893122 -0.03101817]
%% accuracy, sse 0.0 6.180637811060918
%
%
%\cref{alg:reg:linear:ridge_regression_sgd} shows the pseudo-code for the
%stochastic gradient descent algorithm for ridge regression. After
%augmenting the data matrix, in each iteration it updates the weight
%vector by considering the gradient at each point in random order. The
%method stops when the weight vector converges based on the tolerance
%$\epsilon$. The code is for the penalized bias case. It is easy to adapt
%it for unpenalized bias by centering the unaugmented data
%matrix and the response variable.
%
%% python3 ./l2-regression.py iris-plw.dat iris-plw.dat 0 0.001 0.0001 0 12345
%% norm: L2, L1 0.30421791867231 0.7785983170359354
%% weights [ 0.41286024 -0.36573808]
%% accuracy, sse 0.0 6.373183862600599
%
%% python3 ./l2-regression.py iris-plw.dat iris-plw.dat 10 0.001 0.0001 0 12345
%% norm: L2, L1 0.2098155552639997 0.6318069638111246
%% weights [ 0.3874071  -0.24439986]
%% accuracy, sse 0.0 6.759539526828374
%
%% python3 ./l2-regression.py iris-plw.dat iris-plw.dat 100 0.001 0.0001 0 12345
%% norm: L2, L1 0.10758845553021518 0.34892926948217245
%% weights [ 0.32729238 -0.02163689]
%% accuracy, sse 0.0 10.042870236515677
%
%\enlargethispage{1\baselineskip}
%\begin{example}[Ridge Regression: SGD]
\begin{frame}{Ridge Regression: SGD}
\framesubtitle{Example}
We apply ridge regression on the Iris dataset ($n=150$), using {\tt
petal length} ($X$) as the independent attribute, and {\tt petal width}
($Y$) as the response variable. 

\medskip

Using SGD (with $\eta=0.001$ and
$\epsilon=0.0001$) we obtain different lines of best fit for different
values of the regularization constant $\alpha$:
%, which essentially match
%the results from the exact method in \cref{ex:reg:linear:ridge}:
    \begin{align*}
        \alpha = 0: & \hY = -0.366 + 0.413 \cdot X & SSE_{SGD} & = 6.37 & SSE_{Ridge} & = 6.34 \\
        \alpha = 10: & \hY = -0.244 + 0.387 \cdot X & SSE_{SGD} & = 6.76  & SSE_{Ridge} & = 6.38 \\
       \alpha = 100: & \hY = -0.022 + 0.327 \cdot X & SSE_{SGD} & = 10.04 & SSE_{Ridge} & = 8.87 
    \end{align*}
\end{frame}
%%These values are essentially the same as those from the exact method in
%%\cref{ex:reg:linear:ridge}.
%\end{example}
%
%
%
%\section{Kernel Regression}
%\label{sec:reg:linear:kernel}
%% \index{kernel trick!linear regression}
%\index{regression!kernel trick}
%\index{regression!kernel regression}
%\index{kernel regression}
%\index{kernel ridge regression}

\ifdefined\wox \begin{frame} \titlepage \end{frame} \fi

\begin{frame}{Kernel Regression}
%
Kernel generalizes linear regression to the non-linear
case, i.e., finding a non-linear fit to the data to minimize the squared
error, along with regularization. 

%For this we will adopt the kernel
%trick, i.e., we will show that all relevant operations can be carried
%out via the kernel matrix in input space.
%
$\phi (\bx_i)$ maps the input point $\bx_i$ to the feature
space.
%To avoid explicitly dealing with the bias term, we add the fixed value
%$1$ as the first element $\phi(\bx_i)$ to obtain the {\em augmented
%transformed point} $\aphi(\bx_i)^T = \bigl(1\;\; \phi(\bx_i)^T\bigr)$.
%Let $\abD_\phi$
%denote the {\em augmented dataset in feature space}, comprising the transformed
%points $\aphi(\bx_i)$ for $i=1,2,\cdots,n$.
%The {\em augmented kernel function} in feature space is given as
%\begin{align*}
%    \aK(\bx_i, \bx_j) = \aphi(\bx_i)^T\aphi(\bx_j) = 1 +
%    \phi(\bx_i)^T\phi(\bx_j) = 1 + K(\bx_i, \bx_j)
%\end{align*}
%where $K(\bx_i, \bx_j)$ is a standard, unaugmented kernel function.
%
%Let $Y$ denote the observed response vector.
%Following \cref{eq:reg:linear:multiple_hY},
%we model the predicted response as 
%\begin{align*}
%    \hY = \abD_\phi \abw \label{eq:reg:linear:kernel_hy}
%\end{align*}
%where $\abw$ is the augmented weight vector in feature space. The first
%element of $\abw$ denotes the bias in feature space.
%

\medskip

For regularized regression, we have to
solve the following objective in feature space:
\begin{empheq}[box=\tcbhighmath]{align*}
    \min_{\abw}\;\; J(\abw) = \norm{Y - \hY}^2 + \alpha \cdot \norm{\abw}^2
    = \norm{Y - \abD_\phi\abw}^2 + \alpha \cdot \norm{\abw}^2
    %\label{eq:reg:linear:kernelobj}
\end{empheq}
%where $\alpha \ge 0$ is a regularization constant.
%% Expanding the above objective we obtain
%% \begin{align*}
%%     Y^TY - 2 \abw^T (\abD_\phi^TY) + \abw^T
%%     (\abD_\phi^T\abD_\phi) \abw + \alpha \cdot \abw^T\abw
%% \end{align*}
%
%Taking the derivative of $J(\abw)$ with respect to $\abw$ and setting it
%to the zero vector, we get
%%Following a derivation similar to that in \cref{eq:reg:linear:ridge_w1}, we have
%\begin{align*}
%    \frac{\partial}{\partial \abw} J(\abw) & = 
%    \frac{\partial}{\partial \abw} \lB\{ 
%    \norm{Y - \abD_\phi\abw}^2 + \alpha \cdot \norm{\abw}^2  \rB\} = \bzero \notag\\
%    \implies & \frac{\partial}{\partial \abw} \lB\{ Y^TY - 2 \abw^T
%        (\abD_\phi^TY) + \abw^T
%    (\abD_\phi^T\abD_\phi) \abw + \alpha \cdot \abw^T\abw \rB\} = \bzero \notag\\
%    \implies & -2\abD_\phi^T Y + 2 (\abD_\phi^T\abD_\phi) \abw + 2\alpha
%    \cdot \abw = \bzero \notag\\
%    \implies & \alpha \cdot \abw  = \abD_\phi^T Y - (\abD_\phi^T\abD_\phi) \abw \notag\\
%    \implies & \abw = \abD_\phi^T \lB( \frac{1}{\alpha} \lB(Y - \abD_\phi \abw\rB)\rB) \notag\\
%    \implies & \abw = \abD_\phi^T \bc = \sum_{i=1}^n c_i \cdot \aphi(\bx_i)
%    \label{eq:reg:linear:kernel_w}
%\end{align*}
%where $\bc = (c_1, c_2, \cdots, c_n)^T 
%= \frac{1}{\alpha} (Y - \abD_\phi \abw)$.
%\cref{eq:reg:linear:kernel_w} indicates that the weight vector $\abw$ is 
%a linear combination of the feature points, with $\bc$ specifying the
%mixture coefficients for the points.
%
%Rearranging the terms in the expression for $\bc$, we have
%\begin{align*}
%    \bc & = \frac{1}{\alpha} (Y - \abD_\phi \abw)\\
%    \alpha \cdot \bc & = Y - \abD_\phi \abw
%\end{align*}
%Now, plugging in the form of $\abw$ from \cref{eq:reg:linear:kernel_w}
%we get
%\begin{align*}
%    \alpha \cdot \bc & = Y - \abD_\phi (\abD_\phi^T \bc) \notag\\
%    (\abD_\phi \abD_\phi^T + \alpha \cdot \bI) \bc & = Y \notag\\
%    \bc & = (\abD_\phi \abD_\phi^T + \alpha \cdot \bI)^{-1} Y \notag\\
%\end{align*}
The optimal solution is therefore given as
\begin{align*}
    \tcbhighmath{
    \bc = (\abK + \alpha \cdot \bI)^{-1} Y}
    %\label{eq:reg:linear:kernel_c}
\end{align*}
where $\bI \in \setR^{n\times n}$ is the $n \times n$ identity matrix,
and $\abD_\phi \abD_\phi^T$ is the augmented kernel matrix $\abK$.%, since
%\begin{align*}
%    \abD_\phi \abD_\phi^T & = \lB\{ \aphi(\bx_i)^T\aphi(\bx_{\!j}) \rB\}_{i,j = 1,2,\cdots,n}
%    = \lB \{ \aK(\bx_i, \bx_{\!j}) \rB\}_{i,j = 1,2,\cdots,n} = \abK
%\end{align*}
%
\end{frame}


\begin{frame}{Kernel Regression}
%Putting it all together, 
%we can substitute \cref{eq:reg:linear:kernel_c}
%and \cref{eq:reg:linear:kernel_w} into \cref{eq:reg:linear:kernel_hy} 
%to 
The expression for the
predicted response is:
\begin{align*}
    \hY & = \abD_\phi \abw \notag\\
     & = \abD_\phi \abD_\phi^T \bc \notag\\
     & = \lB(\abD_\phi \abD_\phi^T\rB) \lB(\abK + \alpha \cdot \bI\rB)^{-1} Y \notag\\
     & = \abK \lB(\abK + \alpha \cdot \bI\rB)^{-1} Y
\end{align*}
where $\abK (\abK + \alpha \cdot \bI)^{-1}$ is the {\em kernel hat
    matrix}.
%\index{kernel regression!kernel hat matrix}

\medskip

    $\alpha > 0$ ensures that  the inverse always
    exists, which is another advantage of using (kernel) ridge
    regression, in addition to the regularization.
%%MJZ -- what happens when $\alpha=0$.
%
%    \cref{alg:reg:linear:kernelregression} shows the pseudo-code for
%    kernel regression. The main step is to compute the augmented kernel
%    matrix $\abK \in \setR^{n \times n}$, and the 
%    vector of mixture
%    coefficients $\bc \in \setR^n$. The predicted response on the
%    training data is then given as $\hY = \abK \bc$. As for prediction
%    for a new test point $\bz$, we use \cref{eq:reg:linear:kernel_w} to
%predict the response, which is given as:
%\begin{align*}
%    \hy & = \aphi(\bz)^T \abw = 
%    \aphi(\bz)^T \lB( \abD_\phi^T \bc \rB)
%     = \aphi(\bz)^T \lB( \sum_{i=1}^{n} c_i \cdot \aphi(\bx_i)
%     \rB)\\
%    & = \sum_{i=1}^n c_i \cdot
%    \aphi(\bz)^T\aphi(\bx_i) = 
%    \sum_{i=1}^n c_i \cdot \aK(\bz, \bx_i) = \bc^T \abK_{\bz}
%\end{align*}

\medskip

We compute the vector $\abK_{\bz}$ comprising the augmented
kernel values of $\bz$ with respect to
all of the data points in $\bD$,  
and take its dot product with the mixture
coefficient vector $\bc$ to obtain the predicted response.
\end{frame}
%
%
\begin{frame}{Kernel Regression Algorithm}
\begin{tightalgo}[H]{\textwidth-18pt}
    \SetKwInOut{Algorithm}{\textsc{Kernel-Regression} ($\bD, Y, K, \alpha$)}
\Algorithm{} 
$\bK \assign \bigl\{K(\bx_i, \bx_{\!j})\bigr\}_{i,j=1,\ldots,n}$
\tcp*[h]{standard kernel matrix}\;
$\abK \assign \bK+1$\tcp*[h]{augmented kernel matrix}\;
$\bc \assign \bigl( \abK + \alpha \cdot \bI \bigr)^{-1} Y$ \tcp*[h]{compute
mixture coefficients}\;
$\hY \assign \abK \bc$\;
\BlankLine
\SetKwInput{Algorithm}{\textsc{Testing} ($\bz, \bD, K, \bc$)}
\Algorithm{} 
$\abK_{\bz} \assign \bigl\{1 + K(\bz, \bx_i)\bigr\}_{\forall\; \bx_i \in
\bD}$\;
$\hy \assign \bc^T\abK_{\bz}$\;
%\caption{Kernel Regression Algorithm}
%\label{alg:reg:linear:kernelregression}
\end{tightalgo}
\end{frame}
%
%
\readdata{\dataI}{REG/linear/figs/iris-nl.dat}
%\begin{figure}[b!]
\begin{frame}{Kernel Regression on Iris}
\framesubtitle{Example}
    Consider the nonlinear Iris dataset
% shown in \cref{fig:reg:linear:iris-kernel}, 
obtained via a nonlinear
    transformation of 
	{\tt sepal length} ($A_1$) and {\tt sepal width} ($A_2$) attributes ($A_2$):
    \begin{align*}
	    X &= A_2 & Y &= 0.2A_1^2 + A_2^2 + 0.1A_1A_2
    \end{align*}
%\vspace{0.1in}
\psset{stepFactor=0.4}
\psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray,arrowscale=2,PointName=none}
\psset{xAxisLabel=$X$,yAxisLabel= $Y$}
\psset{xAxisLabelPos={c,-0.4in},yAxisLabelPos={-0.4in,c}}
\centerline{
\scalebox{0.70}{
\psgraph[axesstyle=frame,Dx=0.5,Dy=0.5,Ox=-1.5,Oy=-0.5]{->}(-1.5,-0.5)(1.5,1.5){4in}{2.5in}%
\dataplot[plotstyle=dots,showpoints=true]{\dataI}
\psset{algebraic}
\psclip{\psframe[linestyle=none](-1.5,-0.5)(1.5,1.5)}
%\psplot[plotstyle=line,linecolor=gray,linewidth=1pt]{-1.5}{1.5}{0.168 * x} 
%\psplot[plotstyle=line,linewidth=1pt]{-0.5}{1.5}{-0.046 + 0.469 * x^2}
%\psplot[plotstyle=line,linewidth=2pt]{-1.5}{1.5}{-0.086 + 0.0257 * x +
%    0.922 * x^2}
%\psplot[plotstyle=line,linewidth=1pt]{-1.5}{1.5}{-0.173 + 0.927 * x^2}
\endpsclip
\endpsgraph
}}
\end{frame}

\begin{frame}{Kernel Regression on Iris}
\framesubtitle{Example}
    We treat $Y$ as the response variable and $X$ is the independent
    attribute. The points show a clear quadratic (nonlinear)
    relationship between the them.

	\medskip

    The linear fit is
    \begin{align*}
        \hY = 0.168\cdot X
    \end{align*}
    Using the quadratic (inhomogeneous) kernel over
    $X$ comprising constant ($1$), linear ($X$), and quadratic terms ($X^2$),  and $\alpha=0.1$:
    \begin{align*}
        \hY = -0.086 + 0.026 \cdot X + 0.922 \cdot X^2
    \end{align*}
\end{frame}
\begin{frame}{Kernel Regression on Iris}
\framesubtitle{Example}
The linear (in gray) and quadratic (in black) fit are shown.
  
\medskip

The SSE error is $13.82$ for the linear and $4.33$ for the quadratic kernel.
    
\medskip

The quadratic kernel (as expected) gives a much better fit to the data. 

\medskip

%\vspace{0.1in}
\psset{stepFactor=0.4}
\psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray,arrowscale=2,PointName=none}
\psset{xAxisLabel=$X$,yAxisLabel= $Y$}
\psset{xAxisLabelPos={c,-0.4in},yAxisLabelPos={-0.4in,c}}
\centerline{
\scalebox{0.7}{
\psgraph[axesstyle=frame,Dx=0.5,Dy=0.5,Ox=-1.5,Oy=-0.5]{->}(-1.5,-0.5)(1.5,1.5){4in}{2.5in}%
\dataplot[plotstyle=dots,showpoints=true]{\dataI}
\psset{algebraic}
\psclip{\psframe[linestyle=none](-1.5,-0.5)(1.5,1.5)}
\psplot[plotstyle=line,linecolor=gray,linewidth=1pt]{-1.5}{1.5}{0.168 * x} 
%\psplot[plotstyle=line,linewidth=1pt]{-0.5}{1.5}{-0.046 + 0.469 * x^2}
\psplot[plotstyle=line,linewidth=2pt]{-1.5}{1.5}{-0.086 + 0.0257 * x +
    0.922 * x^2}
%\psplot[plotstyle=line,linewidth=1pt]{-1.5}{1.5}{-0.173 + 0.927 * x^2}
\endpsclip
\endpsgraph
}}
\end{frame}
%\vspace{0.6in}
%\caption{Kernel regression on nonlinear Iris dataset.}
%\label{fig:reg:linear:iris-kernel}
%\end{figure}
%
%% $ python3 ./kernel-ridge-regression.py iris-nl.dat iris-nl.dat 0.01 0.001 0.0001 0 linear
%% ['./kernel-ridge-regression.py', 'iris-nl.dat', 'iris-nl.dat', '0.01', '0.001', '0.0001', '0', 'linear']
%% params linear 0.01
%% 150 2
%% coeffs 9.999936835198143e-05 361455 361.7780178805616
%% gradient descent
%% weight [1.67988445e-01 3.33311105e-06]
%% accuracy, sse 0.0 13.821844397815468
%% [ -9.39114297 -16.20999401  -3.05396932  53.05655609  -7.53845467
%%  -21.84690806 -12.89699208 -16.92188551  17.5376328 ]
%% [ -9.65308016 -16.66331126  -3.13296461  54.5258799   -7.75296461
%%  -22.44319571 -13.24354236 -17.39308016  18.02622654]
%% exact solution
%% weight [1.67988445e-01 3.33311106e-06]
%% accuracy, sse 0.0 13.821844397815871
%% python3 ./kernel-ridge-regression.py iris-nl.dat iris-nl.dat 0.01 0.001 0.000  42.27s user 34.42s system 765% cpu 10.023 total
%
%% $ python3 ./kernel-ridge-regression.py iris-nl.dat iris-nl.dat 0.01 0.001 0.0001 0 quadratic
%% ['./kernel-ridge-regression.py', 'iris-nl.dat', 'iris-nl.dat', '0.01', '0.001', '0.0001', '0', 'quadratic']
%% params quadratic 0.01
%% 150 2
%% coeffs 9.999976481899052e-05 303589 198.02960900710735
%% gradient descent
%% row [0.046 1.   ]
%% weight [ 0.92177923  0.02567401 -0.08606691]
%% accuracy, sse 0.0 4.327562627996495
%% [  7.56473797  -3.49466745  13.35904343  -6.42628173   8.97793325
%%   -5.89848574 -11.67765921   0.23256415  -0.32625643]
%% [  7.97131367  -3.66355871  14.03841218  -6.73270343   9.41841218
%%   -6.20934329 -12.24266492   0.23131367  -0.34600496]
%% exact solution
%% row [0.046 1.   ]
%% weight [ 0.92177923  0.02567401 -0.08606691]
%% accuracy, sse 0.0 4.327562627996173
%% python3 ./kernel-ridge-regression.py iris-nl.dat iris-nl.dat 0.01 0.001 0.000  33.91s user 29.33s system 755% cpu 8.366 total
%
%
%
%\begin{example}
%    \label{ex:reg:linear:kernel}
%\end{example}
%
%\begin{figure}[b!]
%\vspace{0.2in}
%\captionsetup[subfloat]{captionskip=2pt}
\begin{frame}{Kernel ridge regression}
\framesubtitle{Example}

Consider the Iris principal components dataset, where $X_1$ and $X_2$ denote
the first two principal components. 

\medskip

The response variable $Y$ is binary,
with value $1$ corresponding to {\tt Iris-virginica} (points on the
top right, with $Y$ value 1) and $0$
corresponding to {\tt Iris-setosa} and {\tt Iris-versicolor} (other
two groups of points, with $Y$ value $0$).

\vspace*{1cm}

\psset{unit=0.25in}
\psset{lightsrc=viewpoint}
\psset{incolor=gray}
\psset{opacity=0.2}
\centerline{
%\subfloat[Linear Kernel]{ \label{fig:reg:linear:iris-vir:linear}
    \scalebox{0.8}{%
\psset{viewpoint=30 100 10 rtp2xyz,Decran=60}
%\psset{fillcolor=white}
\begin{pspicture}(-4,-3)(4,3)
\psSolid[object=parallelepiped,a=8,b=5,c=1.0,action=draw](0,-0.5,0.5)
\axesIIID[axisnames={X_1, X_2, Y}](-4,-1.5,0)(4.5,5,1.5)
\psset{linewidth=0.5pt, dotsize=0.3}
\psset{dotstyle=Bo,fillcolor=white}
\input{REG/linear/figs/N3Dlinear.tex}
\psset{showAxes=false, fillcolor=gray}
\psset{dotstyle=Bo,fillcolor=white}
\input{REG/linear/figs/P3Dlinear.tex}
\end{pspicture}
}}%}
\end{frame}
%% \vspace{0.05in}
%\vspace{0.2in}
%\caption{Kernel ridge regression: linear and (inhomogeneous) quadratic
%kernels.}
%\label{fig:reg:linear:iris-vir}
%\end{figure}
%
%
%% $ python3 ./kernel-ridge-regression.py iris-PC-virginica.txt iris-PC-virginica.txt 0.01 0.0001 0.00027 0 quadratic
%% ['./kernel-ridge-regression.py', 'iris-PC-virginica.txt', 'iris-PC-virginica.txt', '0.01', '0.0001', '0.00027', '0', 'quadratic']
%% params quadratic 0.01
%% 150 3
%% coeffs 0.00026999979811888103 73511 20.833996542200126
%% gradient descent
%% row [-1.41407223 -0.57492506  1.        ]
%% weight [ 0.09199753  0.10039532 -0.16713253  0.02872426 -0.18601614 -0.02983015]
%% accuracy, sse 0.96 8.442704457580298
%% [ 2.20587631  0.57943907 -0.64655216  4.28714917  1.13458009  2.7439245
%%   2.89230949  3.20547109  1.4564544 ]
%% [ 26.53042954  12.69933473 -13.08461675  54.89922892  15.82327117
%%   42.10024878  39.65698145  47.00545304  24.32211166]
%% exact solution
%% row [-1.41407223 -0.57492506  1.        ]
%% weight [ 0.09199753  0.10039532 -0.16713253  0.02872426 -0.18601614 -0.02983015]
%% accuracy, sse 0.96 8.44270445760333
%
%% $ python3 ./kernel-ridge-regression.py iris-PC-virginica.txt iris-PC-virginica.txt 0.01 0.001 0.0025 0 linear
%% ['./kernel-ridge-regression.py', 'iris-PC-virginica.txt', 'iris-PC-virginica.txt', '0.01', '0.001', '0.0025', '0', 'linear']
%% params linear 0.01
%% 150 3
%% coeffs 0.0024999793522733856 45303 143.39211268412006
%% gradient descent
%% weight [-0.16749884  0.07410432  0.33331111]
%% accuracy, sse 0.8866666666666667 15.473252336301812
%% [17.29449895  9.65836138 11.02246384 16.98218259  6.84570555 16.35015207
%%  13.16876896 24.25288602  8.74936952]
%% [47.24378629 26.5513653  29.63425697 45.91308351 18.7437055  45.48845518
%%  36.53946319 66.79771039 23.24999496]
%% exact solution
%% weight [-0.16749884  0.07410432  0.33331111]
%% accuracy, sse 0.8866666666666667 15.473252336303545
%
%
%\begin{example}[Kernel Ridge Regression]

\begin{frame}{Kernel ridge regression}
\framesubtitle{Example}
Figure shows the fitted regression
    plane using a linear kernel with ridge value $\alpha=0.01$:
    \begin{align*}
        \hY = 0.333 - 0.167 \cdot X_1 + 0.074 \cdot X_2
    \end{align*}

\psset{unit=0.25in}
\psset{lightsrc=viewpoint}
\psset{incolor=gray}
\psset{opacity=0.2}
\centerline{
%\subfloat[Linear Kernel]{ \label{fig:reg:linear:iris-vir:linear}
    \scalebox{0.8}{%
\psset{viewpoint=30 100 10 rtp2xyz,Decran=60}
%\psset{fillcolor=white}
\begin{pspicture}(-4,-3)(4,3)
\psSolid[object=parallelepiped,a=8,b=5,c=1.0,action=draw](0,-0.5,0.5)
\axesIIID[axisnames={X_1, X_2, Y}](-4,-1.5,0)(4.5,5,1.5)
\psset{linewidth=0.5pt, dotsize=0.3}
\psset{dotstyle=Bo,fillcolor=white}
\input{REG/linear/figs/N3Dlinear.tex}
\psset{showAxes=false, fillcolor=gray}
    \psSurface[ngrid=.2 .2,incolor=gray,axesboxed=false,
        linewidth=0.5\pslinewidth,
        color1 = {[rgb]{0 0 0}},
        color2 = {[rgb]{1 1 1}},
        hue=(color1) (color2), lightintensity=5,
    algebraic,Zmin=0,Zmax=1](-4,-5)(4,4){%
(0.074*y)-(0.167*x)+0.333 }
%(0.148*y)-(0.335*x)-0.333 }
\psset{dotstyle=Bo,fillcolor=white}
\input{REG/linear/figs/P3Dlinear.tex}
\end{pspicture}
}}%}
\vspace{0.2in}
\end{frame}

\begin{frame}{Kernel ridge regression}
\framesubtitle{Example}
Figure shows the fitted model
    when we use an inhomogeneous quadratic kernel with $\alpha=0.01$:
    \begin{align*}
        \hY = -0.03 - 0.167 \cdot X_1 - 0.186 \cdot X_2 
        + 0.092 \cdot X_1^2 + 0.1 \cdot X_1 \cdot X_2 + 0.029 \cdot X_2^2
    \end{align*}

	\vspace*{1cm}

\psset{unit=0.25in}
\psset{lightsrc=viewpoint}
\psset{incolor=gray}
\psset{opacity=0.2}
\centerline{
%\subfloat[Inhomogeneous Quadratic Kernel]{ \label{fig:reg:linear:iris-vir:quadratic}
\scalebox{0.8}{%
%    \psset{lightintensity=4}
%\psset{viewpoint=30 -30 20 rtp2xyz,Decran=60}
%\psset{viewpoint=30 -50 20 rtp2xyz,Decran=60}
\psset{viewpoint=30 100 10 rtp2xyz,Decran=60}
%\psset{fillcolor=white}
\begin{pspicture}(-6,-3)(6,3)
    \axesIIID[axisnames={X_1, X_2, Y}](-4,-1.5,0)(4.5,5,1.5)
\psSolid[object=parallelepiped,a=8,b=5,c=1.0,action=draw](0,-0.5,0.5)
\psset{linewidth=0.5pt, dotsize=0.3}
\psset{dotstyle=Bo,fillcolor=white}
\input{REG/linear/figs/N3Dquadratic.tex}
\psset{showAxes=false}
    \psSurface[ngrid=.2 .2,axesboxed=false,
        linewidth=0.5\pslinewidth, 
        color1 = {[rgb]{0 0 0}},
        color2 = {[rgb]{1 1 1}},
        hue=(color1) (color2), lightintensity=5,
    algebraic,Zmin=0,Zmax=1](-3,-2)(4,3){%
    (0.029*y^2)+(0.1*x*y)+(0.092*x^2)+(-0.186*y)-(0.167*x)-0.03 }
%    (0.057*y^2)+(0.2*x*y)+(0.184*x^2)+(-0.372*y)-(0.335*x)-1.119 }
\psset{dotstyle=Bo,fillcolor=white}
\input{REG/linear/figs/P3Dquadratic.tex}
\end{pspicture}
}}%}

\medskip

    The SSE error for the linear model is $15.47$, whereas for the
    quadratic kernel it is $8.44$, indicating a 
    better fit for the training data.
\end{frame}
%\end{example}
%
%
%\section{$L_1$ Regression: Lasso}
%\label{sec:reg:linear:lasso}
%\index{Lasso}
%\index{linear regression!Lasso}
%% \index{Lasso!linear regression}
%\index{least absolute selection and shrinkage operator|see{Lasso}}
%\index{L$_1$ regularized regression|see{Lasso}}
%\index{L$_1$ regularization!linear regression}
%\index{L$_1$ regularization}
%

\ifdefined\wox \begin{frame} \titlepage \end{frame} \fi

\begin{frame}{$L_1$ Regression: Lasso}
The {\em Lasso} ({\em least absolute selection and shrinkage
operator}) is a regularization method that aims to sparsify the
regression weights.

\medskip

%Instead of using the $L_2$ or Euclidean norm for weight regularization,
%as in ridge regression (see
%\cref{eq:reg:linear:ridgeobj_unpenalizedbias}), 
Lasso uses the $L_1$ norm for regularization:
\begin{empheq}[box=\tcbhighmath]{align*}
    \min_\bw\;\; J(\bw) = \frac{1}{2} \cdot ||\mY - \mbD\;\bw||^2 + \alpha \cdot \norm{\bw}_1
    %\label{eq:reg:linear:lasso_obj}
\end{empheq}
where $\alpha \ge 0$ is the regularization constant and 
$$\norm{\bw}_1 = \sum^{d}_{i=1} \abs{w_i}$$
%Note that, we have added the
%factor $\tfrac{1}{2}$ for convenience; it does not change the
%objective.
%Furthermore,  
We assume that 
$X_1$, $X_2$, \ldots, $X_d$ and $Y$ have 
been centered. %That is, we assume that
%\begin{align*}
%    \mbD & = \bD - \bone \cdot \bmu^T\\
%    \mY & = Y - \mu_Y \cdot \bone
%\end{align*}
%where $\bone \in \setR^n$ is the vector of all ones, $\bmu =
%(\mu_{X_1}, \mu_{X_2}, \cdots, \mu_{X_d})^T$ is the
%multivariate mean for the data, and $\mu_Y$ is the mean response value.

\medskip

Centering relieves us from explicitly dealing with the bias term
$b=w_0$, since we do not want to penalize $b$.
%Once the regression coefficients have
%been estimated, we can obtain the bias term %via
%\cref{eq:reg:linear:multiple_b}, 
%as follows:
%\begin{align*}
    %b = w_0 = \mu_Y - \sum_{j=1}^{d} w_{\!j} \cdot \mu_{X_{\!j}}
%\end{align*}
%
%
\end{frame}

\begin{frame}{$L_1$ Regression: Lasso}
The usage of the $L_1$ norm leads to {\em
sparsity} in the solution vector. 

	\medskip

Ridge regression reduces the value of the regression coefficients $w_i$, 
they may remain small but still non-zero. 
\medskip

$L_1$ regression can drive
the coefficients to zero, resulting in a more interpretable model,
especially when there are many predictor attributes.

\medskip

The Lasso objective comprises two parts, the squared error term
$\norm{\mY-\mbD\;\bw}^2$ which is convex and differentiable, and the $L_1$
penalty term 
$$\alpha \cdot \norm{\bw}_1 = \alpha \sum^{d}_{i=1} \abs{w_i}$$ 
which is convex but unfortunately non-differentiable at $w_i
= 0$. 

\medskip

We cannot simply compute the gradient and set it
to zero, as we did in the case of ridge regression.

\medskip

It can be solved via the generalized approach of {\em subgradients}.
\end{frame}
%
%% Given a convex function $f: \setR^d \to \setR$, a vector $\bs \in
%% \setR^d$ is said to be a {\em subgradient} of $f$ at $bw$ if
%% \begin{align*}
%%     f(\bw') \ge f(\bw) + bs^T (\bw'-bw) \qquad \text{for all } \bw' \in
%%     \setR^d
%% \end{align*}
%% Essentially, a subgradient $\bs$ is normal to some hyperplane 
%
%\begin{figure}[t!]
%% \vspace*{-0.1in}
%    % \vspace{0.2in}
\begin{frame}{$L_1$ Regression: Subgradients}
Consider the absolute value function $f(w) = \abs{w}$.
    
\medskip

When $w > 0$, $f'(w) = +1$, and
    when $w<0$, $f'(w) = -1$. 

\medskip

There is a discontinuity at $w=0$ where the derivative does not exist. 
    
\vspace*{0.5cm}

    \centering
    \scalebox{0.75}{%
    \psset{xAxisLabel=$w$,yAxisLabel=$\abs{w}$,%
    xAxisLabelPos={c,-0.6},yAxisLabelPos={-0.75,c} }
    \psset{linewidth=1pt,axesstyle=frame,algebraic,plotstyle=curve}
    \psgraph[Dy=1,Ox=-3,Oy=-2]{->}(-3,-2)(3,3){3in}{2.5in}%
    %\psline[linewidth=1pt,linecolor=gray](-3,1.5)(3,-1.5)
    %\psline[linewidth=1pt,linecolor=gray](-3,-0.75)(3,0.75)
    \psline[linewidth=1pt,linestyle=dotted](0,-2)(0,3)
    \psline[linewidth=1pt,linestyle=dotted](-3,0)(3,0)
    \psline[linewidth=2pt](0,0)(3,3)
    \psline[linewidth=2pt](0,0)(-3,3)
    %\uput[r](3,-1.5){$m=-0.5$}
    %\uput[r](3,0.75){$m=0.25$}
    \endpsgraph
    }
\end{frame}

\begin{frame}{$L_1$ Regression: Subgradients}

{\em Subgradients} generalize the notion of a derivative. 

\medskip

For $f(w) = \abs{w}$, the slope $m$ of any line that passes
    through $w=0$ that remains below or touches the graph of $f$ is
    called a subgradient of $f$ at $w=0$. 

    %since only lines with slope between $-1$ and $+1$ remain below
    %or (partially) coincide with the absolute value graph.
    
\vspace*{0.3cm}

    \centering
    \scalebox{0.65}{%
    \psset{xAxisLabel=$w$,yAxisLabel=$\abs{w}$,%
    xAxisLabelPos={c,-0.6},yAxisLabelPos={-0.75,c} }
    \psset{linewidth=1pt,axesstyle=frame,algebraic,plotstyle=curve}
    \psgraph[Dy=1,Ox=-3,Oy=-2]{->}(-3,-2)(3,3){3in}{2.5in}%
    \psline[linewidth=1pt,linecolor=gray](-3,1.5)(3,-1.5)
    \psline[linewidth=1pt,linecolor=gray](-3,-0.75)(3,0.75)
    \psline[linewidth=1pt,linestyle=dotted](0,-2)(0,3)
    \psline[linewidth=1pt,linestyle=dotted](-3,0)(3,0)
    \psline[linewidth=2pt](0,0)(3,3)
    \psline[linewidth=2pt](0,0)(-3,3)
    \uput[r](3,-1.5){$m=-0.5$}
    \uput[r](3,0.75){$m=0.25$}
    \endpsgraph
    }
\end{frame}
%    \vspace{0.4in}
%    \caption{Absolute value function: subgradients.}
%    \label{fig:reg:linear:abs}
%    \vspace{-0.2in}
%\end{figure}
%
%% \vspace*{-0.1in}
%\subsection{Subgradients and Subdifferential}
%\index{Lasso!subgradient}
%\index{Lasso!subdifferential}

\begin{frame}{Subgradients and Subdifferential}
The set of all the
subgradients at $w$ is called the {\em subdifferential}, denoted
as $\partial\abs{w}$. 

\medskip

The subdifferential of $f(w) = \abs{w}$ at $w=0$ is given as $\partial\abs{w} = [-1,1]$.

	\medskip

    Considering all the cases, 
    the subdifferential for $f(w) = \abs{w}$ is:
    \begin{align*}
        \partial\abs{w} & = 
        \begin{cases}
            1 & \text{ iff } w > 0\\
            -1 & \text{ iff } w < 0\\
            [-1,1] & \text{ iff } w = 0\\
        \end{cases}
        %\label{eq:reg:linear:subdifferential_abs}
    \end{align*}
    When the derivative exists, the subdifferential is
    unique and corresponds to the derivative (or gradient).

    \medskip

When the derivative does not exist the subdifferential corresponds to a set of
subgradients. 
\end{frame}
%
%
%\subsection{Bivariate $L_1$ Regression}
\begin{frame}{Bivariate $L_1$ Regression}
%
Consider the bivariate $L_1$ regression, where we have a
single independent attribute $\mX$ and a response attribute $\mY$ (both  centered). The bivariate regression model is given as
\begin{align*}
    \hy_i = w \cdot \mx_i
\end{align*}
%
The Lasso objective 
%from \cref{eq:reg:linear:lasso_obj} 
can then be written as
\begin{empheq}[box=\tcbhighmath]{align*}
    \min_w\; J(w) = \frac{1}{2} \sum^{n}_{i=1} (\my_i - w\cdot \mx_i)^2 +
    \alpha \cdot \abs{w}
    %\label{eq:reg:linear:l1_bivariate}
\end{empheq}
We can compute the subdifferential of this objective
as follows:
\begin{align*}
    \partial J(w) & =  \frac{1}{2} \cdot \sum^{n}_{i=1} 
    2 \cdot (\my_i - w \cdot \mx_i) \cdot (-\mx_i) 
    + \alpha \cdot \partial\abs{w} \notag\\
    & = - \sum^{n}_{i=1} \mx_i \cdot \my_i +
    w \cdot \sum^{n}_{i=1} \mx_i^2 + \alpha \cdot
    \partial\abs{w} \notag \\
    & = - \mX^T\mY + w \cdot \norm{\mX}^2 
    + \alpha \cdot \partial\abs{w}
\end{align*}
%
%We can solve for $w$ by setting the subdifferential to zero; we get
%\begin{align*}
%    & \partial J(w)  = 0\\
%    \implies & w \cdot \norm{\mX}^2 + 
%    \alpha \cdot \partial\abs{w} =  \mX^T \mY \\
%    % \implies & w = \frac{1}{\norm{\mX}^2} \cdot 
%    % \lB( \mX^T \mY - \alpha \cdot \partial\abs{w}\rB)\\
%    \implies & w + \eta \cdot \alpha \cdot\partial\abs{w}
%    = \eta \cdot \mX^T \mY 
%\end{align*}
%where $\eta  = 1/\norm{\mX}^2 > 0$ is a scaling constant.
%
\end{frame}

\begin{frame}{Bivariate $L_1$ Regression}
Corresponding to the three cases for the subdifferential of the absolute
value function 
%in \cref{eq:reg:linear:subdifferential_abs},
we have three cases to consider: 
\begin{description}
    
    \item[Case I ($w>0$ and $\partial\abs{w} = 1$):]% In this case we get
$ w   = \eta \cdot \mX^T \mY - \eta \cdot \alpha $ 

    Since $w > 0$, $\eta \cdot \mX^T \mY > \eta \cdot \alpha$ or $|\eta \cdot \mX^T\mY| > \eta \cdot \alpha$.

\item[Case II ($w<0$ and $\partial\abs{w} = -1$):]% In this case we have
        $w  = \eta \cdot \mX^T \mY + \eta \cdot \alpha$ 

    Since $w < 0$, 
    $\eta \cdot \mX^T \mY < - \eta \cdot \alpha$ or 
    $|\eta \cdot \mX^T \mY | > \eta \cdot \alpha$.

\item[Case III (\text{$w=0$ and $\partial\abs{w} \in [-1,1]$}):]
$        w  \in\; \lB[\eta \cdot \mX^T \mY - \eta \cdot \alpha, \; \eta \cdot \mX^T\mY + \eta \cdot \alpha\rB] $

    However, since $w=0$,
    $| \eta \cdot \mX^T \mY | \le \eta \cdot \alpha$. 

\end{description}
%
%Let $\tau \ge 0$ be some fixed value. 
%Define the {\em soft-threshold} function $\cS_\tau: \setR\to\setR$ as follows:
%\index{Lasso!soft-threshold function}
%\begin{align*}
%    \cS_\tau(z) = \sign(z) \cdot \max\Bigl\{ 0, \bigl(\abs{z} - \tau\bigr) \Bigr\}
%\end{align*}
Then the above three cases can be written compactly as:
\begin{align*}
    \tcbhighmath{
    w = \cS_{\eta \cdot \alpha}(\eta \cdot \mX^T \mY)}
    %\label{eq:reg:linear:l1_bi_optw}
\end{align*}
with $\tau = \eta \cdot \alpha$, where $w$ is the optimal solution to the
problem. % in
%\cref{eq:reg:linear:l1_bivariate}.
\end{frame}
%

%WMJ Skipped
%\subsection{Multiple $L_1$ Regression}
%
%Consider the $L_1$ regression objective from 
%\cref{eq:reg:linear:lasso_obj}
%\begin{align*}
%    \min_{\bw}\;\; J(\bw) & =  
%    \frac{1}{2} \cdot \norm[\Big]{\mY - \sum_{i=1}^d w_i \cdot \mX_i}^2 + \alpha \cdot
%    \norm{\bw}_1\\
%    & = 
%    \frac{1}{2} \cdot \Bigl( \mY^T \mY - 2 \sum_{i=1}^d w_i \cdot
%        \mX_{\!i}^T \mY 
%        + \sum_{i=1}^d \sum_{j=1}^d w_i \cdot w_{\!j} \cdot
%        \mX_{\!i}^T\mX_{\!j}
%    \Bigr) + 
%\alpha \cdot \sum_{i=1}^d \abs{w_{\!i}}
%\end{align*}
%
%We generalize the bivariate solution to the multiple $L_1$
%formulation by optimizing for each $w_k$ individually, via the approach
%of {\em cyclical coordinate descent}.
%\index{Lasso!cyclical coordinate descent}
%We rewrite the $L_1$ objective by focusing only on the $w_k$ terms, 
%and ignoring all terms not involving $w_k$ which are assumed to be
%fixed:
%\begin{align*}
%    \min_{w_k}\;\; J(w_k) & =  
%    - w_k \cdot \mX_{k}^T\mY + \frac{1}{2} w_k^2 \cdot \norm{\mX_k}^2 +
%    w_k \cdot 
%    \sum_{j\ne k}^d w_{\!j} \mX_k^T\mX_{\!j} +
%     \alpha \cdot \abs{w_k}
%        \label{eq:reg:linear:lasso_obj_wk}
%\end{align*}
%
%% \begin{align*}
%%     \min_{w_k}\;\; J(w_k) & =  
%%     \frac{1}{2} \sum^{n}_{i=1} 
%%     \Bigl(\my_i - \sum_{j=1}^d w_{\!j} \cdot \mx_{ij} \Bigr)^2 
%%     + \alpha \cdot \sum_{j=1}^d \abs{w_{\!j}} \notag\\
%%     & = \frac{1}{2}
%%    \sum^{n}_{i=1} 
%%    \Bigl(\my_i - \sum_{j\ne k}^d w_{\!j} \cdot \mx_{ij} - w_k \cdot \mx_{ik}
%%    \Bigr)^2 
%%    + \alpha \cdot \sum_{j\ne k}^d \abs{w_{\!j}} + \alpha \cdot \abs{w_k}
%%     \label{eq:reg:linear:lasso_obj_wk}
%% \end{align*}
%% where all terms not involving $w_k$ are assumed to be fixed.
%
%Setting the subdifferential of $J(w_k)$ to zero, we get
%\begin{align*}
%    & \partial J(w_k) = 0 \notag\\
%    \implies &  w_k \cdot \norm{\mX_k}^2 + \alpha \cdot
%    \partial\abs{w_k} = \mX_k^T\mY -\sum_{j\ne k}^d w_{\!j} \cdot
%    \mX_k^T\mX_{\!j}\\  
%    \implies &  w_k \cdot \norm{\mX_k}^2 + \alpha \cdot
%    \partial\abs{w_k} = \mX_k^T\mY -\sum_{j=1}^d w_{\!j} \cdot
%    \mX_k^T\mX_{\!j} + w_k \mX_k^T\mX_k\\  
%    \implies & w_k \cdot \norm{\mX_k}^2 + \alpha \cdot \partial\abs{w_k}= 
%   w_k \norm{\mX_k^T}^2  + 
%   \mX_k^T \lB(\mY - \mbD\;\bw\rB) 
%\end{align*}
%% \begin{align*}
%%     & \partial J(w_k) = 0 \notag\\
%%     \implies & - \sum^{n}_{i=1} (-\mx_{ik}) \cdot 
%%    \Bigl(\my_i - \sum_{j\ne k}^d w_{\!j} \cdot \mx_{ij} - w_k \cdot \mx_{ik}
%%    \Bigr)
%%    + \alpha \cdot \partial\abs{w_k} = 0\notag\\
%%    \implies & w_k \sum^{n}_{i=1} \mx_{ik}^2 + 
%%    \alpha \cdot \partial\abs{w_k} = 
%%     \sum^{n}_{i=1} \mx_{ik} \my_i - 
%%     \sum^{n}_{i=1} \mx_{ik} \Bigl( 
%%     \sum_{j\ne k}^d w_{\!j} \cdot \mx_{ij}\Bigr) 
%%     \\
%%    \implies & w_k \sum^{n}_{i=1} \mx_{ik}^2 = 
%%     \sum^{n}_{i=1} \mx_{ik} \my_i - 
%%     \sum^{n}_{i=1} \mx_{ik} \Bigl( 
%%     \sum_{j=1}^d w_{\!j} \cdot \mx_{ij} - w_k \cdot \mx_{ik} \Bigr) - \alpha \cdot
%%     \partial\abs{w_k}\notag\\
%%     \implies & w_k \cdot \norm{\mX_k}^2 = 
%%    w_k \norm{\mX_K^T}^2  + 
%%    \mX_k^T \lB(\mY - \mbD\;\bw\rB) - \alpha \cdot \partial\abs{w_k}
%% \end{align*}
%
%    % \implies & w_k \cdot \norm{\mX_k}^2 = \mX_k^T \mY - \mX_k^T
%    % \bigl(\mbD_{-k}\bw_{-k}\bigr) - \alpha \cdot \partial\abs{w_k}\\
%    % \label{eq:reg:linear:lasso_wk_subdiff_k}
%
%
%%where $\mX_k = (\mx_{1k}, \mx_{2k}, \cdots, \mx_{nk})^T \in \setR^n$ is the
%%centered attribute vector for the $k$th attribute, and where
%%\begin{align*}
%%    \mbD_{-k}\bw_{-k} = \sum_{i=1}^n \sum_{j\ne k}^d w_{\!j} \cdot \mx_{ij}
%%    = \sum_{i=1}^n \Bigl(\sum_{j=1}^d w_{\!j} \cdot \mx_{ij} - w_k
%%\mx_{ik}\Bigr) =  \mbD \;\bw - w_k \cdot \mX_k
%%\end{align*}
%%Note that all $w_{\!j}$ values are fixed, since only $w_k$ is being optimized. 
%%Thus, in \cref{eq:reg:linear:lasso_wk_subdiff_k}  we can treat 
%%$\bw_{-k}$ as the previous estimate for all the regression coefficients,
%%%and $w_k$ as a previous (fixed) estimate as well.
%%so that
%%\begin{align*}
%%w_k \cdot \norm{\mX_k}^2 & =
%%\mX_k^T \mY - \mX_k^T
%%    \bigl(\mbD_{-k}\bw_{-k}\bigr) - \alpha \cdot \partial\abs{w_k}\notag\\
%%& = \mX_k^T \mY - \mX_k^T
%%    \bigl(\mbD\;\bw - w_k \cdot \mX_k\bigr) - \alpha \cdot \partial\abs{w_k}\notag\\
%%& = w_k \cdot \norm{\mX_k}^2 + \mX_k^T \bigl(\mY - \bD\bw\bigr) - \alpha \cdot \partial\abs{w_k}
%%    \label{eq:reg:linear:lasso_wk_subdiff}
%%\end{align*}
%We can interpret the above equation as specifying an iterative solution for
%$w_k$. In essence, we let the $w_k$ on the left hand side be the new estimate for
%$w_k$, whereas we treat the $w_k$ on the right hand side as the previous
%estimate.
%More concretely, 
%let $\bw^{\;t}$ represent the weight vector at step $t$, with $w_k^t$
%denoting the estimate for $w_k$ at time $t$. The
%new estimate for $w_k$ at step $t+1$ is then given as
%\begin{align*}
%w_k^{t+1} + \frac{1}{\norm{\mX_k}^2} \cdot \alpha \cdot
%\partial\abs{w_k^{t+1}} & = 
%    w_k^t + \frac{1}{\norm{\mX_k}^2} \cdot \mX_k^T \bigl(\mY - \mbD\;\bw^{\;t}\bigr)
%        \notag\\
%        w_k^{t+1} + \eta \cdot \alpha \cdot \partial\abs{w_k^{t+1}} & = 
%        w_k^t + \eta \cdot \mX_k^T \bigl(Y - \mbD\;\bw^{\;t}\bigr)
%\label{eq:reg:linear:l1_w_update_subdiff}
%\end{align*}
%where $\eta = 1/\norm{\mX_k}^2 > 0$ is just a scaling constant.
%Based on the three cases for $w_k^{t+1}$ and 
%the subdifferential $\partial\abs{w_k^{t+1}}$, 
%following a similar approach as in the bivariate case
%[\cref{eq:reg:linear:l1_bi_optw}], the new
%estimate for $w_k$ can be written compactly as
%\begin{align*}
%    \tcbhighmath{
%w_k^{t+1} = \cS_{\eta \cdot \alpha}\Bigl( w_k^t + 
%    \eta \cdot \mX_k^T \bigl(\mY - \mbD\;\bw^{\;t}\bigr) \Bigr) }
%    \label{eq:reg:linear:l1_w_update_cS}
%\end{align*}
%
\begin{frame}{$L_1$-Regression Algorithm}
\begin{tightalgo}[H]{\textwidth-18pt}
    \SetKwInOut{Algorithm}{\textsc{$L_1$-Regression} ($\bD, Y, \alpha,
    \eta, \epsilon$)}
\Algorithm{} 
$\bmu \assign \mean(\bD)$ \tcp*[h]{compute mean}\;
$\mbD \assign \bD - \bone \cdot \bmu^T$ \tcp*[h]{center the data}\;
$\mY \assign Y - \mu_Y \cdot \bone$ \tcp*[h]{center the response}\;
$t \assign 0$ \tcp*[h]{step/iteration counter}\; 
$\bw^{\;t} \assign $ random vector in $\setR^d$ \tcp*[h]{initial weight vector}\;
\Repeat{$\norm{\bw^{\;t}-\bw^{\;t-1}} \le \epsilon$}
{%
    \ForEach{$k = 1,2,\cdots,d$}{%
        $\grad(w_k^t) \assign - \mX_k^T (Y - \mbD\;\bw^{\;t})$ 
        \tcp*[h]{compute gradient at $w_k$}\;
        $w_k^{t+1} \assign w_k^t - \eta \cdot \grad(w_k^t)$
        \tcp*[h]{update estimate for $w_k$}\;
        $w_k^{t+1} \assign \cS_{\eta \cdot \alpha}\bigl(w_k^{t+1}\bigr)$
        \tcp*[h]{apply soft-threshold function}
    }
    $t \assign t+1$\;
}%
$b \assign \mu_Y - \bigl(\bw^{\;t}\bigr)^T\bmu$ \tcp*[h]{compute the bias term}
%\caption{$L_1$ Regression Algorithm: Lasso}
%\label{alg:reg:linear:l1_regression}
\end{tightalgo}
\end{frame}
%
%The pseudo-code for $L_1$ regression is shown in
%\cref{alg:reg:linear:l1_regression}. The algorithm starts with a random
%estimate for $\bw$ at step $t=0$, and then cycles through each dimension to estimate $w_k$
%until convergence. 
%Interestingly, the term $-\mX_k^T(\mY - \mbD \;\bw^{\;t})$ is in
%fact the gradient at $w_k$ of the squared error term in the Lasso
%objective, and thus the update equation is the same as gradient descent
%with step size $\eta$, followed by the soft-threshold operator.
%Note also that since $\eta$ is just a positive scaling constant, we make
%it a parameter of the algorithm, denoting the step size for gradient
%descent.
%
%% python3 l1-regression.py iris-int.txt iris-int.txt 0 0.0001 0.000001 1
%% norm: L2, L1 0.435646977681 0.990089428872
%% weights [-0.1106399  -0.04332264  0.22835992  0.60776697]
%% bias 0.191974322396
%% accuracy, sse 0.9733333333333334 6.95779154729
%
%% $ python3 l1-regression.py iris-int.txt iris-int.txt 1 0.0001 0.000001 1
%% ['l1-regression.py', 'iris-int.txt', 'iris-int.txt', '1', '0.0001', '0.000001', '1']
%% 150 5
%% classes [0.0, 1.0, 2.0]
%% skikit lasso [-0.07617168 -0.01572641  0.25268333  0.51718796] -0.0765634352866
%% 14819 9.99787851306e-07 0.582608374968 [-0.07452392 -0.01729157  0.25046679  0.52042876]
%% norm: L2, L1 0.339432518583 0.862711030133
%% weights [-0.07452392 -0.01729157  0.25046679  0.52042876]
%% bias -0.0769652428367
%% accuracy, sse 0.9666666666666667 7.08516988378
%
%% $ python3 l1-regression.py iris-int.txt iris-int.txt 5 0.0001 0.000001 1
%% ['l1-regression.py', 'iris-int.txt', 'iris-int.txt', '5', '0.0001', '0.000001', '1']
%% 150 5
%% classes [0.0, 1.0, 2.0]
%% skikit lasso [-0.         -0.          0.36008257  0.16761778] -0.554348205343
%% 8864 9.99465911711e-07 0.397265541748 [-0.         -0.          0.35916962  0.16975598]
%% norm: L2, L1 0.15781991066 0.52892560187
%% weights [-0.         -0.          0.35916962  0.16975598]
%% bias -0.553479724421
%% accuracy, sse 0.96 8.81601292938
%
%% $ python3 l1-regression.py iris-int.txt iris-int.txt 10 0.0001 0.000001 1
%% ['l1-regression.py', 'iris-int.txt', 'iris-int.txt', '10', '0.0001', '0.000001', '1']
%% 150 5
%% classes [0.0, 1.0, 2.0]
%% skikit lasso [-0.         -0.          0.41908859  0.        ] -0.575214330185
%% 1731 9.90715320337e-07 0.419068227229 [-0.         -0.          0.41906823  0.        ]
%% norm: L2, L1 0.175618179073 0.419068227229
%% weights [-0.         -0.          0.41906823  0.        ]
%% bias -0.575137776744
%% accuracy, sse 0.9466666666666667 10.1478128947
%
%\begin{example}[$L_1$ Regression]
	\begin{frame}[fragile]{$L_1$ Regression}
\framesubtitle{Example}
    We apply $L_1$ regression to the full Iris dataset with $n=150$
    points, and four independent attributes, namely {\tt sepal-width}
    ($X_1$),
    {\tt sepal-length} ($X_2$), {\tt petal-width} ($X_3$), and {\tt
    petal-length} ($X_4$). 

	\medskip

The Iris type attribute comprises the response variable $Y$. 
    There are three
    Iris types, namely {\tt Iris-setosa}, {\tt Iris-versicolor}, and
    {\tt Iris-virginica}, which are coded as $0$, $1$ and $2$,
    respectively.

\medskip

    The $L_1$ regression for $\alpha$
    ($\eta=0.0001$) are
    shown below:
\begin{footnotesize}
    \begin{align*}
        \alpha = 0:\; \hY & = +0.19 -0.11 \cdot X_1 -0.05 \cdot X_2 + 
	    0.23 \cdot X_3 + 0.61 \cdot X_4 & SSE=6.96 & \quad  \norm{\bw}_1=0.44\\
        \alpha = 1:\; \hY & = -0.08 -0.08 \cdot X_1 -0.02 \cdot X_2 + 
	    0.25 \cdot X_3 + 0.52 \cdot X_4 & SSE=7.09 & \quad  \norm{\bw}_1=0.34\\
            \alpha = 5:\; \hY & = -0.55 + 0.00 \cdot X_1  + 0.00 \cdot X_2 + 
	    0.36 \cdot X_3 + 0.17 \cdot X_4 & SSE=8.82 & \quad  \norm{\bw}_1=0.16\\
            \alpha = 10:\; \hY & = -0.58 + 0.00 \cdot X_1  + 0.00 \cdot X_2 + 
	    0.42 \cdot X_3 + 0.00 \cdot X_4 & SSE=10.15 & \quad  \norm{\bw}_1=0.18
    \end{align*}
\end{footnotesize}
Note the sparsity inducing effect, 
for $\alpha=5$ and $\alpha=10$, which drives some $w_i$
to 0. 
\end{frame}

\begin{frame}{$L_1$ Regression}
\framesubtitle{Example}

    %The $L_1$ norm values for the weight vectors (excluding the bias term) are
    %$0.436$, $0.339$, $0.156$, and $0.176$, respectively. 

%\medskip
    
    We can contrast the coefficients for $L_2$ (ridge)
    and $L_1$ (Lasso) regression by comparing models with the same level
    of squared error.

	\medskip

    For $\alpha=5$, the $L_1$ model has $SSE = 8.82$.

	\medskip

    We adjust the ridge value in $L_2$ regression, with $\alpha=35$
    resulting in a similar SSE value. 
    The two models are given as follows:
\begin{small}
    \begin{align*}
        L_1:\; \hY & = -0.553 + 0.0 \cdot X_1  + 0.0 \cdot X_2 + 
        0.359 \cdot X_3 + 0.170 \cdot X_4 & \norm{\bw}_1 = 0.156\\
        L_2:\; \hY & = -0.394 + 0.019 \cdot X_1  - 0.051 \cdot X_2 + 
        0.316 \cdot X_3 + 0.212 \cdot X_4 & \norm{\bw}_1 = 0.598
    \end{align*}
\end{small}

$L_2$: 
    the coefficients for $X_1$ and $X_2$ are small, and
    therefore less important, but 
    they are not zero. 

	\medskip

 $L_1$: the
    coefficients for $X_1$
    and $X_2$ are exactly zero, leaving only $X_3$ and $X_4$; 

	\medskip

    Lasso can thus act as an automatic feature selection approach.
    
\end{frame}
%\end{example}
%
%% $ python3 ridge-regression.py iris-int.txt iris-int.txt 35 0.001 0.0001 1
%% ['ridge-regression.py', 'iris-int.txt', 'iris-int.txt', '35', '0.001', '0.0001', '1']
%% 150 5
%% classes [0.0, 1.0, 2.0]
%% weights [ 0.01893608 -0.05139119  0.31568432  0.21152962] 0.147401003598
%% bias -0.39380679819
%% accuracy, sse 0.96 8.82907130503
%
%\section{Further Reading}
%\label{sec:reg:linear:ref}
%\begin{refsection}
%
%For a geometrical approach to multivariate statistics see
%\citet{wickens2014geometry}, and \citet{saville2012statistical}. 
%For a description of the class of generalized linear models, of which
%linear regression is a special case, see \citet{agresti2015foundations}.
%An excellent overview of Lasso and sparsity based methods is given in
%\citet{hastie2015statistical}. For a description of cyclical
%coordinate descent for $L_1$ regression, and also other approaches for
%sparse statistical models see \citet{hastie2015statistical}.
%
%\printbibliography[heading=emptyheading]
%\end{refsection}
%
%\section{Exercises}
%\label{sec:reg:linear:exercise}
%
%\begin{exercises}[Q1.]
%
%
%\item \label{q:reg:linear:cov} Consider the data in
%    \cref{tab:reg:linear:tabcov}, with $Y$ as
%    the response variable and $X$ as the independent variable. Answer
%    the following questions:
%    \begin{sexercises}
%        \item Compute the predicted response vector $\hY$
%            for least square regression using the geometric approach.
%    
%   
%        \item Based on the geometric approach extract the value of the
%            bias and slope, and give the equation of the best
%            fitting regression line.
%
%    \end{sexercises}
%
%\bgroup{
%\makeatletter
%\long\def\@tablecaption#1#2{%
%\fontsize{8}{11}\selectfont\raggedright%\sans%
%\setbox\@tempboxa=\hbox{{#1}\ignorespaces{\hskip4\p@}\ignorespaces{#2}}%
%\ifdim\wd\@tempboxa>\tempdime%
%\centering{#1}\ignorespaces{\hskip4\p@}\ignorespaces{#2}%
%\else%
%\hbox to \tempdime{\hss{#1}\ignorespaces{\hskip4\p@}\ignorespaces#2\hss}\fi%
%\vskip\belowcaptionskipa}
%\makeatother
%
%\begin{table}[!h]
%% \processtable{Data for Q\ref{q:reg:linear:cov}\label{tab:reg:linear:tabcov}}
%%     {\tabcolsep12pt\renewcommand{\arraystretch}{1.1}
%\caption{Data for Q\ref{q:reg:linear:cov}\label{tab:reg:linear:tabcov}}
%\centering{\tabcolsep12pt\renewcommand{\arraystretch}{1.1}
%\begin{tabular}{|c|c|}
%  \hline
%  $X$ & $Y$\\ \hline
%  5 & 2\\
%  0 & 1\\
%  2 & 1\\
%  1 & 1\\
%  2 & 0\\ \hline
%\end{tabular}}{}
%\end{table}\egroup}%
%
%\item \label{q:reg:linear:ridgeregression} Given data in
%    \cref{tab:reg:linear:ridgedata}, let $\alpha = 0.5$ be
%    the regularization constant. Compute the equation for ridge
%    regression of $Y$ on $X$, where both the bias and slope are
%    penalized.  
%    Use the fact that the inverse of the matrix 
%    $\bA = \matr{\begin{array}{cc}a & b\\c & d\end{array}}$ is
%    given as $\bA^{-1} = {\frac{1}{det(\bA)}}
%    \matr{\begin{array}{cc}d &-b\\-c & a\end{array}}$, with $det(\bA) =
%    ad-bc$.
%
%\bgroup{
%\makeatletter
%\long\def\@tablecaption#1#2{%
%\fontsize{8}{11}\selectfont\raggedright%\sans%
%\setbox\@tempboxa=\hbox{{#1}\ignorespaces{\hskip4\p@}\ignorespaces{#2}}%
%\ifdim\wd\@tempboxa>\tempdime%
%\centering{#1}\ignorespaces{\hskip4\p@}\ignorespaces{#2}%
%\else%
%\hbox to \tempdime{\hss{#1}\ignorespaces{\hskip4\p@}\ignorespaces#2\hss}\fi%
%\vskip\belowcaptionskipa}
%\makeatother
%
%\begin{table}[!h]
%    \vspace{-0.1in}
%        \caption{Data for Q\ref{q:reg:linear:ridgeregression}
%    \label{tab:reg:linear:ridgedata}}
%        \centering{\tabcolsep12pt\renewcommand{\arraystretch}{1.1}
%    \begin{tabular}{|c|c|}
%        \hline
%        $X$ & $Y$\\ \hline
%        1 & 1\\
%        2 & 3\\
%        4 & 4\\
%        6 & 3\\ \hline
%\end{tabular}}{}
%    \vspace{-0.2in}
%    \end{table}\egroup}%
%
%
%
%
%\item Show that \cref{eq:reg:linear:w-cov-var} holds, that is
%\begin{align*}
%    w = 
%   \frac{\sum_{i=1}^n x_i\cdot y_i - n\cdot \mu_X\cdot \mu_Y}{\sum_{i=1}^n x_i^2 -
%       n\cdot \mu_X^2} 
%   =
%    \frac{\sum_{i=1}^n (x_i - \mu_X)(y_i - \mu_Y)}{\sum_{i=1}^n (x_i
%        - \mu_X)^2} 
%\end{align*}
%
%\item Derive an expression for the bias term $b$ and the weights $\bw =
%    (w_1, w_2, \cdots, w_d)^T$ in multiple regression using
%    \cref{eq:reg:linear:multiple_w_noaugment}, without adding the augmented
%    column.
%
%
%\item Show analytically (i.e., without using the geometry)  
%    that the bias term in multiple regression, as shown in 
%   \cref{eq:reg:linear:multiple_b}, is given as
%    $$w_0 = \mu_Y - w_1 \cdot \mu_{X_1} - w_2 \cdot \mu_{X_2} 
%    - \cdots - w_d \cdot \mu_{X_d}$$
%
%
%\item Show that $\hY^T\bepsilon = 0$.
%
%\item Prove that $\norm{\bepsilon}^2 = \norm{Y}^2 - \norm{\hY}^2$.
%
%\item Show that if $\lambda_i$ is an eigenvalue of $\bD^T\bD$
%    corresponding to the eigenvector $\bu_i$, then $\lambda_i +\alpha$ is an
%    eigenvalue of $(\bD^T\bD + \alpha \cdot \bI)$ for the same eigenvector
%    $\bu_i$.
%
%
%\item Show that
%$\bDelta^{-1} \bQ^T Y$ gives the vector
%of scalar projections of $Y$ onto each of the orthogonal basis vectors in
%multiple regression
%\begin{align*}
%    \bDelta^{-1}\bQ^T Y = \matr{\proj_{U_0}\!(Y)\\
%        \proj_{U_1}\!(Y)\\ \vdots \\ \proj_{U_d}\!(Y)}
%\end{align*}
%
%\item Formulate a solution to the ridge regression problem via
%    QR-factorization.
%
%\item Derive the solution for the weight vector $\bw = (w_1, w_2,
%    \cdots, w_d)^T$ and bias $b=w_0$ in ridge regression without
%    subtracting the means from $Y$ and the independent attributes 
%    $X_1, X_2, \cdots, X_d$, and without adding the augmented column.
%
%
%
%\item Show that the solution for ridge regression and kernel ridge
%    regression are exactly the same when we use a linear kernel.
%
%
%\item \label{q:reg:linear:biL1} Show that 
%$w=\cS_{\eta\cdot\alpha} (\eta \cdot \mX^T \mY)$ [\cref{eq:reg:linear:l1_bi_optw}] is
%the solution for bivariate $L_1$ regression.
%
%
%\item Derive the three cases for the subdifferential in
%    \cref{eq:reg:linear:l1_w_update_subdiff}, and show that they 
%    correspond to the soft-threshold update in
%    \cref{eq:reg:linear:l1_w_update_cS}.
%
%\item Show that that the gradient at $w_k$ of the SSE term in the $L_1$
%    formulation is given as
%    \begin{align*}
%        \grad(w_k) = \frac{\partial}{\partial w_k} \frac{1}{2}
%        \cdot\norm{\mY - \mbD\;\bw}^2 =
%        - \mX_k^T (\mY - \mbD\;\bw) 
%    \end{align*}
%    where $Y$ is the response vector, and $X_k$ is the $k$th predictor
%    vector.
%
%\end{exercises}

