\lecture{graph}{graph}

\date{Chapter 4: Graph Data}

\begin{frame}
\titlepage
\end{frame}


\begin{frame}{Graphs}
A {\em graph} $G=(V,E)$ comprises
a f\/{i}nite nonempty set $V$
of {\em vertices} or {\em nodes}, and
a set $E \subseteq V \times V$ of {\em edges} consisting of
{\em unordered} pairs of vertices.

\medskip
The number of nodes in the graph $G$, given as $|V|=n$, is called
the {\em order} of the graph, and the number of edges in the
graph, given as $|E|=m$, is called the {\em size} of $G$.

%
\medskip
A {\em directed graph} or {\em digraph} has an edge set $E$ consisting of {\em ordered} pairs of
vertices. 

\medskip
A {\em weighted graph}
consists of a graph together with a weight $w_{ij}$ for each edge
$(v_i, v_{j}) \in E$. 

\medskip
A graph $H = (V_H, E_H)$ is called a {\em
subgraph} of $G=(V,E)$ if $V_H \subseteq V$ and $E_H \subseteq
E$.

\end{frame}

    \psset{unit=0.75in}
    \def\mcnode#1{\cnodeput#1}
\begin{frame}{Undirected and Directed Graphs}
%    \captionsetup[subfloat]{captionskip=10pt}
    \centerline{
\scalebox{0.9}{
%    \subfloat[]{
%    \label{fig:eda:graph:ex1a}
    \begin{pspicture}(3,2.2)
        \mcnode{(1,2){v1}{$v_1$}}
        \mcnode{(2,2){v2}{$v_2$}}
        \mcnode{(0,1){v3}{$v_3$}}
        \mcnode{(1,1){v4}{$v_4$}}
        \mcnode{(2,1){v5}{$v_5$}}
        \mcnode{(3,1){v6}{$v_6$}}
        \mcnode{(1,0){v7}{$v_7$}}
        \mcnode{(2,0){v8}{$v_8$}}
        \ncline{v1}{v2}
        \ncline{v1}{v3}
        \ncline{v1}{v4}
        \ncline{v1}{v5}
        \ncline{v2}{v5}
        \ncline{v2}{v6}
        \ncline{v3}{v4}
        \ncline{v4}{v5}
        \ncline{v4}{v7}
        \ncline{v5}{v8}
        \ncline{v7}{v8}
    \end{pspicture}
%    }
    \hspace{0.5in}
%    \subfloat[]{
%    \label{fig:eda:graph:ex1b}
    \psset{arrowscale=1.5}
    \begin{pspicture}(3,2.2)
        \mcnode{(1,2){v1}{$v_1$}}
        \mcnode{(2,2){v2}{$v_2$}}
        \mcnode{(0,1){v3}{$v_3$}}
        \mcnode{(1,1){v4}{$v_4$}}
        \mcnode{(2,1){v5}{$v_5$}}
        \mcnode{(3,1){v6}{$v_6$}}
        \mcnode{(1,0){v7}{$v_7$}}
        \mcnode{(2,0){v8}{$v_8$}}
        \ncline{->}{v1}{v2}
        \ncline{<-}{v1}{v3}
        \ncline{->}{v1}{v4}
        \ncline{<-}{v1}{v5}
        \ncline{->}{v2}{v5}
        \ncline{->}{v2}{v6}
        \ncline{<-}{v3}{v4}
        \ncline{<-}{v4}{v5}
        \ncline{->}{v4}{v7}
        \ncarc[arcangle=40]{->}{v5}{v8}
        \ncarc[arcangle=-40]{<-}{v5}{v8}
        \ncline{<-}{v7}{v8}
    \end{pspicture}
} }
\end{frame}



\begin{frame}{Degree Distribution}
The {\em degree} of a node $v_i \in V$ is the
number of edges incident with it, and is denoted as $d(v_i)$ or just
$d_i$. 

\medskip
The
{\em degree sequence} of a graph is the list of the degrees of
the nodes sorted in non-increasing order.

\medskip
Let $N_k$ denote the
number of vertices with degree $k$.
The {\em degree frequency distribution} of a graph is given as
$$(N_0, N_1, \ldots, N_t)$$
where $t$ is the maximum degree for a node in $G$.

\medskip
Let $X$ be a random variable denoting the degree of a node.
The {\em degree distribution} of a graph gives the
probability mass function $f$ for $X$, given as
$$\bigl(f(0), f(1), \ldots, f(t)\bigr)$$
where $f(k) = P(X=k) = {N_k\over n}$ is the probability of a
node with degree $k$.
\end{frame}

\begin{frame}{Degree Distribution}
\centerline{
\scalebox{0.9}{
    \begin{pspicture}(3,2.2)
        \mcnode{(1,2){v1}{$v_1$}}
        \mcnode{(2,2){v2}{$v_2$}}
        \mcnode{(0,1){v3}{$v_3$}}
        \mcnode{(1,1){v4}{$v_4$}}
        \mcnode{(2,1){v5}{$v_5$}}
        \mcnode{(3,1){v6}{$v_6$}}
        \mcnode{(1,0){v7}{$v_7$}}
        \mcnode{(2,0){v8}{$v_8$}}
        \ncline{v1}{v2}
        \ncline{v1}{v3}
        \ncline{v1}{v4}
        \ncline{v1}{v5}
        \ncline{v2}{v5}
        \ncline{v2}{v6}
        \ncline{v3}{v4}
        \ncline{v4}{v5}
        \ncline{v4}{v7}
        \ncline{v5}{v8}
        \ncline{v7}{v8}
    \end{pspicture}
	}
} 
\bigskip
    The degree sequence of the graph is
    $$(4, 4, 4, 3, 2, 2, 2, 1)$$
    Its degree frequency distribution
    is
    $$(N_0, N_1, N_2, N_3, N_4) = (0, 1, 3, 1, 3)$$
	The degree
    distribution is given as
    $$\bigl(f(0), f(1), f(2), f(3), f(4)\bigr) = (0, 0.125, 0.375, 0.125, 0.375)$$

\end{frame}



\begin{frame}{Path, Distance and Connectedness}
  \small
A {\em walk} in a graph $G$ between
nodes $x$ and $y$ is an ordered sequence of vertices, starting at
$x$ and ending at $y$,
$$x= v_0,\; v_1,\; \ldots,\; v_{t-1},\; v_t=y$$
such that there is
an edge between every pair of consecutive vertices,
that is, ${(v_{i-1}, v_i) \in E}$ for all $i=1, 2, \ldots, t$.  
The
length of the walk, $t$, is measured in terms of {\em hops} --
the number of edges along the walk.  

\medskip
A {\em path} is a walk with {\em distinct} vertices (with the
exception of the start and end vertices). 
A path of minimum length between nodes
$x$ and $y$ is called a {\em
shortest path}, and the length of the shortest path is called
the {\em distance} between $x$ and $y$, denoted as $d(x,y)$.  If
no path exists between the two nodes, the distance is assumed to
be $d(x,y) = \infty$.

\medskip
Two nodes $v_i$ and $v_{j}$ are 
{\em connected} if there exists a path between them.  A graph
is {\em connected} if there is a path between all pairs of
vertices. A {\em connected component}, or just {\em component},
of a graph is a maximal connected subgraph. 

\medskip
A directed graph is {\em strongly connected}
if there is a (directed) path between all ordered pairs of
vertices. It is {\em weakly connected} if there
exists a path between node pairs only by considering edges as
undirected.
\end{frame}


\begin{frame}{Adjacency Matrix}

A graph $G=(V,E)$, with $|V|=n$ vertices, can be
represented as an $n \times n$, symmetric binary {\em
adjacency matrix}, $\bA$, def\/{i}ned as
\begin{align*}
    \bA(i,j) =
    \begin{cases}
        1 & \text{if } v_i \text{ is adjacent to } v_{j}\\
        0 & \text{otherwise}
    \end{cases}
\end{align*}
If the graph is
directed, then the adjacency matrix $\bA$ is not symmetric.
%

\bigskip
If the graph is weighted, then we obtain an
$n \times n$ {\em weighted adjacency matrix}, $\bA$, def\/{i}ned as
\begin{align*}
    \bA(i,j) = \begin{cases}
        w_{ij} & \text{if } v_i \text{ is adjacent to } v_{j}\\
        0 & \text{otherwise}
    \end{cases}
\end{align*}
where $w_{ij}$ is the weight on edge $(v_i, v_{j}) \in E$.
\end{frame}


\begin{frame}{Graphs from Data Matrix}
Many datasets that are not in the form of a graph can
still be converted into one. 

\medskip
Let
$\bD = \{ \bx_i \}_{i=1}^n$ (with $\bx_i \in \setR^d$), be a
dataset.
Def\/{i}ne a weighted graph $G=(V,E)$,
with edge weight
$$w_{ij} = sim(\bx_i, \bx_{j})$$
where $sim(\bx_i, \bx_{j})$ denotes the
similarity between points $\bx_i$ and $\bx_{j}$.

\medskip
For instance, using the Gaussian
similarity 
\begin{align*} 
  w_{ij} = sim(\bx_i, \bx_{j}) =
    \exp \lB\{- {\norm{\bx_i - \bx_{j}}^2 \over 2\sigma^2} \rB\}
\end{align*}
where $\sigma$ is the spread parameter.
\end{frame}


\begin{frame}{Iris Similarity Graph: Gaussian Similarity}
  \framesubtitle{$\sigma=1/\sqrt{2}$; edge exists iff $w_{ij} \ge
  0.777$\\
  order: $|V|=n=150$; size: $|E|=m=753$}
\centering
        \scalebox{0.7}{
        \psset{unit=0.75in,dotscale=2}
        \begin{pspicture}(5,5)
            \input{EDA/graph/iris-graph}
        \end{pspicture}
        }
\end{frame}

\begin{frame}{Topological Graph Attributes}

Graph attributes
are {\em local} if they apply to only a single node
(or an edge), and
{\em global} if they refer to the entire graph.

\medskip
{\bf Degree:}
The degree of a node $v_i\in G$ is defined as
\begin{align*}
d_i = \sum_{j} \bA(i,j)
\end{align*}
The corresponding global attribute for the entire graph $G$ 
is the {\em average degree}:
\begin{align*}
\mu_d = {\sum_i d_i \over n}
\end{align*}

\medskip
{\bf Average Path Length:}
The {\em average path length}
is given as
\begin{align*}
    \mu_L =  {\sum_i \sum_{j>i} d(v_i, v_{j}) \over {n \choose 2}}
    = {2 \over n(n-1)} \sum_i \sum_{j>i} d(v_i, v_{j})
\end{align*}
\end{frame}


\begin{frame}{Iris Graph: Degree Distribution}
\begin{center}
\scalebox{0.9}{
    \psset{xAxisLabel=Degree:\ $k$,yAxisLabel=$f(k)$,%
    xAxisLabelPos={c,-1.5},yAxisLabelPos={-5,c}}
    \pstScalePoints(1,100){}{}
    \begin{psgraph}[Dy=0.01,dy=1,Dx=2,
        Ox=-1,showorigin=false]{->}(-10,0)(36,11){4.5in}{2.5in}
        \listplot[plotstyle=bar,barwidth=0.2cm,
        fillcolor=lightgray,fillstyle=solid]{%
0.00 0.04
1.00000000 0.08666667
2.00000000 0.05333333
3.00 0.04
4.00000000 0.03333333
5.00000000 0.05333333
6.00000000 0.05333333
7.00000000 0.08666667
8.00000000 0.06666667
9.00 0.04
10.00  0.06
11.00  0.04
12.00000000  0.04666667
13.00  0.04
14.00000000  0.03333333
15.000000000  0.006666667
16.000000000  0.006666667
17.00000000  0.01333333
18.00000000  0.02666667
19.00000000  0.02666667
20.00  0.02
21.00000000  0.02666667
22.00000000  0.03333333
23.000000000  0.006666667
24.00  0.02
25  0
26.000000000  0.006666667
27  0
28.000000000  0.006666667
29.00000000  0.01333333
30.000000000  0.006666667
31  0
32  0
33  0
34.000000000  0.006666667
}
\uput[80](0 , 3.9735099 ) {\scriptsize 6 }
\uput[80](1 , 8.6092715 ) {\scriptsize 13}
\uput[80](2 , 5.2980132 ) {\scriptsize 8 }
\uput[80](3 , 3.9735099 ) {\scriptsize 6 }
\uput[80](4 , 3.3112583 ) {\scriptsize 5 }
\uput[80](5 , 5.2980132 ) {\scriptsize 8 }
\uput[80](6 , 5.2980132 ) {\scriptsize 8 }
\uput[80](7 , 8.6092715 ) {\scriptsize 13}
\uput[80](8 , 6.6225166 ) {\scriptsize 10}
\uput[80](9 , 3.9735099 ) {\scriptsize 6 }
\uput[80](10, 5.9602649) {\scriptsize 9 }
\uput[80](11, 3.9735099) {\scriptsize 6 }
\uput[80](12, 4.6357616) {\scriptsize 7 }
\uput[80](13, 3.9735099) {\scriptsize 6 }
\uput[80](14, 3.3112583) {\scriptsize 5 }
\uput[80](15, 0.6622517) {\scriptsize 1 }
\uput[80](16, 0.6622517) {\scriptsize 1 }
\uput[80](17, 1.3245033) {\scriptsize 2 }
\uput[80](18, 2.6490066) {\scriptsize 4 }
\uput[80](19, 2.6490066) {\scriptsize 4 }
\uput[80](20, 1.9867550) {\scriptsize 3 }
\uput[80](21, 2.6490066) {\scriptsize 4 }
\uput[80](22, 3.3112583) {\scriptsize 5 }
\uput[80](23, 0.6622517) {\scriptsize 1 }
\uput[80](24, 1.9867550) {\scriptsize 3 }
\uput[80](25, 0.0000000) {\scriptsize 0 }
\uput[80](26, 0.6622517) {\scriptsize 1 }
\uput[80](27, 0.0000000) {\scriptsize 0 }
\uput[80](28, 0.6622517) {\scriptsize 1 }
\uput[80](29, 1.3245033) {\scriptsize 2 }
\uput[80](30, 0.6622517) {\scriptsize 1 }
\uput[80](31, 0.0000000) {\scriptsize 0 }
\uput[80](32, 0.0000000) {\scriptsize 0 }
\uput[80](33, 0.0000000) {\scriptsize 0 }
\uput[80](34, 0.6622517) {\scriptsize 1 }
    \end{psgraph}
}
\end{center}
\end{frame}


\begin{frame}{Iris Graph: Path Length Histogram}
    \begin{center}
     \scalebox{0.9}{
        \psset{xAxisLabel=Path Length: $k$,yAxisLabel=Frequency,%
        xAxisLabelPos={c,-150},yAxisLabelPos={-1.5,c}}
        \begin{psgraph}[dy=100,Dy=100]{->}(0,0)(12,1100){4in}{2.5in}
            \listplot[plotstyle=bar,barwidth=0.6cm,
            fillcolor=lightgray,fillstyle=solid]{%
            1 753 2 1044 3 831 4 668 5 529 6 330 7 240 8 146 %
            9 90 10 30 11 12}
            %\listplot[PSfont=CM-Bright,plotstyle=values,decimals=0]{%
            %0.75 753 1.5 1044 2.75 831 3.75 668 4.75 529 %
            %5.75 330 6.75 240 7.75 146 8.75 90 9.75 30 10.75 12}
            \uput[90](1  ,753 ) {\scriptsize 753}
            \uput[90](2   ,1044){\scriptsize 1044}
            \uput[90](3  ,831 ) {\scriptsize 831}
            \uput[90](4  ,668 ) {\scriptsize 668}
            \uput[90](5  ,529 ) {\scriptsize 529}
            \uput[90](6  ,330 ) {\scriptsize 330}
            \uput[90](7  ,240 ) {\scriptsize 240}
            \uput[90](8  ,146 ) {\scriptsize 146}
            \uput[90](9  ,90  ) {\scriptsize 90}
            \uput[90](10  ,30  ){\scriptsize 30}
            \uput[90](11 ,12  ) {\scriptsize 12}
        \end{psgraph}
    }
    \end{center}
\end{frame}


\begin{frame}{Eccentricity, Radius and Diameter}
The {\em eccentricity} of a node $v_i$ is the maximum distance
from $v_i$ to any other node in the graph:
\begin{align*}
e(v_i) = \max_{j} \bigl\{ d(v_i, v_{j}) \bigr\}
\end{align*}

\medskip
The {\em radius} of a connected
graph, denoted $r(G)$, is the minimum eccentricity of any node in
the graph:
\begin{align*}
r(G) = \min_{i} \bigl\{ e(v_i) \bigr\} =
\min_i \Bigl\{\max_{j} \bigl \{ d(v_i, v_{j}) \bigr\} \Bigr\}
\end{align*}

\medskip
The {\em diameter},
denoted $d(G)$, is the maximum eccentricity of any vertex in the
graph:
\begin{align*}
d(G) = \max_{i} \bigl\{ e(v_i) \bigr\} = \max_{i,j} \bigl\{
d(v_i, v_{j})\bigr\}
\end{align*}

For a disconnected graph, values are
computed over the connected components of the graph.

\medskip
The diameter of a graph $G$ is sensitive to outliers.
{\em Effective diameter} is more robust; def\/{i}ned as the minimum
number of hops for which a large fraction, typically 90\%, of all
connected pairs of nodes can reach each other. 
\end{frame}




\begin{frame}{Clustering Coeff\/{i}cient}
The {\em clustering coeff\/{i}cient} of a node $v_i$ is a measure of
the density of edges in the neighborhood of $v_i$.

\smallskip
Let $G_i=(V_i, E_i)$ be the subgraph induced by the neighbors
of vertex $v_i$. Note that $v_i \not\in V_i$, as we assume that $G$
is simple.

\smallskip
Let $|V_i| = n_i$ be the number of neighbors of
$v_i$, and $|E_i| = m_i$ be the number of edges among the
neighbors of $v_i$.
The clustering coeff\/{i}cient of $v_i$ is def\/{i}ned as
\begin{align*}
    C(v_i) & = \frac{\text{no. of edges in $G_i$}}{\text{maximum
    number of edges in $G_i$}} = {m_i \over {n_i \choose 2}}
    = \frac{2 \cdot m_i}{n_i (n_i-1)}
\end{align*}

The {\em clustering coeff\/{i}cient} of a graph $G$ is simply the
average clustering coeff\/{i}cient over all the nodes, given as
\begin{align*}
    C(G) = {1 \over n} \sum_i C(v_i)
\end{align*}
$C(v_i)$ is well def\/{i}ned only for nodes with
degree $d(v_i) \ge 2$, thus
def\/{i}ne ${C(v_i) = 0}$ if $d_i < 2$.
\end{frame}
%

\begin{frame}{Transitivity and Efficiency}

  {\em Transitivity} of the graph is def\/{i}ned
as
\begin{align*}
    T(G) = \frac{3 \times \text{no. of triangles in } G}
    {\text{no. of connected triples in } G}
\end{align*}
where the subgraph composed of the edges
$(v_i,v_{j})$ and $(v_i, v_k)$ is a {\em connected triple}
centered at $v_i$, and a connected
triple centered at $v_i$ that includes $(v_{j}, v_k)$ is called a
{\em triangle} (a complete
subgraph of size 3).


\bigskip
The {\em eff\/{i}ciency} for a pair of nodes $v_i$ and $v_{j}$
is def\/{i}ned as
${1 \over d(v_i, v_{j})}$.
If $v_i$ and $v_{j}$ are not connected, then $d(v_i,v_{j}) = \infty$
and the eff\/{i}ciency is $1/\infty = 0$.

The {\em eff\/{i}ciency} of a graph $G$ is the average
eff\/{i}ciency over all pairs of nodes, whether
connected or not, given as
\begin{align*}
    {2 \over n(n-1)} \sum_{i} \sum_{j>i} {1 \over d(v_i, v_{j})}
    %\label{eq:eda:graph:eff}
\end{align*}

\end{frame}



\def\mcnode#1{\cnodeput#1}
\begin{frame}{Clustering Coefficient}
  \begin{columns}
	\column{0.5\textwidth}
  \centerline{
\scalebox{0.9}{
    \begin{pspicture}(3,2.2)
        \mcnode{(1,2){v1}{$v_1$}}
        \mcnode{(2,2){v2}{$v_2$}}
        \mcnode{(0,1){v3}{$v_3$}}
        \mcnode{(1,1){v4}{$v_4$}}
        \mcnode{(2,1){v5}{$v_5$}}
        \mcnode{(3,1){v6}{$v_6$}}
        \mcnode{(1,0){v7}{$v_7$}}
        \mcnode{(2,0){v8}{$v_8$}}
        \ncline{v1}{v2}
        \ncline{v1}{v3}
        \ncline{v1}{v4}
        \ncline{v1}{v5}
        \ncline{v2}{v5}
        \ncline{v2}{v6}
        \ncline{v3}{v4}
        \ncline{v4}{v5}
        \ncline{v4}{v7}
        \ncline{v5}{v8}
        \ncline{v7}{v8}
    \end{pspicture}
	}
} 
\column{0.5\textwidth}
\small
\centerline{Subgraph induced by node $\textit{v}_4$:}
    \centerline{
	\scalebox{0.8}{
    \psset{unit=0.75in}
    \begin{pspicture}(0,-0.25)(2,2.2)
        \mcnode{(1,2){v1}{$v_1$}}
        \mcnode{(0,1){v3}{$v_3$}}
        \mcnode{(2,1){v5}{$v_5$}}
        \mcnode{(1,0){v7}{$v_7$}}
        \ncline{v1}{v3}
        \ncline{v1}{v5}
    \end{pspicture}
	}}
    The
    clustering coeff\/{i}cient of $v_4$ is
    \begin{align*}
        C(v_4) = {2 \over {4 \choose 2}} = {2 \over 6} = 0.33
    \end{align*}
    The clustering coeff\/{i}cient for $G$ is
    \begin{align*}
        C(G) = {1\over 8} \lB({1\over 2} + {1 \over 3} + 1 +
        {1\over 3} + {1 \over 3}\rB) = {2.5 \over 8}
        = 0.3125
	\end{align*}
  \end{columns}
\end{frame}


\begin{frame}{Centrality Analysis}
A centrality is a function $c\!: V \to \setR$, that induces a ranking on
$V$. 

\medskip
{\bf Degree Centrality:}
The simplest notion of centrality is the degree $d_i$ of a
vertex $v_i$ --
the higher the degree, the more important or central the vertex.

\medskip
{\bf Eccentricity Centrality:}
%\index{graph!centrality!eccentricity}
Eccentricity centrality is def\/{i}ned as:
\begin{align*}
    c(v_i) = {1 \over e(v_i)} = {1 \over \max_{j} \lB\{ d(v_i, v_{j}) \rB\}}
\end{align*}
The less eccentric a node is, the more
central it is. 

\medskip
{\bf Closeness Centrality:}
closeness centrality uses the sum of all
the distances to rank how central a node is
\begin{align*}
    c(v_i) = {1\over \sum_j d(v_i, v_{j}) }
\end{align*}

\end{frame}



\begin{frame}{Betweenness Centrality}
The betweenness centrality measures how
many shortest paths between all pairs of vertices include $v_i$.
It gives an indication as to the central ``monitoring'' role
played by $v_i$ for various pairs of nodes.

\medskip
Let $\eta_{jk}$
denote the number of shortest paths between vertices $v_{j}$
 and $v_k$, and let $\eta_{jk}(v_i)$ denote the number of such
 paths that include or contain $v_i$. 
 
\medskip
 The fraction of
 paths through $v_i$ is denoted as
 \begin{align*}
     \gamma_{jk}(v_i) = {\eta_{jk}(v_i) \over \eta_{jk}}
 \end{align*}

\medskip
 The betweenness centrality for a node $v_i$ is def\/{i}ned as
 \begin{align*}
     c(v_i) = \sum_{j \ne i} \sum_{\substack{k \ne i\\k>j}} \gamma_{jk}
     = \sum_{j \ne i} \sum_{\substack{k \ne i\\k>j}} {\eta_{jk}(v_i) \over \eta_{jk}}
 \end{align*}

 \end{frame}

 
 
 \begin{frame}{Centrality Values}
  \centerline{
\scalebox{0.8}{
    \begin{pspicture}(3,2.2)
        \mcnode{(1,2){v1}{$v_1$}}
        \mcnode{(2,2){v2}{$v_2$}}
        \mcnode{(0,1){v3}{$v_3$}}
        \mcnode{(1,1){v4}{$v_4$}}
        \mcnode{(2,1){v5}{$v_5$}}
        \mcnode{(3,1){v6}{$v_6$}}
        \mcnode{(1,0){v7}{$v_7$}}
        \mcnode{(2,0){v8}{$v_8$}}
        \ncline{v1}{v2}
        \ncline{v1}{v3}
        \ncline{v1}{v4}
        \ncline{v1}{v5}
        \ncline{v2}{v5}
        \ncline{v2}{v6}
        \ncline{v3}{v4}
        \ncline{v4}{v5}
        \ncline{v4}{v7}
        \ncline{v5}{v8}
        \ncline{v7}{v8}
    \end{pspicture}
	}
} 
\begin{center}
  \small
\begin{tabular}{@{}|l||c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|@{}}
\hline
{\bf Centrality} &\mc{1}{c|}{$v_1$} &\mc{1}{c|}{$v_2$} &\mc{1}{c|}{$v_3$} &\mc{1}{c|}{$v_4$} &\mc{1}{c|}{$v_5$} &\mc{1}{c|}{$v_6$}
&\mc{1}{c|}{$v_7$} &\mc{1}{c|}{$v_8$}\\
\hline\hline
    Degree & 4 & 3 & 2 & 4 & 4 & 1 & 2 & 2\\
\hline
    Eccentricity & 0.5 & 0.33 & 0.33 & 0.33 & 0.5 & 0.25 & 0.25 &
    0.33\\
    $e(v_i)$ & 2 & 3 & 3 & 3 & 2 & 4 & 4 & 3\\
\hline
    Closeness & 0.100 & 0.083 & 0.071 & 0.091 & 0.100 & 0.056
     & 0.067 & 0.071\\
    $\sum_j d(v_i, v_{j})$ & 10 & 12 & 14 & 11 & 10 & 18 &15 &14\\
\hline
    Betweenness & 4.5 & 6 & 0 & 5 & 6.5 & 0 & 0.83 & 1.17\\
\hline
\end{tabular}%}
%{}
\end{center}
\end{frame}


\begin{frame}{Prestige or Eigenvector Centrality}


Let $p(u)$ be a positive real number, called the {\em prestige}
score for node $u$. 
Intuitively the more (prestigious) the links that
point to a given node, the higher its prestige. 
\begin{align*}
  p(v) & = \sum_u \bA(u,v) \cdot p(u) \\
  & = \sum_u \bA^T(v,u) \cdot p(u)
\end{align*}

Across all the nodes, we have
\begin{align*}
\bp' = \bA^T \bp
\end{align*}
where $\bp$ is an $n$-dimensional prestige vector.

By recursive expansion, we see that
\begin{align*}
  \bp_k & = \bA^T \bp_{k-1} = \lB(\bA^T\rB)^2 \bp_{k-2}
	= \cdots = \lB(\bA^T\rB)^k \bp_{0}
\end{align*}
where $\bp_0$ is the initial prestige vector.
It is well known that the vector $\bp_k$ converges to the
dominant eigenvector of $\bA^T$.
\end{frame}
%

\begin{frame}{Computing Dominant Eigenvector: Power Iteration}
\begin{columns}
  \column{0.4\textwidth}
\small
The dominant eigenvector of $\bA^T$ and the corresponding eigenvalue can
be computed using the {\em power iteration} method.

\smallskip
It
starts with an initial vector $\bp_0$, and
in each iteration, it multiplies on the
left by $\bA^T$, and scales the intermediate $\bp_k$ vector by dividing
it by the maximum entry $\bp_k[i]$ in $\bp_k$ to prevent numeric
overflow. 

\smallskip
The ratio of the maximum entry in iteration $k$ to that in
$k-1$, given as $\lambda = \tfrac{\bp_k[i]}{\bp_{k-1}[i]}$, yields an
estimate for the eigenvalue. 

\smallskip
The iterations continue until the
difference between successive eigenvector estimates falls below some
threshold $\epsilon > 0$.

\column{0.55\textwidth}
\newcommand{\PowerMethod}{\textsc{PowerIteration}}
\SetKwInOut{Algorithm}{\PowerMethod\ ($\bA, \epsilon$)}
\Algorithm{}
$k \assign 0$ \tcp{iteration}\;
$\bp_0 \assign \bone \in \setR^n$\tcp{initial vector}\;
\Repeat{$\norm{\bp_k - \bp_{k-1}} \le \epsilon$}{
  $k \assign k +1$\;
  $\bp_k \assign \bA^T \bp_{k-1}$ \tcp{eigenvector estimate}\;
  $i \assign \arg \max_{j} \bigl\{ \bp_{k}[j] \bigr\}$
  \tcp{maximum value index}\;
  $\lambda \assign \bp_k[i]/\bp_{k-1}[i]$ \tcp{eigenvalue estimate}\;
  $\bp_k \assign \frac{1}{\bp_{k}[i]}\bp_k$ \tcp{scale vector}\;
}
$\bp \assign \frac{1}{\norm{\bp_k}} \bp_k$ \tcp{normalize eigenvector}\;
\Return{$\bp, \lambda$}
\end{columns}
\end{frame}


\begin{frame}{Power Iteration for Eigenvector Centrality: Example}
  \centerline{
  %\subfloat[]{
      \psset{unit=1in}
      \psset{arrows=->,arrowscale=1.5,treesep=0.75,levelsep=1,treemode=R}
      \centering
        \pstree[]{\Tcircle[name=u1]{$v_1$}}{
          \pstree[]{\Tcircle[name=u4]{$v_4$}}{
            \Tcircle[name=u5]{$v_5$}
          }
          \pstree[arrows=<-]{\def\psedge{\ncline[arrows=<-]}
            \Tcircle[name=u3]{$v_3$}}{
            \Tcircle[name=u2]{$v_2$}
          }
        }
        \ncarc[arcangle=40]{u5}{u2}
        \ncarc[arcangle=40]{u2}{u5}
        \ncline[]{u4}{u3}
        \ncline[]{u4}{u5}
        \ncarc[arcangle=-20]{u4}{u2}
        }%}
  \vspace{0.1in}
  \centerline{
%  \subfloat[]{
      $\bA = \matr{
          0 & 0 & 0 & 1 & 0\\
          0 & 0 & 1 & 0 & 1\\
          1 & 0 & 0 & 0 & 0\\
          0 & 1 & 1 & 0 & 1\\
          0 & 1 & 0 & 0 & 0\\
      }$
      \hspace{0.5in}
%      \subfloat[]{
      $\bA^T = \matr{
          0 & 0 & 1 & 0 & 0\\
          0 & 0 & 0 & 1 & 1\\
          0 & 1 & 0 & 1 & 0\\
          1 & 0 & 0 & 0 & 0\\
          0 & 1 & 0 & 1 & 0\\
      }$
      }
      %}}
\end{frame}


\begin{frame}{Power Method via Scaling}
\begin{center}
  \scalebox{0.8}{
\begin{tabular}{|c|c|c|c|}
  \hline
  $\bp_0$ & $\bp_1$ & $\bp_2$ & $\bp_3$ \\
  \hline
  $\matr{1\\1\\1\\1\\1}$ &
  $\matr{1\\2\\2\\1\\2} \to \matr{0.5\\1\\1\\0.5\\1}$ &
  $\matr{1\\1.5\\1.5\\0.5\\1.5} \to \matr{0.67\\1\\1\\0.33\\1}$
  &
  $\matr{1\\1.33\\1.33\\0.67\\1.33} \to
  \matr{0.75\\1\\1\\0.5\\1}$ \\
  \hline
  $\lambda$ & 2 & 1.5 & 1.33\\
  \hline
  \multicolumn{4}{c}{~}\\
  \hline
  $\bp_4$ & $\bp_5$ & $\bp_6$
        & $\bp_7$\\
  \hline
  $\matr{1\\1.5\\1.5\\0.75\\1.5} \to \matr{0.67\\1\\1\\0.5\\1}$ &
  $\matr{1\\1.5\\1.5\\0.67\\1.5} \to \matr{0.67\\1\\1\\0.44\\1}$ &
  $\matr{1\\1.44\\1.44\\0.67\\1.44} \to \matr{0.69\\1\\1\\0.46\\1}$ &
  $\matr{1\\1.46\\1.46\\0.69\\1.46} \to \matr{0.68\\1\\1\\0.47\\1}$ \\
  \hline
   1.5 & 1.5 & 1.444 & 1.462\\
   \hline
\end{tabular}%}
}
\end{center}
\end{frame}


\begin{frame}{Convergence of the Ratio to Dominant Eigenvalue}
  \centering
    \psset{xunit=0.5cm,yunit=4cm}
    \pspicture[](0,1.25)(16,2.25)
    \psline[linecolor=gray,linewidth=1pt]{-}(0,1.46557)(16,1.46557)
    \psaxes[Oy=0.5,Dy=0.25,Ox=0,Oy=1.25,Dx=2,axesstyle=frame]{->}(0,1.25)(16,2.25)
    %\dataplot[plotstyle=line,showpoints=true,dotstyle=o,dotsize=8pt]{\eigdata}
    \listplot[plotstyle=line,showpoints=true,dotstyle=o,dotscale=2]{%
    1   2
    2   1.5
    3   1.333333
    4   1.5
    5   1.5
    6   1.444444
    7   1.461538
    8   1.473684
    9   1.464286
    10   1.463415
    11   1.466667
    12   1.465909
    13   1.465116
    14   1.465608
    15   1.465704
    16   1.465517
    %17   1.465546
    }
    \uput[0](16.125,1.465){$\lambda=1.466$}
    \endpspicture
\end{frame}


\begin{frame}{PageRank}
PageRank is based on (normalized) prestige combined with a 
{\em random jump} assumption. 
The PageRank of
a node $v$ recursively depends on the PageRank of other nodes that
point to it.

\medskip
{\bf Normalized Prestige:}
Define $\bN$ as the {\em normalized adjacency matrix}
\begin{align*}
  \bN(u,v) =
  \begin{cases}
    \frac{1}{od(u)} & \mbox{if } (u,v) \in E\\
    0 & \mbox{if } (u,v) \not\in E
  \end{cases}
\end{align*}
where $od(u)$ is the out-degree of node $u$.

Normalized prestige is given as
\begin{align*}
  p(v)  & = \sum_u \bN^T(v,u) \cdot p(u)
\end{align*}


\medskip
{\bf Random Jumps:}
In the random surf\/{i}ng approach, there is a small probability of
jumping from one node to any of the other nodes in the graph.
The normalized adjacency matrix for a fully connected graph is 
$$\bN_r = {1\over n} \bone_{n \times n}$$
where $\bone_{n \times n}$ is the $n\times n$ matrix of all ones.
\end{frame}


\begin{frame}{PageRank: Normalized Prestige and Random Jumps}
The PageRank vector is recursively defined as
\begin{align*}
  \bp' & = (1-\alpha) \bN^T \bp + \alpha \bN_r^T \bp \\
    & = \left((1-\alpha) \bN^T + \alpha\bN_r^T \right) \bp\\
    & = \bM^T \bp
\end{align*}
$\alpha$ denotes the probability of random jumps.
The solution is the dominant eigenvector of $\bM^T$, where 
$\bM = (1-\alpha) \bN +
\alpha\bN_r$ is the combined normalized
adjacency matrix.

\medskip
{\bf Sink Nodes:} If $od(u) = 0$, then only random jumps from $u$ are
allowed. The modified $\bM$ matrix is given as
\begin{align*}
    \bM_u =
    \begin{cases}
        \bM_u & \text{if } od(u)>0\\
        {1\over n}\bone_n^T & \text{if } od(u)=0
    \end{cases}
\end{align*}
where $\bone_n$ is the $n$-dimensional vector of all ones.
\end{frame}


\begin{frame}{Hub and Authority Scores (HITS)}
  \small
The
{\em authority score} of a page is analogous to PageRank or prestige,
and it depends on how many ``good'' pages point to it. 
The {\em hub score} of a page is based on how many ``good'' pages
it points to. 
In other words, a page with high authority has many hub
pages pointing to it, and a page with high hub score points to many
pages that have high authority.

\medskip
Let $a(u)$ be the authority score and $h(u)$ the hub score of
node $u$. We have:
\begin{align*}
  a(v) = \sum_u \bA^T(v,u)\cdot h(u)\\
  h(v) = \sum_u \bA(v,u)\cdot a(u)
\end{align*}
In matrix notation, we obtain
\begin{align*}
  \ba' & = \bA^T \bh & 
  \bh' & = \bA \ba
\end{align*}

\medskip
Recursively, we have:
\begin{align*}
  \ba_k = \bA^T \bh_{k-1} = \bA^T (\bA \ba_{k-1}) = (\bA^T\bA) \ba_{k-1} \\
  \bh_k = \bA \ba_{k-1} = \bA (\bA^T \bh_{k-1}) = (\bA\bA^T) \bh_{k-1}
\end{align*}
The authority score converges to the
dominant eigenvector of $\bA^T\bA$, whereas the hub score converges to
the dominant eigenvector of $\bA\bA^T$. 
\end{frame}


\begin{frame}{Small World Property}

Real-world graphs exhibit the
 {\em small-world} property that there is a short path
between any pair of nodes.
A
graph $G$ exhibits small-world behavior if the average path
length $\mu_L$ scales logarithmically with the number of nodes in the
graph, that is, if
\begin{align*}
    \mu_L \propto \log n
\end{align*}
where $n$ is the number of nodes in the graph.

\medskip
A graph is said to have {\em ultra-small-world} property if the
average path length is much smaller than $\log n$, that is, if
$\mu_L \ll \log n$.
\end{frame}



\begin{frame}{Scale-free Property}
In many real-world graphs it has been observed that the
empirical degree
distribution $f(k)$ exhibits a {\em scale-free} behavior captured
by a power-law relationship with $k$, that is, the probability that
a node has degree $k$ satisf\/{i}es the condition
\begin{align*}
    f(k) \propto k^{-\gamma}
\end{align*}

\medskip


\medskip
Taking the logarithm on both sides gives
\begin{align*}
    \log f(k) & = \log (\alpha k^{-\gamma}) \notag\\
    \text{or } \log f(k) & = - \gamma \log k + \log \alpha
\end{align*}
which is the equation of a
straight line in the log-log plot of $k$ versus $f(k)$, with
$-\gamma$ giving the slope of the line. 

\medskip
A power-law relationship leads to a
scale-free or scale invariant
behavior because scaling the argument by some constant $c$
does not change the
proportionality.


\end{frame}


\begin{frame}{Clustering Effect}
Real-world graphs often also exhibit a {\em clustering effect}, that is,
two nodes are more likely to be connected if they share a common
neighbor. The clustering effect is captured by a high clustering
coeff\/{i}cient for the graph $G$. 

\medskip
Let $C(k)$ denote the \hbox{average}
clustering coeff\/{i}cient for all nodes with degree $k$; then the
clustering effect also manifests itself as a power-law
relationship between $C(k)$ and $k$:
\begin{align*}
    C(k) \propto k^{-\gamma}
\end{align*}

In other words, a log-log plot of $k$ versus $C(k)$ exhibits
a straight line behavior with negative slope $-\gamma$.

\end{frame}


\readdata{\dataHPRD}{EDA/graph/figs/hprd-deg.dat}
\begin{frame}{Degree Distribution: Human Protein Interaction Network}
  \framesubtitle{$|V|=n=9521$, $|E|=m=37060$}
  \centerline{
    \scalebox{1}{%
    \psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray,
            arrowscale=2,PointName=none}
    \psset{xAxisLabel=Degree: $\log_2 k$,
            yAxisLabel=Probability: $\log_2 f(k)$,
            xAxisLabelPos={c,-0.4in},
            yAxisLabelPos={-0.6in,c}}
    \psgraph[tickstyle=bottom, Oy=-14,Dy=2]{->}(0,-14)(0,-14)(9,-1){3.5in}{2in}%
    \pstScalePoints(1,1){ln 2 ln div} {ln 2 ln div}
    \listplot[plotstyle=dots,showpoints=true,
    plotNo=1,plotNoMax=2]{\dataHPRD}
    \listplot[plotstyle=LSM,linewidth=1pt,
    linecolor=black,plotNo=1,plotNoMax=2,nStart=4,nEnd=100]{\dataHPRD}
    \rput[r](4.2,-3){$-\gamma=-2.15$}
    \endpsgraph
    }}%}
\end{frame}



\begin{frame}{Cumulative Degree Distribution}
  \framesubtitle{$F^c(k) = 1-F(k)$ where $F(k)$ is the CDF for $f(k)$}
    \centerline{
    \scalebox{1}{%
    \psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray,
            arrowscale=2,PointName=none}
    \psset{xAxisLabel=Degree: $\log_2 k$,
    yAxisLabel=Probability: $\log_2 F^c(k)$,
            xAxisLabelPos={c,-0.4in},
            yAxisLabelPos={-0.6in,c}}
    \psgraph[tickstyle=bottom, Oy=-14,Dy=2]{->}(0,-14)(0,-14)(9,1){3.5in}{2in}%
    \pstScalePoints(1,1){ln 2 ln div} {ln 2 ln div}
    \listplot[plotstyle=dots,showpoints=true,
    plotNo=2,plotNoMax=2]{\dataHPRD}
    \listplot[plotstyle=LSM,linewidth=1pt,
    linecolor=black,plotNo=2,plotNoMax=2,nStart=4]{\dataHPRD}
    \rput[r](5.8,-0.5){$-(\gamma-1)=-1.85$}
    \endpsgraph
    }}
\end{frame}


\readdata{\dataHPRDCk}{EDA/graph/figs/hprd-ck.dat}
\begin{frame}{Average Clustering Coeff\/{i}cient}
    \centerline{
  \hspace{0.3in}
    \psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray,
            arrowscale=2,PointName=none}
    \psset{xAxisLabel=Degree: $\log_2 k$,
            yAxisLabel=Average Clustering Coefficient: $\log_2 C(k)$,
            xAxisLabelPos={c,-0.4in},
            yAxisLabelPos={-0.6in,c}}
    \psgraph[tickstyle=bottom, Oy=-8,Dy=2,Ox=1]{->}(1,-8)(9,-1){3.75in}{2.25in}%
    \pstScalePoints(1,1){ln 2 ln div} {ln 2 ln div}
    \listplot[plotstyle=dots,showpoints=true]{\dataHPRDCk}
    \listplot[plotstyle=LSM,linewidth=1pt,
    linecolor=black,nStart=4,nEnd=128]{\dataHPRDCk}
    \rput[r](4,-2){$-\gamma=-0.55$}
    \endpsgraph
    }
\end{frame}


\begin{frame}{Erd\"{o}s--R\'{e}nyi Random Graph Model}

The ER model specif\/{i}es a collection of graphs $\cG(n,m)$ with $n$
nodes and $m$ edges, such
that each graph $G \in \cG$ has equal probability of being
selected:
$$P(G) = {1 \over {M \choose m}} = {M \choose m}^{-1}$$
where $M = {n \choose 2} = \frac{n(n-1)}{2}$ and
 ${M \choose m}$ is the number of
possible graphs with $m$ edges (with $n$ nodes).

\medskip
{\bf Random Graph Generation:}
Randomly
select two distinct vertices $v_i, v_{j} \in V$, and add an
edge $(v_i, v_{j})$ to $E$, provided the edge is not already in the
graph $G$. Repeat the process until exactly $m$ edges have
been added to the graph.

\medskip
Let $X$ be a random variable denoting the degree of a node for $G
\in \cG$.
Let $p$ denote the probability of an edge in $G$
\begin{align*}
    p = \frac{m}{M} = \frac{m}{ {n \choose 2}} = \frac{2m}{n(n-1)}
\end{align*}
\end{frame}


\begin{frame}{Random Graphs: Average Degree}
Degree of a node follows a binomial distribution with probability of
success $p$, given as
$$f(k) = P(X=k) = {n-1 \choose k} p^k (1-p)^{n-1-k}$$
since a node can be connected to $n-1$ other vertices.


\medskip
The average degree $\mu_d$ is then given as the expected value of
$X$:
\begin{align*}
    \mu_d = E[X] = (n-1)p
\end{align*}

\medskip
The variance of the degree is
\begin{align*}
    \sigma^2_d = var(X) = (n-1)p(1-p)
\end{align*}
\end{frame}


\begin{frame}{Random Graphs: Degree Distribution}

As $n \to \infty$ and $p \to 0$
the expected value and
variance of $X$ can be rewritten as
\begin{align*}
    E[X] & = (n-1)p \;\; \approx \; np \text{ as } n \to \infty\\
    var(X) & = (n-1)p(1-p) \;\; \approx \; np \text{ as } n \to \infty
    \text{ and } p \to 0
\end{align*}

\medskip
The binomial distribution can be approximated by a Poisson
distribution with parameter $\lambda$, given as
\begin{align*}
f(k) = {\lambda^k \mathrm{e}^{-\lambda} \over k!}
    %\label{eq:eda:graph:PoissonFK}
\end{align*}
where $\lambda = np$ represents both the expected value and
variance of the distribution.

\medskip
Thus, ER random graphs do not exhibit power law degree distribution.
\end{frame}


\begin{frame}{Random Graphs: Clustering Coefficient and Diameter}
\small
{\bf Clustering Coeff\/{i}cient:}
Consider a node $v_i$ with degree $k$.
Since $p$ is the
probability of an edge, the expected number of edges $m_i$ among
the neighbors of a node $v_i$ is simply
$$m_i = {p k (k-1) \over 2}$$
The clustering coefficient is 
$$C(v_i) = {2m_i \over k(k-1)} = p$$
which implies that $C(G) = {1 \over n} \sum_i C(v_i) = p$.
\smallskip
Since for sparse graphs we have $p \to 0$, this means that ER random
graphs do not show clustering effect.

\bigskip
{\bf Diameter:}
Expected degree of a node is $\mu_d=\lambda$, so in
one hop a node can reach $\lambda$ nodes. 
Coarsely, in $k$ hops it can reach $\lambda^k$ nodes. Thus, we have
$$\sum_{k=1}^t \lambda^k \le n,
\text{ which implies that }
t = \log_{\lambda} n$$
It follows that
the diameter of the graph is
$$d(G) \propto \log n$$
Thus, ER random graphs are small-world.
\end{frame}


\begin{frame}{Watts--Strogatz Small-world Graph Model}
The Watts--Strogatz (WS) model 
starts with a regular graph of degree $2k$, 
having $n$ nodes arranged in a circular
layout,
with each node having 
edges to its $k$ neighbors on the right and left.

The regular graph has high local clustering.
Adding a small amount
of randomness leads to
the emergence of the small-world phenomena.

\bigskip
\centerline{Watts--Strogatz Regular Graph: $n=8$, $k=2$}

    \begin{center}
	  \scalebox{0.7}{
        \psset{unit=0.5in}
        \begin{pspicture}(5,5)
            %nodes
            \cnodeput(2.500000 , 5.000000){n0}{$v_0$}
            \cnodeput(4.267767 , 4.267767){n1}{$v_1$}
            \cnodeput(5.0000000 ,2.5000000){n2}{$v_2$}
            %\cnodeput[fillcolor=lightgray,fillstyle=solid]
            %   (4.267767 , 4.267767){n1}{$v_1$}
            %\cnodeput[fillcolor=lightgray,fillstyle=solid]
            %   (5.0000000 ,2.5000000){n2}{$v_2$}
            \cnodeput(4.267767 , 0.732233){n3}{$v_3$}
            \cnodeput(2.500000 , 0.000000){n4}{$v_4$}
            \cnodeput(0.732233 , 0.732233){n5}{$v_5$}
            \cnodeput(0.0000000,2.5000000){n6}{$v_6$}
            \cnodeput(0.732233, 4.267767){n7}{$v_7$}
            %\cnodeput[fillcolor=lightgray,fillstyle=solid]
            %   (0.0000000,2.5000000){n6}{$v_6$}
            %\cnodeput[fillcolor=lightgray,fillstyle=solid]
            %   (0.732233, 4.267767){n7}{$v_7$}
            %edges
            \ncarc[arcangle=10]{n0}{n1}
            \ncarc[arcangle=10]{n1}{n2}
            %\ncarc[arcangle=10,linewidth=2pt]{n1}{n2}
            \ncarc[arcangle=10]{n2}{n3}
            \ncarc[arcangle=10]{n3}{n4}
            \ncarc[arcangle=10]{n4}{n5}
            \ncarc[arcangle=10]{n5}{n6}
            \ncarc[arcangle=10]{n6}{n7}
            %\ncarc[arcangle=10,linewidth=2pt]{n6}{n7}
            \ncarc[arcangle=10]{n7}{n0}
            \ncarc[arcangle=0]{n0}{n2}
            \ncarc[arcangle=0]{n1}{n3}
            \ncarc[arcangle=0]{n2}{n4}
            \ncarc[arcangle=0]{n3}{n5}
            \ncarc[arcangle=0]{n4}{n6}
            \ncarc[arcangle=0]{n5}{n7}
            \ncarc[arcangle=0]{n6}{n0}
            \ncarc[arcangle=0]{n7}{n1}
            %\ncarc[arcangle=0,linewidth=2pt]{n7}{n1}
        \end{pspicture}
	}
    \end{center}
\end{frame}


\begin{frame}{WS Regular Graph: Clustering Coeff\/{i}cient and Diameter}
The clustering coeff\/{i}cient of a node
$v$ is given as
\begin{align*}
    C(v) = {m_v \over M_v} = {3k-3 \over 4k-2}
    %\label{eq:eda:graph:swCC}
\end{align*}
As $k$ increases, the clustering
coeff\/{i}cient approaches $\tfrac{3}{4}$ because $C(G) = C(v) \to {3 \over 4}$ as $k \to \infty$.
The WS regular graph thus has a high clustering coeff\/{i}cient.


\medskip
The
diameter of a regular WS graph is given as
\begin{align*}
    d(G) = \begin{cases}
    \lB\lceil{n \over 2k}\rB\rceil & \text{if $n$ is even}\\
    \lB\lceil{n-1 \over 2k}\rB\rceil &  \text{if $n$ is odd}
    \end{cases}
\end{align*}
The regular graph has a diameter that scales linearly in the number
of nodes, and thus it is not small-world.
\end{frame}


\begin{frame}{Random Perturbation of Regular Graph}
  \medskip
{\bf Edge Rewiring:}
For each
edge $(u,v)$ in the graph, with probability $r$, replace $v$ with
another randomly chosen node avoiding loops and duplicate
edges. 

\smallskip
The WS regular graph has $m = kn$ total edges, so 
after rewiring, $rm$ of the edges are random, and $(1-r)m$ are regular.

\bigskip
{\bf Edge Shortcuts:}
Add 
a few {\em shortcut} edges between random pairs of nodes, with $r$ being 
the probability, per
edge, of adding a shortcut edge.

\smallskip
The
total number of random shortcut edges added to the network
is $mr = knr$.
The total number of edges in the
graph is $m + mr = (1+r)m = (1+r)kn$. 
\end{frame}


\begin{frame}{Watts--Strogatz Graph: Shortcut Edges}
  \framesubtitle{$n=20$, $k=3$}
    \begin{center}
      \scalebox{0.8}{%
        \psset{unit=0.6in,radius=0.06in}
        \begin{pspicture}(5,5.5)
            %nodes
            \psset{fillstyle=solid,fillcolor=lightgray}
            \Cnode(5.0000000 ,2.5000000){n00}
            \Cnode(4.8776413 ,3.2725425){n01}
            \Cnode(4.5225425 ,3.9694631){n02}
            \Cnode(3.9694631 ,4.5225425){n03}
            \Cnode(3.2725425 ,4.8776413){n04}
            \Cnode(2.5000000 ,5.0000000){n05}
            \Cnode(1.7274575 ,4.8776413){n06}
            \Cnode(1.0305369 ,4.5225425){n07}
            \Cnode(0.4774575 ,3.9694631){n08}
            \Cnode(0.1223587 ,3.2725425){n09}
            \Cnode(0.0000000 ,2.5000000){n10}
            \Cnode(0.1223587 ,1.7274575){n11}
            \Cnode(0.4774575 ,1.0305369){n12}
            \Cnode(1.0305369 ,0.4774575){n13}
            \Cnode(1.7274575 ,0.1223587){n14}
            \Cnode(2.5000000 ,0.0000000){n15}
            \Cnode(3.2725425 ,0.1223587){n16}
            \Cnode(3.9694631 ,0.4774575){n17}
            \Cnode(4.5225425 ,1.0305369){n18}
            \Cnode(4.8776413 ,1.7274575){n19}
            %edges k=1
            \psset{fillstyle=none}
            \ncarc[arcangle=-10]{n00}{n01}
            \ncarc[arcangle=-10]{n01}{n02}
            \ncarc[arcangle=-10]{n02}{n03}
            \ncarc[arcangle=-10]{n03}{n04}
            \ncarc[arcangle=-10]{n04}{n05}
            \ncarc[arcangle=-10]{n05}{n06}
            \ncarc[arcangle=-10]{n06}{n07}
            \ncarc[arcangle=-10]{n07}{n08}
            \ncarc[arcangle=-10]{n08}{n09}
            \ncarc[arcangle=-10]{n09}{n10}
            \ncarc[arcangle=-10]{n10}{n11}
            \ncarc[arcangle=-10]{n11}{n12}
            \ncarc[arcangle=-10]{n12}{n13}
            \ncarc[arcangle=-10]{n13}{n14}
            \ncarc[arcangle=-10]{n14}{n15}
            \ncarc[arcangle=-10]{n15}{n16}
            \ncarc[arcangle=-10]{n16}{n17}
            \ncarc[arcangle=-10]{n17}{n18}
            \ncarc[arcangle=-10]{n18}{n19}
            \ncarc[arcangle=-10]{n19}{n00}
            %edges k=2
            \ncarc[arcangle=-60]{n00}{n02}
            \ncarc[arcangle=-60]{n01}{n03}
            \ncarc[arcangle=-60]{n02}{n04}
            \ncarc[arcangle=-60]{n03}{n05}
            \ncarc[arcangle=-60]{n04}{n06}
            \ncarc[arcangle=-60]{n05}{n07}
            \ncarc[arcangle=-60]{n06}{n08}
            \ncarc[arcangle=-60]{n07}{n09}
            \ncarc[arcangle=-60]{n08}{n10}
            \ncarc[arcangle=-60]{n09}{n11}
            \ncarc[arcangle=-60]{n10}{n12}
            \ncarc[arcangle=-60]{n11}{n13}
            \ncarc[arcangle=-60]{n12}{n14}
            \ncarc[arcangle=-60]{n13}{n15}
            \ncarc[arcangle=-60]{n14}{n16}
            \ncarc[arcangle=-60]{n15}{n17}
            \ncarc[arcangle=-60]{n16}{n18}
            \ncarc[arcangle=-60]{n17}{n19}
            \ncarc[arcangle=-60]{n18}{n00}
            \ncarc[arcangle=-60]{n19}{n01}
            %edges k=3
            \ncarc[arcangle=-5]{n00}{n03}
            \ncarc[arcangle=-5]{n01}{n04}
            \ncarc[arcangle=-5]{n02}{n05}
            \ncarc[arcangle=-5]{n03}{n06}
            \ncarc[arcangle=-5]{n04}{n07}
            \ncarc[arcangle=-5]{n05}{n08}
            \ncarc[arcangle=-5]{n06}{n09}
            \ncarc[arcangle=-5]{n07}{n10}
            \ncarc[arcangle=-5]{n08}{n11}
            \ncarc[arcangle=-5]{n09}{n12}
            \ncarc[arcangle=-5]{n10}{n13}
            \ncarc[arcangle=-5]{n11}{n14}
            \ncarc[arcangle=-5]{n12}{n15}
            \ncarc[arcangle=-5]{n13}{n16}
            \ncarc[arcangle=-5]{n14}{n17}
            \ncarc[arcangle=-5]{n15}{n18}
            \ncarc[arcangle=-5]{n16}{n19}
            \ncarc[arcangle=-5]{n17}{n00}
            \ncarc[arcangle=-5]{n18}{n01}
            \ncarc[arcangle=-5]{n19}{n02}
            %shortcut edges
            \psset{linestyle=dotted,linewidth=2pt}
            \ncline{n09}{n19}
            \ncline{n02}{n08}
            \ncline{n00}{n13}
        \end{pspicture}
      }
    \end{center}
\end{frame}


\begin{frame}{Properties of Watts--Strogatz Graphs}
\small

{\bf Degree Distribution:}
Let $X$ denote the random variable denoting the number of
shortcuts for each node. Then the probability
of a node with $j$ shortcut edges is given
as
\begin{align*}
    f(j) = P(X=j) = {n' \choose j} p^{j} (1-p)^{n'-j}
\end{align*}
with $E[X] = n'p = 2kr$ and 
$p = {2kr \over n-2k-1} = {2kr \over n'}$.

\medskip
The expected degree of each
node in the network is therefore
$2k + E[X] = 2k(1+r)$.
The degree distribution is not a power law.

\bigskip
{\bf Clustering Coeff\/{i}cient:}
The clustering coefficient is 
\begin{align*}
    C(v) & \approx{3(k-1) \over (1+r)(4kr +
    2(2k-1))} = {3k-3 \over 4k-2 + 2r(2kr + 4k - 1)}
\end{align*}
Thus, for small values of $r$ the
clustering coeff\/{i}cient remains high.

\bigskip
{\bf Diameter:}
Small values of
shortcut edge probability $r$ are enough to reduce the diameter
from $O(n)$ to $O(\log n)$. 
\end{frame}



\def\WSdata{
0.000    0 167.0 0.6000000 0.6000000
0.005   15  60.5 0.5928934 0.5934542
0.010   30  37.7 0.5860925 0.5870152
0.015   45  29.0 0.5793523 0.5806808
0.020   60  25.8 0.5726690 0.5744485
0.025   75  21.7 0.5659058 0.5683164
0.030   90  19.6 0.5595481 0.5622821
0.035  105  18.3 0.5533186 0.5563437
0.040  120  17.4 0.5471221 0.5504991
0.045  135  16.1 0.5409936 0.5447464
0.050  150  14.7 0.5351035 0.5390836
0.055  165  14.2 0.5293898 0.5335088
0.060  180  13.4 0.5235273 0.5280203
0.065  195  13.3 0.5180469 0.5226162
0.070  210  12.6 0.5124421 0.5172949
0.075  225  12.1 0.5064227 0.5120546
0.080  240  12.4 0.5011826 0.5068938
0.085  255  11.8 0.4961249 0.5018107
0.090  270  11.3 0.4911021 0.4968039
0.095  285  11.3 0.4856155 0.4918718
0.100  300  10.7 0.4804644 0.4870130
0.105  315  10.5 0.4759515 0.4822260
0.110  330  10.2 0.4710369 0.4775093
0.115  345  10.0 0.4663479 0.4728617
0.120  360  10.1 0.4613466 0.4682817
0.125  375   9.7 0.4565051 0.4637681
0.130  390   9.5 0.4520970 0.4593196
0.135  405   9.5 0.4477671 0.4549349
0.140  420   9.1 0.4430851 0.4506128
0.145  434   9.1 0.4388431 0.4463522
0.150  450   9.3 0.4346421 0.4421518
0.155  465   9.0 0.4302663 0.4380106
0.160  480   8.9 0.4262253 0.4339273
0.165  495   8.6 0.4220223 0.4299011
0.170  510   8.5 0.4181102 0.4259307
0.175  525   8.7 0.4143608 0.4220151
0.180  540   8.3 0.4101796 0.4181534
0.185  555   8.4 0.4066864 0.4143446
0.190  570   8.0 0.4026982 0.4105877
0.195  585   8.0 0.3987708 0.4068817
0.200  600   8.2 0.3955218 0.4032258
}
\begin{frame}{Watts-Strogatz Model: Diameter (circles) and Clustering
  Coeff\/{i}cient (triangles)}
    \centerline{
    \scalebox{0.9}{
    \psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray,
                    arrowscale=2,PointName=none}
    \psset{xAxisLabel=Edge probability: $r$,
                    yAxisLabel=Diameter: $d(G)$,
                    xAxisLabelPos={c,-0.4in},
                    yAxisLabelPos={-0.4in,c}}
    \psgraph[tickstyle=bottom,Dx=0.02,Dy=10](0,0)(0.21,100){4in}{2.25in}%
    \listplot[nStep=2,showpoints=true,linestyle=dotted,nStart=2,plotNo=2,plotNoMax=4]{\WSdata}
    \psdot(0,110)
    \uput[r](0,110){\small 167}
    \psline[linestyle=dotted]{-}(0,110)(0.005,60.5)
    \pstScalePoints(1,100){}{}
    \psset{dotstyle=Btriangle}
    \listplot[nStep=2,nStart=1,linewidth=1pt,plotNo=4,plotNoMax=4]{\WSdata}
    \listplot[nStep=2,showpoints=true,linestyle=dotted,nStart=1,plotNo=3,plotNoMax=4]{\WSdata}
    \psaxes[xAxis=false,tickstyle=top,ylabelPos=right,Dy=0.1,dy=10]{-}(0.205,0)(0.205,100)
    \rput{90}(0.23,50){Clustering coefficient: $C(G)$}
    \endpsgraph
    }}
\end{frame}


\begin{frame}{Barab\'{a}si--Albert Scale-free Model}

The Barab\'{a}si--Albert (BA) yields a scale-free degree distribution
based on {\em preferential attachment}; that is,
edges from the new vertex are more likely to link to
nodes with higher degrees.

\smallskip
Let $G_t$ denote the
graph at time $t$, and let $n_t$ denote the number of nodes, and $m_t$
the number of edges in $G_t$.

\medskip
{\bf Initialization:} 
The BA model starts with $G_0$, with each node connected to its left and
right neighbors in a circular layout.
Thus $m_0 = n_0$.


\medskip{\bf Growth and Preferential Attachment:}
The BA model derives a new graph $G_{t+1}$ from
$G_{t}$ by adding exactly one new node $u$
and adding $q \le n_0$ new edges from $u$ to $q$ distinct
nodes $v_{j} \in G_{t}$, where node $v_{j}$ is
chosen with probability $\pi_{t}(v_{j})$
proportional to its degree in $G_{t}$, given as
\begin{align*}
    \pi_{t}(v_{j}) = {d_{j} \over \sum_{v_i \in G_{t}} d_i}
\end{align*}

\end{frame}


\begin{frame}{Barab\'{a}si--Albert Graph}
  \framesubtitle{$n_0=3$, $q=2$, $t=12$}

  \begin{columns}
	\column{0.5\textwidth}
  At $t=0$, start with 3 vertices $v_0$, $v_1$, and $v_2$ 
  fully connected
  (shown in gray).

	\smallskip
At $t=1$, vertex $v_3$ is added, with edges to
    $v_1$ and $v_2$, chosen according to the distribution $$\pi_0
    (v_i)={1/3} \;\; \text{for } i=0,1,2$$

	\smallskip
	At $t=2$, $v_4$ is added.
    Nodes $v_2$ and $v_3$ are preferentially chosen according to
    the probability distribution
    \begin{align*}
    \pi_1(v_0) & = \pi_1(v_3) = {2 \over 10} = 0.2\\
    \pi_1(v_1) & = \pi_1(v_2) = {3 \over 10} = 0.3
    \end{align*}

	\column{0.5\textwidth}
    \begin{center}
    \scalebox{0.6}{
        \psset{unit=0.7in}
        \begin{pspicture}(5,5.5)
            %nodes
            \psset{fillstyle=solid,fillcolor=lightgray}
            \cnodeput(2.50000000, 5.0000000){n0}{$v_0$}
            \cnodeput(1.48315839, 4.7838636){n1}{$v_1$}
            \cnodeput(0.64213794, 4.1728265){n2}{$v_2$}
            \psset{fillstyle=none}
            \cnodeput(0.12235871, 3.2725425){n3}{$v_3$}
            \cnodeput(0.01369526, 2.2386788){n4}{$v_4$}
            \cnodeput(0.33493649, 1.2500000){n5}{$v_5$}
            \cnodeput(1.03053687, 0.4774575){n6}{$v_6$}
            \cnodeput(1.98022077, 0.0546310){n7}{$v_7$}
            \cnodeput(3.01977923, 0.0546310){n8}{$v_8$}
            \cnodeput(3.96946313, 0.4774575){n9}{$v_9$}
            \cnodeput(4.66506351, 1.2500000){n10}{$v_{10}$}
            \cnodeput(4.98630474, 2.2386788){n11}{$v_{11}$}
            \cnodeput(4.87764129, 3.2725425){n12}{$v_{12}$}
            \cnodeput(4.35786206, 4.1728265){n13}{$v_{13}$}
            \cnodeput(3.51684161, 4.7838636){n14}{$v_{14}$}
            %edges
            \ncarc[arcangle=0,linewidth=2pt]{n1}{n0}
            \ncarc[arcangle=0,linewidth=2pt]{n2}{n1}
            \ncarc[arcangle=-20,linewidth=2pt]{n2}{n0}
            \ncarc[arcangle=-20]{n3}{n1}
            \ncarc[arcangle=-0]{n3}{n2}
            \ncarc[arcangle=0]{n4}{n3}
            \ncarc[arcangle=-20]{n4}{n2}
            \ncarc[arcangle=-20]{n5}{n3}
            \ncarc[arcangle=-0]{n5}{n1}
            \ncarc[arcangle=-20]{n6}{n4}
            \ncarc[arcangle=-20]{n6}{n3}
            \ncarc[arcangle=-0]{n7}{n2}
            \ncarc[arcangle=-20]{n7}{n4}
            \ncarc[arcangle=-0]{n8}{n1}
            \ncarc[arcangle=-0]{n8}{n7}
            \ncarc[arcangle=-0]{n9}{n1}
            \ncarc[arcangle=-0]{n9}{n8}
            \ncarc[arcangle=-0]{n10}{n0}
            \ncarc[arcangle=-0]{n10}{n1}
            \ncarc[arcangle=-0]{n11}{n1}
            \ncarc[arcangle=-0]{n11}{n5}
            \ncarc[arcangle=-20]{n12}{n10}
            \ncarc[arcangle=-0]{n12}{n8}
            \ncarc[arcangle=-0]{n13}{n1}
            \ncarc[arcangle=-20]{n13}{n11}
            \ncarc[arcangle=-0]{n14}{n3}
            \ncarc[arcangle=-0]{n14}{n6}
        \end{pspicture}
    }
    \end{center}

  \end{columns}
\end{frame}


\begin{frame}{Properties of the BA Graphs}
\small
\smallskip
{\bf Degree Distribution:}
The degree distribution for BA graphs is given as
\begin{align*}
f(k) = {(q+2)(q+1)q \over (k+2)(k+1)k} \cdot {2 \over
(q+2)} = {2q(q+1) \over k(k+1)(k+2)}
\end{align*}
For constant $q$ and large $k$, the degree
distribution scales as
\begin{align*}
    f(k) \propto k^{-3}
\end{align*}
The BA model yields a power-law degree
distribution with $\gamma=3$, especially for large degrees.

\medskip
{\bf Diameter:}
The diameter of BA graphs scales as
$$d(G_t) = O\lB( {\log n_t \over \log \log n_t} \rB)$$
suggesting that they exhibit {\em ultra-small-world} behavior,
when $q>1$.

\medskip
{\bf Clustering Coefficient:}
The expected clustering coeff\/{i}cient of the BA graphs
scales as
$$E[C(G_t)] = O\lB( {(\log n_t)^2 \over n_t} \rB) $$
which is only slightly better than for random graphs.
\end{frame}



\def\BAdata{
1.584963  -1.333516
2.000000  -2.328435
2.321928  -3.081919
2.584963  -3.797864
2.807355  -4.480357
3.000000  -4.848921
3.169925  -5.237864
3.321928  -5.725470
3.459432  -6.287712
3.584963  -6.702750
3.700440  -6.779918
3.807355  -7.380822
3.906891  -7.333516
4.000000  -7.828281
4.087463  -8.078259
4.169925  -8.333516
4.247928  -8.965784
4.321928  -8.380822
4.392317  -8.587273
4.459432  -8.828281
4.523562  -9.587273
4.584963  -9.117787
4.643856  -9.287712
4.700440  -9.480357
4.754888  -9.287712
4.807355  -9.702750
4.857981 -10.287712
4.906891 -10.287712
4.954196 -10.702750
5.000000  -9.828281
5.044394 -10.965784
5.087463 -12.287712
5.129283 -11.702750
5.169925 -11.287712
5.209453 -10.965784
5.247928 -11.287712
5.285402 -11.702750
5.321928 -10.965784
5.357552 -10.117787
5.392317 -12.287712
5.426265 -11.287712
5.459432 -12.287712
5.491853 -13.287712
5.523562 -11.287712
5.554589 -11.702750
5.584963 -12.287712
5.614710 -13.287712
5.643856 -12.287712
5.672425 -11.702750
5.700440 -12.287712
5.754888 -12.287712
5.807355 -11.702750
5.832890 -12.287712
5.857981 -10.480357
5.882643 -13.287712
5.930737 -11.287712
5.977280 -13.287712
6.000000 -11.287712
6.022368 -13.287712
6.044394 -13.287712
6.066089 -12.287712
6.087463 -12.287712
6.149747 -11.702750
6.189825 -12.287712
6.228819 -13.287712
6.266787 -12.287712
6.357552 -11.702750
6.375039 -11.702750
6.409391 -13.287712
6.475733 -12.287712
6.491853 -12.287712
6.507795 -13.287712
6.584963 -13.287712
6.629357 -13.287712
6.672425 -13.287712
6.686501 -13.287712
6.857981 -13.287712
6.870365 -13.287712
7.011227 -13.287712
}
\begin{frame}{Barab\'{a}si--Albert Model: Degree Distribution} 
  \framesubtitle{$n_0=3,t=997,q=3$}
    \centerline{
    \scalebox{0.9}{
    \psset{dotstyle=Bo,dotscale=1.5,fillcolor=lightgray,
            arrowscale=2,PointName=none}
    \psset{xAxisLabel=Degree: $\log_2 k$,
            yAxisLabel=Probability: $\log_2 f(k)$,
            xAxisLabelPos={c,-0.4in},
            yAxisLabelPos={-0.6in,c}}
    \psgraph[tickstyle=bottom,Ox=1,Dx=1,Oy=-14,Dy=2]{->}(1,-14)(8,-1){4in}{2.5in}%
    \listplot[plotstyle=dots,showpoints=true]{\BAdata}
    \listplot[plotstyle=LSM,linewidth=1pt,
    linecolor=black,nStart=1,nEnd=60]{\BAdata}
    \rput[r](4.2,-3){$-\gamma=-2.64$}
    \endpsgraph
    }}
\end{frame}
